{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Importing Libraries"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-01-27T22:29:39.675498Z","iopub.status.busy":"2024-01-27T22:29:39.675143Z","iopub.status.idle":"2024-01-27T22:29:46.748067Z","shell.execute_reply":"2024-01-27T22:29:46.747074Z","shell.execute_reply.started":"2024-01-27T22:29:39.675469Z"},"trusted":true},"outputs":[],"source":["import os\n","import math\n","import spacy\n","import torch\n","import numpy as np\n","import pandas as pd\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import torchvision.transforms as T\n","\n","from PIL import Image\n","from torch import nn\n","from matplotlib import pyplot as plt\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","from torch.nn.utils.rnn import pad_sequence\n","from torchvision.models import vgg16, VGG16_Weights\n","from nltk.translate.bleu_score import sentence_bleu\n","\n","spacy_eng = spacy.load(\"en_core_web_sm\")"]},{"cell_type":"markdown","metadata":{},"source":["# Preparing the Flikr30k Dataset\n","### Preparing the Vocabulary"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-01-27T22:29:46.750455Z","iopub.status.busy":"2024-01-27T22:29:46.749957Z","iopub.status.idle":"2024-01-27T22:29:46.760908Z","shell.execute_reply":"2024-01-27T22:29:46.759735Z","shell.execute_reply.started":"2024-01-27T22:29:46.750431Z"},"trusted":true},"outputs":[],"source":["class Vocabulary:\n","    def __init__(self, freq_threshold):\n","        self.freq_threshold = freq_threshold\n","        self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n","        self.stoi = {v: k for k,v in self.itos.items()}\n","    \n","    def __len__(self):\n","        return len(self.itos)\n","  \n","    def build_vocab(self, sentence_list):\n","        freqs = {}\n","        idx = 4\n","\n","        for sentence in sentence_list:\n","            sentence = str(sentence)\n","\n","            for word in self.tokenize(sentence):\n","                if word not in freqs:\n","                    freqs[word] = 1\n","                    \n","                else:\n","                    freqs[word] += 1\n","\n","                if freqs[word] == self.freq_threshold:\n","                    self.itos[idx] = word\n","                    self.stoi[word] = idx\n","                    idx += 1\n","\n","    def numericalize(self, sentence):\n","        tokens = self.tokenize(sentence)\n","        result = []\n","\n","        for token in tokens:\n","            if token in self.stoi:\n","                result.append(self.stoi[token])\n","            else:\n","                result.append(self.stoi[\"<UNK>\"])\n","\n","        return result\n","    \n","    @staticmethod\n","    def tokenize(sentence):\n","        return [token.text.lower() for token in spacy_eng.tokenizer(sentence)]"]},{"cell_type":"markdown","metadata":{},"source":["### Defining a Custom Dataset"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-01-27T22:29:46.763411Z","iopub.status.busy":"2024-01-27T22:29:46.762779Z","iopub.status.idle":"2024-01-27T22:29:46.774755Z","shell.execute_reply":"2024-01-27T22:29:46.773611Z","shell.execute_reply.started":"2024-01-27T22:29:46.763375Z"},"trusted":true},"outputs":[],"source":["class Flickr(Dataset):\n","    def __init__(self, root_dir, caps, transforms=None, freq_threshold=5):\n","        self.root_dir = root_dir\n","        self.df = pd.read_csv(caps, delimiter='|')\n","        self.transforms = transforms\n","\n","        self.img_pts = self.df['image_name']\n","        self.caps = self.df[' comment']\n","        self.vocab = Vocabulary(freq_threshold)\n","        self.vocab.build_vocab(self.caps.tolist())\n","\n","    def __len__(self):\n","        return len(self.df)\n","  \n","    def __getitem__(self, idx):\n","        captions = self.caps[idx]\n","        img_pt = self.img_pts[idx]\n","        img = Image.open(os.path.join(self.root_dir, img_pt)).convert('RGB')\n","\n","        if self.transforms is not None:\n","            img = self.transforms(img)\n","\n","        numberized_caps = []\n","        numberized_caps += [self.vocab.stoi[\"<SOS>\"]]\n","        numberized_caps += self.vocab.numericalize(captions)\n","        numberized_caps += [self.vocab.stoi[\"<EOS>\"]]\n","        \n","        return img, torch.tensor(numberized_caps)"]},{"cell_type":"markdown","metadata":{},"source":["### Defining a Custom Caption Collat for Padding"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-01-27T22:29:46.777594Z","iopub.status.busy":"2024-01-27T22:29:46.777142Z","iopub.status.idle":"2024-01-27T22:29:46.789231Z","shell.execute_reply":"2024-01-27T22:29:46.788218Z","shell.execute_reply.started":"2024-01-27T22:29:46.777555Z"},"trusted":true},"outputs":[],"source":["class CapCollat:\n","    def __init__(self, pad_seq, batch_first=False):\n","        self.pad_seq = pad_seq\n","        self.batch_first = batch_first\n","  \n","    def __call__(self, batch):\n","        imgs = [itm[0].unsqueeze(0) for itm in batch]\n","        imgs = torch.cat(imgs, dim=0)\n","\n","        target_caps = [itm[1] for itm in batch]\n","        target_caps = pad_sequence(target_caps, batch_first=self.batch_first,\n","                                   padding_value=self.pad_seq)\n","        return imgs, target_caps"]},{"cell_type":"markdown","metadata":{},"source":["### Loading and Testing the Dataset "]},{"cell_type":"code","execution_count":70,"metadata":{"execution":{"iopub.execute_input":"2024-01-27T23:59:23.412407Z","iopub.status.busy":"2024-01-27T23:59:23.412024Z","iopub.status.idle":"2024-01-27T23:59:47.520172Z","shell.execute_reply":"2024-01-27T23:59:47.519037Z","shell.execute_reply.started":"2024-01-27T23:59:23.412375Z"},"trusted":true},"outputs":[],"source":["root_folder = \"/kaggle/input/flickr-image-dataset/flickr30k_images/flickr30k_images/\"\n","csv_file = \"/kaggle/input/flickr-image-dataset/flickr30k_images/results.csv\"\n","\n","transforms = T.Compose([\n","                        T.Resize((224,224)),\n","                        T.ToTensor(),\n","                        T.Normalize(mean=[0.485, 0.456, 0.406],\n","                                    std=[0.229, 0.224, 0.225])\n","                       ])\n","batch_size = 32\n","num_workers = 2\n","batch_first = True\n","pin_memory = True\n","dataset = Flickr(root_folder, csv_file, transforms)\n","pad_idx = dataset.vocab.stoi[\"<PAD>\"]\n","\n","data_size = len(dataset)\n","train_size = int(0.9 * data_size)\n","val_size = data_size - train_size\n","\n","train_set, val_set = torch.utils.data.random_split(dataset, [train_size, val_size])\n","\n","dataset_loader = DataLoader(dataset,\n","                            batch_size=batch_size,\n","                            pin_memory=pin_memory,\n","                            num_workers=num_workers,\n","                            collate_fn=CapCollat(pad_seq=pad_idx, batch_first=batch_first))\n","\n","train_loader = DataLoader(train_set,\n","                            batch_size=batch_size,\n","                            pin_memory=pin_memory,\n","                            num_workers=num_workers,\n","                            shuffle=True,\n","                            collate_fn=CapCollat(pad_seq=pad_idx, batch_first=batch_first))\n","\n","val_loader = DataLoader(val_set,\n","                            batch_size=batch_size,\n","                            pin_memory=pin_memory,\n","                            num_workers=num_workers,\n","                            shuffle=False,\n","                            collate_fn=CapCollat(pad_seq=pad_idx, batch_first=batch_first))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-27T23:59:51.861879Z","iopub.status.busy":"2024-01-27T23:59:51.860174Z","iopub.status.idle":"2024-01-27T23:59:52.176616Z","shell.execute_reply":"2024-01-27T23:59:52.174433Z","shell.execute_reply.started":"2024-01-27T23:59:51.861836Z"},"trusted":true},"outputs":[],"source":["val_set_start = data_size - val_size - 1\n","val_set_end = data_size - 1\n","\n","all_labels = list()\n","dict_tokens = dict()\n","\n","idx = 0\n","dataitr = iter(dataset_loader)\n","\n","while idx < data_size:\n","    images, captions = next(dataitr)\n","    \n","    for caption in captions:\n","        caption_label = [dataset.vocab.itos[token] for token in caption.tolist()]\n","\n","        eos_index = caption_label.index('<EOS>')\n","\n","        caption_label = caption_label[1: eos_index]\n","\n","        caption_label = ' '.join(caption_label)\n","        \n","        dict_tokens[idx] = [caption_label]\n","\n","        all_labels.append(caption_label)\n","        \n","        idx += 1"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-27T23:41:11.812223Z","iopub.status.busy":"2024-01-27T23:41:11.811789Z","iopub.status.idle":"2024-01-27T23:41:20.671514Z","shell.execute_reply":"2024-01-27T23:41:20.670078Z","shell.execute_reply.started":"2024-01-27T23:41:11.812191Z"},"trusted":true},"outputs":[],"source":["dataitr = iter(val_loader)\n","batch = next(dataitr)\n","\n","images, captions = batch\n","\n","for i in range(batch_size):\n","    img, cap = images[i], captions[i]\n","    \n","    caption_label = [dataset.vocab.itos[token] for token in cap.tolist()]\n","    \n","    eos_index = caption_label.index('<EOS>')\n","\n","    caption_label = caption_label[1:eos_index]\n","    \n","    caption_label = ' '.join(caption_label)\n","\n","    img = img.permute(1,2,0)\n","    plt.imshow(img)\n","    plt.title(caption_label)\n","\n","    plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["# Pre-Trained CNN Encoder: VGG16 "]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-01-27T14:53:02.834524Z","iopub.status.busy":"2024-01-27T14:53:02.834047Z","iopub.status.idle":"2024-01-27T14:53:02.844526Z","shell.execute_reply":"2024-01-27T14:53:02.843365Z","shell.execute_reply.started":"2024-01-27T14:53:02.834476Z"},"trusted":true},"outputs":[],"source":["class EncoderCNN(nn.Module):\n","    def __init__(self, enc_dim, hidden_size):\n","        super(EncoderCNN, self).__init__()\n","\n","        vgg = vgg16(weights=VGG16_Weights.IMAGENET1K_FEATURES)\n","        all_modules = list(vgg.children())\n","        modules = all_modules[:-2]\n","\n","        self.vgg = nn.Sequential(*modules)\n","\n","        self.V_affine = nn.Linear(enc_dim, hidden_size)\n","\n","        self.disable_learning()\n","       \n","    def forward(self, image):\n","        encoded_image = self.vgg(image)\n","\n","        batch_size = encoded_image.shape[0]\n","        features = encoded_image.shape[1]\n","        num_pixels = encoded_image.shape[2] * encoded_image.shape[3]\n","\n","        enc_image = encoded_image.permute(0, 2, 3, 1)  \n","        enc_image = enc_image.view(batch_size, num_pixels, features)\n","        enc_image = F.relu(self.V_affine(enc_image))\n","  \n","        return enc_image\n","\n","    def disable_learning(self):\n","        for param in self.vgg.parameters():\n","            param.requires_grad = False"]},{"cell_type":"markdown","metadata":{},"source":["# RNN Decoder: Vanilla RNN Module"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-01-27T14:53:03.635589Z","iopub.status.busy":"2024-01-27T14:53:03.634924Z","iopub.status.idle":"2024-01-27T14:53:03.645218Z","shell.execute_reply":"2024-01-27T14:53:03.643974Z","shell.execute_reply.started":"2024-01-27T14:53:03.635542Z"},"trusted":true},"outputs":[],"source":["class DecoderRNN(nn.Module):\n","    def __init__(self, embed_size,vocab_size, hidden_size, num_layers):\n","        super(DecoderRNN, self).__init__()\n","\n","        self.embedding = nn.Embedding(vocab_size, embed_size)\n","        self.rnn = nn.RNN(embed_size, hidden_size, num_layers)\n","        self.linear = nn.Linear(hidden_size, vocab_size)\n","        self.dropout = nn.Dropout(0.5)\n","    \n","    def forward(self, features, caption):\n","        embeddings = self.dropout(self.embedding(caption))\n","        embeddings = torch.cat((features.unsqueeze(0),embeddings), dim=0)\n","        hiddens, _ = self.rnn(embeddings)\n","        outputs = self.linear(hiddens)\n","\n","        return outputs"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class Hybrid(nn.Module):\n","    def __init__(self, embed_size, vocab_size, hidden_size, num_layers):\n","        super(Hybrid, self).__init__()\n","        self.encoderCNN = EncoderCNN(embed_size)\n","        self.decoderRNN = DecoderRNN(embed_size, vocab_size, hidden_size, num_layers)\n","    \n","    def forward(self, images, caption):\n","        x = self.encoderCNN(images)\n","        x = self.decoderRNN(x, caption)\n","        return x\n","    \n","    def captionImage(self, image, vocabulary, maxlength=50):\n","        result_caption = []\n","        \n","        with torch.no_grad():\n","            x = self.encoderCNN(image).unsqueeze(0)\n","            states = None\n","            \n","            for _ in range(maxlength):\n","                hiddens, states = self.decoderRNN.lstm(x, states)\n","                output = self.decoderRNN.linear(hiddens.squeeze(0))\n","                predicted = output.argmax(1)\n","                print(predicted.shape)\n","                result_caption.append(predicted.item())\n","                x = self.decoderRNN.embedding(output).unsqueeze(0)\n","                \n","                if vocabulary.itos[predicted.item()] == \"<EOS>\":\n","                    break\n","                \n","        return [vocabulary.itos[i] for i in result_caption]"]},{"cell_type":"markdown","metadata":{},"source":["# Training and Evaluating the Image Captioning Model: VGG16 + Multi-layer Vanilla RNN"]},{"cell_type":"markdown","metadata":{},"source":["### Setting the Device to GPU"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-01-27T14:53:10.068373Z","iopub.status.busy":"2024-01-27T14:53:10.067943Z","iopub.status.idle":"2024-01-27T14:53:10.075991Z","shell.execute_reply":"2024-01-27T14:53:10.074637Z","shell.execute_reply.started":"2024-01-27T14:53:10.068337Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Device: cpu\n"]}],"source":["if torch.cuda.is_available():\n","    torch.backends.cudnn.deterministic = True\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","print('Device:', device)"]},{"cell_type":"markdown","metadata":{},"source":["### Setting Hyperparameters"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-01-27T14:53:12.505491Z","iopub.status.busy":"2024-01-27T14:53:12.505084Z","iopub.status.idle":"2024-01-27T14:53:12.511924Z","shell.execute_reply":"2024-01-27T14:53:12.510697Z","shell.execute_reply.started":"2024-01-27T14:53:12.505459Z"},"trusted":true},"outputs":[],"source":["num_epochs = 10\n","freq_threshold = 5\n","enc_dim = 2048\n","embed_size = 300\n","hidden_size = 512\n","num_layers = 2\n","learning_rate = 3e-4"]},{"cell_type":"markdown","metadata":{},"source":["### Building the Vocabulary"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-01-27T14:53:14.511906Z","iopub.status.busy":"2024-01-27T14:53:14.511386Z","iopub.status.idle":"2024-01-27T14:53:14.908939Z","shell.execute_reply":"2024-01-27T14:53:14.907664Z","shell.execute_reply.started":"2024-01-27T14:53:14.511866Z"},"trusted":true},"outputs":[],"source":["vocab = Vocabulary(freq_threshold)\n","vocab.build_vocab(all_labels)\n","vocab_size = len(vocab)"]},{"cell_type":"markdown","metadata":{},"source":["### Configuring Models"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-27T14:53:17.358169Z","iopub.status.busy":"2024-01-27T14:53:17.357793Z","iopub.status.idle":"2024-01-27T14:53:27.110188Z","shell.execute_reply":"2024-01-27T14:53:27.108889Z","shell.execute_reply.started":"2024-01-27T14:53:17.358138Z"},"trusted":true},"outputs":[],"source":["\n","criterion = nn.CrossEntropyLoss(ignore_index = vocab.stoi[\"<PAD>\"])\n","\n","model = Hybrid(embed_size, hidden_size,vocab_size, num_layers).to(device=device)\n","\n","optimizer = optim.Adam(model.parameters(), lr = learning_rate)"]},{"cell_type":"markdown","metadata":{},"source":["### Training Models"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-27T14:53:30.497278Z","iopub.status.busy":"2024-01-27T14:53:30.496878Z","iopub.status.idle":"2024-01-27T14:53:49.584654Z","shell.execute_reply":"2024-01-27T14:53:49.583324Z","shell.execute_reply.started":"2024-01-27T14:53:30.497247Z"},"trusted":true},"outputs":[],"source":["!pip install pycocoevalcap"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-01-27T14:53:55.737416Z","iopub.status.busy":"2024-01-27T14:53:55.736966Z","iopub.status.idle":"2024-01-27T14:53:55.764975Z","shell.execute_reply":"2024-01-27T14:53:55.763678Z","shell.execute_reply.started":"2024-01-27T14:53:55.737378Z"},"trusted":true},"outputs":[],"source":["from pycocoevalcap.bleu.bleu import Bleu\n","from pycocoevalcap.meteor.meteor import Meteor\n","from pycocoevalcap.rouge.rouge import Rouge\n","from pycocoevalcap.cider.cider import Cider\n","\n","class Scorer():\n","    def __init__(self,ref,gt):\n","        self.ref = ref\n","        self.gt = gt\n","\n","        self.word_based_scorers = [\n","            (Bleu(4), [\"Bleu_1\", \"Bleu_2\", \"Bleu_3\", \"Bleu_4\"]),\n","            (Meteor(),\"METEOR\"),\n","            (Rouge(), \"ROUGE_L\"),\n","            (Cider(), \"CIDEr\"),\n","            ]\n","\n","    def compute_scores(self):\n","        total_scores = {\n","            \"Bleu1\":[],\n","            \"Bleu2\":[],\n","            \"Bleu3\":[],\n","            \"Bleu4\":[],\n","            \"METEOR\":[],\n","            \"ROUGE_L\":[],\n","            \"CIDEr\":[],\n","        }\n","\n","        for scorer, method in self.word_based_scorers:\n","            score, scores = scorer.compute_score(self.ref, self.gt)\n","    \n","            if type(method) == list:\n","                total_scores[\"Bleu1\"].append(score[0])\n","                total_scores[\"Bleu2\"].append(score[1])\n","                total_scores[\"Bleu3\"].append(score[2])\n","                total_scores[\"Bleu4\"].append(score[3])\n","\n","            else:\n","                total_scores[method].append(score)\n","\n","        return total_scores\n","    \n","    def compute_scores_iterative(self):\n","        total_scores = {\n","            \"Bleu1\":[],\n","            \"Bleu2\":[],\n","            \"Bleu3\":[],\n","            \"Bleu4\":[],\n","            \"METEOR\":[],\n","            \"ROUGE_L\":[],\n","            \"CIDEr\":[],\n","            \"SPICE\":[]\n","        \n","        }\n","\n","        for key in self.ref:\n","            curr_ref = {key:self.ref[key]}\n","            curr_gt = {key:self.gt[key]}\n","\n","            for scorer, method in self.word_based_scorers:\n","                score, _ = scorer.compute_score(curr_ref, curr_gt)\n","                if type(method) == list:\n","                    total_scores[\"Bleu1\"].append(score[0])\n","                    total_scores[\"Bleu2\"].append(score[1])\n","                    total_scores[\"Bleu3\"].append(score[2])\n","                    total_scores[\"Bleu4\"].append(score[3])\n","\n","                else:\n","                    total_scores[method].append(score)\n","\n","        return total_scores"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-27T14:53:59.365530Z","iopub.status.busy":"2024-01-27T14:53:59.365143Z","iopub.status.idle":"2024-01-27T14:54:13.465193Z","shell.execute_reply":"2024-01-27T14:54:13.463231Z","shell.execute_reply.started":"2024-01-27T14:53:59.365498Z"},"trusted":true},"outputs":[],"source":["val_iter = iter(val_loader)\n","\n","train_losses = list()\n","val_losses = list()\n","\n","cumulative_bleu_scores = list()\n","\n","cider_scores = list()\n","\n","meteor_scores = list()\n","\n","rougel_scores = list()\n","\n","bleu1_scores = list()\n","bleu2_scores = list()\n","bleu3_scores = list()\n","bleu4_scores = list()\n","\n","for epoch in range(num_epochs):\n","    model.train()\n","\n","    for batch_idx, (imgs, captions) in enumerate(train_loader):\n","        imgs = imgs.to(device)\n","        captions = captions.to(device)\n","        \n","        train_score = model(imgs, captions[:-1])\n","\n","        optimizer.zero_grad()\n","        train_loss = criterion(train_score.reshape(-1, train_score.shape[2]), captions.reshape(-1))\n","        train_losses.append(train_loss.item())\n","        \n","        train_loss.backward()\n","        optimizer.step()\n","\n","    with torch.no_grad():\n","        model.eval()\n","\n","        idx = val_set_start\n","        dataitr = iter(val_loader)\n","        batch = next(dataitr)\n","\n","        hyp = {}\n","        refs = {}\n","\n","        while idx <= val_set_end:\n","            val_imgs, val_captions = batch\n","\n","            img, cap = val_imgs[idx % 32], val_captions[idx % 32]\n","\n","            caption_label = [dataset.vocab.itos[token] for token in cap.tolist()]\n","\n","            eos_index = caption_label.index('<EOS>')\n","\n","            caption_label = caption_label[1: eos_index]\n","\n","            caption_label = ' '.join(caption_label)\n","\n","            img = img.to_device()\n","            cap = cap.to_device()\n","\n","            val_score = model(img, cap[: -1])\n","\n","            val_loss = criterion(val_score.reshape(-1, val_score.shape[2]), val_captions[1:].reshape(-1))\n","\n","            val_losses.append(val_loss.item())\n","\n","            val_pred = model.caption_image(img, vocab)\n","\n","            hyp[idx] = [' '.join(val_pred)]\n","\n","            refs_token = dict_tokens[idx]\n","\n","            refs[idx] = refs_token\n","\n","            cumulative_bleu_score = sentence_bleu(refs_token, val_pred, weights=(0.25, 0.25, 0.25, 0.25))\n","            cumulative_bleu_scores.append(cumulative_bleu_score)\n","\n","            if idx % 32 == 0:\n","                batch = next(dataitr)\n","            \n","            idx += 1\n","            \n","        metrics = Scorer(refs, hyp).compute_scores()\n","    \n","        bleu1_scores.append(metrics['Bleu_1'])\n","        bleu2_scores.append(metrics['Bleu_2'])\n","        bleu3_scores.append(metrics['Bleu_3'])\n","        bleu4_scores.append(metrics['Bleu_4'])\n","\n","        cider_scores.append(metrics['CIDEr'])\n","\n","        rougel_scores.append(metrics['ROUGE_L'])\n","\n","        meteor_scores.append(metrics['METEOR'])\n","        \n","        print(f\"Epoch [{epoch+1}/{num_epochs}] Batch [{batch_idx+1}/{len(train_loader)}] | Training loss: {train_loss.item()} Validation loss: {val_loss.item()} | Cumulative BLEU Score: {cumulative_bleu_score} CIDEr Score: {metrics['CIDEr']} METEOR Score: {metrics['METEOR']} ROUGE_L: {metrics['ROUGE_L']} \\n\")\n","\n","# if batch_idx == (len(train_loader) - 1):\n","#     torch.save(model.state_dict(), f'/kaggle/working/encoder_{freq_threshold}_{batch_size}_{hidden_size}_{epoch+1}.pth')\n"]},{"cell_type":"markdown","metadata":{},"source":["### Plotting Losses and Scores"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.figure(0)\n","plt.plot(train_losses, label = 'Training loss')\n","plt.plot(val_losses, label = 'Validation loss')\n","plt.ylabel('Cross Entropy Loss')\n","plt.legend()\n","plt.savefig(f'/kaggle/working/{freq_threshold}_{batch_size}_{hidden_size}_{num_epochs}_losses.png')\n","\n","plt.figure(1)\n","plt.plot(bleu1_scores, label = 'BLEU 1')\n","plt.plot(bleu2_scores, label = 'BLEU 2')\n","plt.plot(bleu3_scores, label = 'BLEU 3')\n","plt.plot(bleu4_scores, label = 'BLEU 4')\n","plt.ylabel('BLEU Scores')\n","plt.legend()\n","plt.savefig(f'/kaggle/working/{freq_threshold}_{batch_size}_{hidden_size}_{num_epochs}_bleu_scores.png')\n","        \n","plt.figure(2)\n","plt.plot(cumulative_bleu_scores, label = 'Cumulative BLEU SCORE')\n","plt.plot(cider_scores, label = 'CIDEr SCORE')\n","plt.plot(meteor_scores, label = 'METEOR SCORE')\n","plt.plot(rougel_scores, label = 'ROUGE_L SCORE')\n","plt.ylabel('Scores')\n","plt.legend()\n","plt.savefig(f'/kaggle/working/{freq_threshold}_{batch_size}_{hidden_size}_{num_epochs}_scores.png')"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":31296,"sourceId":39911,"sourceType":"datasetVersion"}],"dockerImageVersionId":30635,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
