{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Importing Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-31T12:43:42.704085Z","iopub.status.busy":"2024-01-31T12:43:42.703470Z","iopub.status.idle":"2024-01-31T12:43:58.363902Z","shell.execute_reply":"2024-01-31T12:43:58.363042Z","shell.execute_reply.started":"2024-01-31T12:43:42.704055Z"},"trusted":true},"outputs":[],"source":["!pip install torchsummary\n","\n","import os\n","import math\n","import spacy\n","import torch\n","import numpy as np\n","import pandas as pd\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import torchvision.transforms as T\n","\n","from PIL import Image\n","from torch import nn\n","from torchsummary import summary\n","from matplotlib import pyplot as plt\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","from torch.nn.utils.rnn import pad_sequence\n","from torchvision.models import vgg16, VGG16_Weights\n","from nltk.translate.bleu_score import sentence_bleu\n","\n","spacy_eng = spacy.load(\"en_core_web_sm\")"]},{"cell_type":"markdown","metadata":{},"source":["### Setting the Device"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-31T12:44:04.148977Z","iopub.status.busy":"2024-01-31T12:44:04.148525Z","iopub.status.idle":"2024-01-31T12:44:04.154094Z","shell.execute_reply":"2024-01-31T12:44:04.153500Z","shell.execute_reply.started":"2024-01-31T12:44:04.148952Z"},"trusted":true},"outputs":[],"source":["if torch.cuda.is_available():\n","    torch.backends.cudnn.deterministic = True\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","print('Device:', device)"]},{"cell_type":"markdown","metadata":{},"source":["# Preparing the Flikr30k Dataset\n","### Preparing the Vocabulary"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-01-31T12:44:04.927611Z","iopub.status.busy":"2024-01-31T12:44:04.927274Z","iopub.status.idle":"2024-01-31T12:44:04.935373Z","shell.execute_reply":"2024-01-31T12:44:04.934601Z","shell.execute_reply.started":"2024-01-31T12:44:04.927584Z"},"trusted":true},"outputs":[],"source":["class Vocabulary:\n","    def __init__(self, freq_threshold):\n","        self.freq_threshold = freq_threshold\n","        self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n","        self.stoi = {v: k for k,v in self.itos.items()}\n","    \n","    def __len__(self):\n","        return len(self.itos)\n","  \n","    def build_vocab(self, sentence_list):\n","        freqs = {}\n","        idx = 4\n","\n","        for sentence in sentence_list:\n","            sentence = str(sentence)\n","\n","            for word in self.tokenize(sentence):\n","                if word not in freqs:\n","                    freqs[word] = 1\n","                    \n","                else:\n","                    freqs[word] += 1\n","\n","                if freqs[word] == self.freq_threshold:\n","                    self.itos[idx] = word\n","                    self.stoi[word] = idx\n","                    \n","                    idx += 1\n","\n","    def numericalize(self, sentence):\n","        tokens = self.tokenize(sentence)\n","        result = []\n","\n","        for token in tokens:\n","            if token in self.stoi:\n","                result.append(self.stoi[token])\n","            else:\n","                result.append(self.stoi[\"<UNK>\"])\n","\n","        return result\n","    \n","    @staticmethod\n","    def tokenize(sentence):\n","        return [token.text.lower() for token in spacy_eng.tokenizer(str(sentence))]"]},{"cell_type":"markdown","metadata":{},"source":["### Defining a Custom Dataset"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-01-31T12:44:06.497938Z","iopub.status.busy":"2024-01-31T12:44:06.497623Z","iopub.status.idle":"2024-01-31T12:44:06.506234Z","shell.execute_reply":"2024-01-31T12:44:06.505634Z","shell.execute_reply.started":"2024-01-31T12:44:06.497911Z"},"trusted":true},"outputs":[],"source":["class Flickr(Dataset):\n","    def __init__(self, root_dir, caption_path, transform, freq_threshold=5):\n","        self.freq_threshold = freq_threshold\n","        self.transform = transform\n","        self.root_dir = root_dir\n","    \n","        self.df = pd.read_csv(caption_path, delimiter='|')\n","        \n","        self.images = self.df['image_name']\n","        self.captions = self.df[' comment']\n","        \n","        self.vocab = Vocabulary(freq_threshold)\n","        \n","        self.vocab.build_vocab(self.captions.tolist())\n","        \n","    def __len__(self):\n","        return len(self.df)\n","    \n","    def __getitem__(self, index):\n","        max_seq_length = 50\n","        \n","        image = self.images[index]\n","        caption = self.captions[index]\n","        \n","        image = Image.open(os.path.join(self.root_dir, image)).convert(\"RGB\")\n","        \n","        image = self.transform(image)\n","        \n","        numericalized_caption = [self.vocab.stoi[\"<SOS>\"]]\n","        \n","        numericalized_caption += self.vocab.numericalize(caption)\n","        \n","        numericalized_caption.append(self.vocab.stoi[\"<EOS>\"])\n","        \n","        if len(numericalized_caption) > max_seq_length:\n","            numericalized_caption = numericalized_caption[:max_seq_length]\n","        else:\n","            numericalized_caption += [self.vocab.stoi[\"<PAD>\"]] * (max_seq_length - len(numericalized_caption))\n","        \n","        return image, torch.tensor(numericalized_caption)\n","    \n","    def get_label(self, index):\n","        image, caption = self[index]\n","    \n","        label = [self.vocab.itos[token] for token in caption.tolist()]\n","\n","        eos_index = label.index('<EOS>')\n","\n","        label = label[1: eos_index]\n","\n","        return ' '.join(label)\n","    \n","    def to_list(self):\n","        return self.captions.tolist()\n","        "]},{"cell_type":"markdown","metadata":{},"source":["### Defining a Custom Caption Collat for Padding"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-01-31T12:44:08.168513Z","iopub.status.busy":"2024-01-31T12:44:08.168206Z","iopub.status.idle":"2024-01-31T12:44:08.173308Z","shell.execute_reply":"2024-01-31T12:44:08.172595Z","shell.execute_reply.started":"2024-01-31T12:44:08.168489Z"},"trusted":true},"outputs":[],"source":["class CapCollat:\n","    def __init__(self, pad_seq, batch_first=False):\n","        self.pad_seq = pad_seq\n","        self.batch_first = batch_first\n","  \n","    def __call__(self, batch):\n","        imgs = [itm[0].unsqueeze(0) for itm in batch]\n","        imgs = torch.cat(imgs, dim=0)\n","\n","        target_caps = [itm[1] for itm in batch]\n","        target_caps = pad_sequence(target_caps, batch_first=self.batch_first,\n","                                   padding_value=self.pad_seq)\n","        \n","        return imgs, target_caps"]},{"cell_type":"markdown","metadata":{},"source":["### Loading and Testing the Dataset "]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-01-31T12:44:09.943952Z","iopub.status.busy":"2024-01-31T12:44:09.943668Z","iopub.status.idle":"2024-01-31T12:44:34.158680Z","shell.execute_reply":"2024-01-31T12:44:34.157968Z","shell.execute_reply.started":"2024-01-31T12:44:09.943929Z"},"trusted":true},"outputs":[],"source":["root_folder = \"/kaggle/input/flickr-image-dataset/flickr30k_images/flickr30k_images/\"\n","csv_file = \"/kaggle/input/flickr-image-dataset/flickr30k_images/results.csv\"\n","\n","transform = T.Compose([\n","        T.Resize((256, 256), interpolation=T.InterpolationMode.BILINEAR),\n","        T.CenterCrop((224, 224)),\n","        T.ToTensor(),\n","        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","])\n","\n","batch_size = 37\n","num_workers = 2\n","freq_threshold = 5\n","batch_first = True\n","pin_memory = True\n","dataset = Flickr(root_folder, csv_file, transform, freq_threshold)\n","pad_idx = dataset.vocab.stoi[\"<PAD>\"]\n","\n","data_size = len(dataset)\n","train_size = int(0.9 * data_size)\n","val_size = data_size - train_size\n","\n","train_set, val_set = torch.utils.data.Subset(dataset, range(0, train_size)), torch.utils.data.Subset(dataset, range(train_size, data_size))\n","\n","train_loader = DataLoader(train_set,\n","                            batch_size=batch_size,\n","                            pin_memory=pin_memory,\n","                            num_workers=num_workers,\n","                            shuffle=True,\n","                            collate_fn=CapCollat(pad_seq=pad_idx, batch_first=batch_first))\n","\n","val_loader = DataLoader(val_set,\n","                            batch_size=batch_size,\n","                            pin_memory=pin_memory,\n","                            num_workers=num_workers,\n","                            shuffle=False,\n","                            collate_fn=CapCollat(pad_seq=pad_idx, batch_first=batch_first))"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-01-31T12:44:37.725203Z","iopub.status.busy":"2024-01-31T12:44:37.724890Z","iopub.status.idle":"2024-01-31T12:44:37.731861Z","shell.execute_reply":"2024-01-31T12:44:37.731005Z","shell.execute_reply.started":"2024-01-31T12:44:37.725175Z"},"trusted":true},"outputs":[],"source":["val_set_start = data_size - val_size - 1\n","val_set_end = data_size - 1\n","\n","all_labels = dataset.to_list()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-31T12:45:18.873873Z","iopub.status.busy":"2024-01-31T12:45:18.873581Z","iopub.status.idle":"2024-01-31T12:45:21.345007Z","shell.execute_reply":"2024-01-31T12:45:21.344172Z","shell.execute_reply.started":"2024-01-31T12:45:18.873849Z"},"trusted":true},"outputs":[],"source":["for idx in range(0, 100, 10):\n","    image, _ = dataset[idx + val_size]\n","    \n","    label = all_labels[idx + val_size]\n","\n","    image = image.permute(1,2,0)\n","    \n","    plt.imshow(image)\n","    plt.title(label)\n","\n","    plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["# Pre-Trained CNN Encoder: VGG16 "]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-01-31T12:58:17.237085Z","iopub.status.busy":"2024-01-31T12:58:17.236735Z","iopub.status.idle":"2024-01-31T12:58:17.243461Z","shell.execute_reply":"2024-01-31T12:58:17.242559Z","shell.execute_reply.started":"2024-01-31T12:58:17.237058Z"},"trusted":true},"outputs":[],"source":["class EncoderCNN(nn.Module):\n","    def __init__(self, embed_size=256):\n","        super(EncoderCNN, self).__init__()\n","\n","        vgg = vgg16(weights=VGG16_Weights.IMAGENET1K_FEATURES)\n","        \n","        for param in vgg.parameters():\n","            param.requires_grad = False\n","            \n","        feature_extractor = list(vgg.children())[:-1]\n","            \n","        embedding_layer = nn.Linear(512 * 7 * 7, embed_size)\n","\n","        self.encoder = nn.Sequential(*feature_extractor,\n","                                 nn.Flatten(),\n","                                 embedding_layer)\n","       \n","    def forward(self, image):\n","        encoded_image = self.encoder(image)\n","  \n","        return encoded_image"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-31T12:45:33.013009Z","iopub.status.busy":"2024-01-31T12:45:33.012693Z","iopub.status.idle":"2024-01-31T12:45:37.523922Z","shell.execute_reply":"2024-01-31T12:45:37.523334Z","shell.execute_reply.started":"2024-01-31T12:45:33.012978Z"},"trusted":true},"outputs":[],"source":["summary(EncoderCNN(256), (3, 224, 224))"]},{"cell_type":"markdown","metadata":{},"source":["# RNN Decoder: Vanilla RNN Module"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-01-31T12:45:40.026324Z","iopub.status.busy":"2024-01-31T12:45:40.025978Z","iopub.status.idle":"2024-01-31T12:45:40.032709Z","shell.execute_reply":"2024-01-31T12:45:40.031898Z","shell.execute_reply.started":"2024-01-31T12:45:40.026298Z"},"trusted":true},"outputs":[],"source":["class DecoderRNN(nn.Module):\n","    def __init__(self, embed_size, vocab_size, hidden_size, num_layers):\n","        super(DecoderRNN, self).__init__()\n","\n","        self.embedding = nn.Embedding(vocab_size, embed_size)\n","        self.rnn = nn.RNN(embed_size, hidden_size, num_layers, batch_first=True)\n","        self.linear = nn.Linear(hidden_size, vocab_size)\n","\n","        self.features2hidden = nn.Linear(embed_size, hidden_size)\n","\n","    def forward(self, features, captions):\n","        captions_embed = self.embedding(captions)\n","\n","        initial_hidden_state = self.features2hidden(features).unsqueeze(0).repeat(self.rnn.num_layers, 1, 1)\n","\n","        output, _ = self.rnn(captions_embed, initial_hidden_state)\n","        output = self.linear(output)\n","\n","        return output\n"]},{"cell_type":"code","execution_count":68,"metadata":{"execution":{"iopub.execute_input":"2024-01-31T13:22:10.611553Z","iopub.status.busy":"2024-01-31T13:22:10.611222Z","iopub.status.idle":"2024-01-31T13:22:10.619500Z","shell.execute_reply":"2024-01-31T13:22:10.618610Z","shell.execute_reply.started":"2024-01-31T13:22:10.611530Z"},"trusted":true},"outputs":[],"source":["class ImageCap(nn.Module):\n","    def __init__(self, embed_size, vocab_size, hidden_size, num_layers):\n","        super(ImageCap, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","        \n","        self.encoderCNN = EncoderCNN(embed_size)\n","        self.decoderRNN = DecoderRNN(embed_size, vocab_size, hidden_size, num_layers)\n","    \n","    def forward(self, images, captions):\n","        x = self.encoderCNN(images)\n","        x = self.decoderRNN(x, captions)\n","        \n","        return x\n","    \n","    def caption(self, image, vocabulary, maxlength=50):\n","        result_caption = []\n","\n","        with torch.no_grad():\n","            x = self.encoderCNN(image).unsqueeze(0)\n","            states = None\n","\n","            for _ in range(maxlength):\n","                hiddens, states = self.decoderRNN.rnn(x, states)\n","                output = self.decoderRNN.linear(hiddens.squeeze(0))\n","                predicted = output.argmax(1)\n","                result_caption.append(predicted.item())\n","                x = self.decoderRNN.embedding(predicted).unsqueeze(0)\n","\n","                if vocabulary.itos[predicted.item()] == \"<EOS>\":\n","                    break\n","\n","        return [vocabulary.itos[i] for i in result_caption]\n"]},{"cell_type":"markdown","metadata":{},"source":["# Training and Evaluating the Image Captioning Model: VGG16 + Multi-layer Vanilla RNN"]},{"cell_type":"markdown","metadata":{},"source":["### Setting Hyperparameters"]},{"cell_type":"code","execution_count":47,"metadata":{"execution":{"iopub.execute_input":"2024-01-31T13:13:27.616567Z","iopub.status.busy":"2024-01-31T13:13:27.616160Z","iopub.status.idle":"2024-01-31T13:13:27.620796Z","shell.execute_reply":"2024-01-31T13:13:27.619901Z","shell.execute_reply.started":"2024-01-31T13:13:27.616534Z"},"trusted":true},"outputs":[],"source":["num_epochs = 4\n","enc_dim = 2048\n","embed_size = 224\n","hidden_size = 512\n","num_layers = 1\n","learning_rate = 3e-4"]},{"cell_type":"markdown","metadata":{},"source":["### Setting the Vocabulary"]},{"cell_type":"code","execution_count":48,"metadata":{"execution":{"iopub.execute_input":"2024-01-31T13:13:30.941922Z","iopub.status.busy":"2024-01-31T13:13:30.941590Z","iopub.status.idle":"2024-01-31T13:13:30.945782Z","shell.execute_reply":"2024-01-31T13:13:30.945008Z","shell.execute_reply.started":"2024-01-31T13:13:30.941899Z"},"trusted":true},"outputs":[],"source":["vocab = dataset.vocab\n","vocab_size = len(vocab)"]},{"cell_type":"markdown","metadata":{},"source":["### Configuring Models"]},{"cell_type":"code","execution_count":69,"metadata":{"execution":{"iopub.execute_input":"2024-01-31T13:22:24.755613Z","iopub.status.busy":"2024-01-31T13:22:24.754987Z","iopub.status.idle":"2024-01-31T13:22:26.447291Z","shell.execute_reply":"2024-01-31T13:22:26.446405Z","shell.execute_reply.started":"2024-01-31T13:22:24.755586Z"},"trusted":true},"outputs":[],"source":["criterion = nn.CrossEntropyLoss(ignore_index = vocab.stoi[\"<PAD>\"]).to(device)\n","\n","model = ImageCap(embed_size, vocab_size, hidden_size, num_layers).to(device)\n","\n","optimizer = optim.Adam(model.parameters(), lr = learning_rate)"]},{"cell_type":"markdown","metadata":{},"source":["### Training Models"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-31T12:46:11.558518Z","iopub.status.busy":"2024-01-31T12:46:11.558012Z","iopub.status.idle":"2024-01-31T12:46:22.341102Z","shell.execute_reply":"2024-01-31T12:46:22.340474Z","shell.execute_reply.started":"2024-01-31T12:46:11.558492Z"},"trusted":true},"outputs":[],"source":["!pip install pycocoevalcap"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-01-31T12:46:29.245287Z","iopub.status.busy":"2024-01-31T12:46:29.244958Z","iopub.status.idle":"2024-01-31T12:46:29.261444Z","shell.execute_reply":"2024-01-31T12:46:29.260801Z","shell.execute_reply.started":"2024-01-31T12:46:29.245261Z"},"trusted":true},"outputs":[],"source":["from pycocoevalcap.bleu.bleu import Bleu\n","from pycocoevalcap.meteor.meteor import Meteor\n","from pycocoevalcap.rouge.rouge import Rouge\n","from pycocoevalcap.cider.cider import Cider\n","\n","class Scorer():\n","    def __init__(self,ref,gt):\n","        self.ref = ref\n","        self.gt = gt\n","\n","        self.word_based_scorers = [\n","            (Bleu(4), [\"Bleu_1\", \"Bleu_2\", \"Bleu_3\", \"Bleu_4\"]),\n","            (Meteor(),\"METEOR\"),\n","            (Rouge(), \"ROUGE_L\"),\n","            (Cider(), \"CIDEr\"),\n","            ]\n","\n","    def compute_scores(self):\n","        total_scores = {\n","            \"Bleu1\":[],\n","            \"Bleu2\":[],\n","            \"Bleu3\":[],\n","            \"Bleu4\":[],\n","            \"METEOR\":[],\n","            \"ROUGE_L\":[],\n","            \"CIDEr\":[],\n","        }\n","\n","        for scorer, method in self.word_based_scorers:\n","            score, scores = scorer.compute_score(self.ref, self.gt)\n","    \n","            if type(method) == list:\n","                total_scores[\"Bleu1\"].append(score[0])\n","                total_scores[\"Bleu2\"].append(score[1])\n","                total_scores[\"Bleu3\"].append(score[2])\n","                total_scores[\"Bleu4\"].append(score[3])\n","\n","            else:\n","                total_scores[method].append(score)\n","\n","        return total_scores\n","    \n","    def compute_scores_iterative(self):\n","        total_scores = {\n","            \"Bleu1\":[],\n","            \"Bleu2\":[],\n","            \"Bleu3\":[],\n","            \"Bleu4\":[],\n","            \"METEOR\":[],\n","            \"ROUGE_L\":[],\n","            \"CIDEr\":[],\n","            \"SPICE\":[]\n","        \n","        }\n","\n","        for key in self.ref:\n","            curr_ref = {key:self.ref[key]}\n","            curr_gt = {key:self.gt[key]}\n","\n","            for scorer, method in self.word_based_scorers:\n","                score, _ = scorer.compute_score(curr_ref, curr_gt)\n","                if type(method) == list:\n","                    total_scores[\"Bleu1\"].append(score[0])\n","                    total_scores[\"Bleu2\"].append(score[1])\n","                    total_scores[\"Bleu3\"].append(score[2])\n","                    total_scores[\"Bleu4\"].append(score[3])\n","\n","                else:\n","                    total_scores[method].append(score)\n","\n","        return total_scores"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_losses = list()\n","val_losses = list()\n","\n","cumulative_bleu_scores = list()\n","\n","cider_scores = list()\n","\n","meteor_scores = list()\n","\n","rougel_scores = list()\n","\n","bleu1_scores = list()\n","bleu2_scores = list()\n","bleu3_scores = list()\n","bleu4_scores = list()\n","\n","val_iter = iter(val_loader)\n","\n","for epoch in range(num_epochs):\n","    model.train()\n","\n","    for batch_idx, (images, captions) in enumerate(train_loader):\n","        images = images.to(device)\n","        captions = captions.to(device)\n","        \n","        train_score = model(images, captions)\n","\n","        optimizer.zero_grad()\n","        \n","        train_loss = criterion(train_score.view(-1, vocab_size), captions.view(-1))\n","        train_losses.append(train_loss.item())\n","        \n","        train_loss.backward()\n","        \n","        optimizer.step()\n","\n","    with torch.no_grad():\n","        model.eval()\n","\n","        idx = val_set_start\n","        \n","        refs = {}\n","        hyps = {}\n","\n","        while idx <= val_set_end:\n","            if idx % batch_size == 0:\n","                batch = next(val_iter)\n","\n","            images, captions = batch\n","\n","            images = images.to_device()\n","            captions = captions.to_device()\n","\n","            val_score = model(images, captions)\n","\n","            val_loss = criterion(val_score.view(-1, vocab_size), captions.view(-1))\n","\n","            val_losses.append(val_loss.item())\n","            \n","            for image in images:\n","                val_pred = model.caption(image.unsqueeze(0), vocab)\n","                \n","                candidate = ' '.join(val_pred)\n","\n","                hyps[idx] = candidate\n","\n","                label = all_labels[idx]\n","\n","                refs[idx] = label\n","\n","                cumulative_bleu_score = sentence_bleu(label, candidate, weights=(0.25, 0.25, 0.25, 0.25))\n","                cumulative_bleu_scores.append(cumulative_bleu_score)\n","            \n","                idx += 1\n","            \n","        metrics = Scorer(refs, hyps).compute_scores()\n","    \n","        bleu1_scores.append(metrics['Bleu_1'])\n","        bleu2_scores.append(metrics['Bleu_2'])\n","        bleu3_scores.append(metrics['Bleu_3'])\n","        bleu4_scores.append(metrics['Bleu_4'])\n","\n","        cider_scores.append(metrics['CIDEr'])\n","\n","        rougel_scores.append(metrics['ROUGE_L'])\n","\n","        meteor_scores.append(metrics['METEOR'])\n","        \n","        print(f\"Epoch [{epoch+1}/{num_epochs}] Batch [{batch_idx+1}/{len(train_loader)}] | Training loss: {train_loss.item()} Validation loss: {val_loss.item()} | Cumulative BLEU Score: {cumulative_bleu_score} CIDEr Score: {metrics['CIDEr']} METEOR Score: {metrics['METEOR']} ROUGE_L: {metrics['ROUGE_L']} \\n\")\n","        \n","        torch.save(model.state_dict(), f'/kaggle/working/ImageCap_{epoch+1}.pth')\n"]},{"cell_type":"markdown","metadata":{},"source":["### Plotting Losses and Scores"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.figure(0)\n","plt.plot(train_losses, label = 'Training loss')\n","plt.plot(val_losses, label = 'Validation loss')\n","plt.ylabel('Cross Entropy Loss')\n","plt.legend()\n","plt.savefig(f'/kaggle/working/{freq_threshold}_{batch_size}_{hidden_size}_{num_epochs}_losses.png')\n","\n","plt.figure(1)\n","plt.plot(bleu1_scores, label = 'BLEU 1')\n","plt.plot(bleu2_scores, label = 'BLEU 2')\n","plt.plot(bleu3_scores, label = 'BLEU 3')\n","plt.plot(bleu4_scores, label = 'BLEU 4')\n","plt.ylabel('BLEU Scores')\n","plt.legend()\n","plt.savefig(f'/kaggle/working/{freq_threshold}_{batch_size}_{hidden_size}_{num_epochs}_bleu_scores.png')\n","        \n","plt.figure(2)\n","plt.plot(cumulative_bleu_scores, label = 'Cumulative BLEU SCORE')\n","plt.plot(cider_scores, label = 'CIDEr SCORE')\n","plt.plot(meteor_scores, label = 'METEOR SCORE')\n","plt.plot(rougel_scores, label = 'ROUGE_L SCORE')\n","plt.ylabel('Scores')\n","plt.legend()\n","plt.savefig(f'/kaggle/working/{freq_threshold}_{batch_size}_{hidden_size}_{num_epochs}_scores.png')"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":31296,"sourceId":39911,"sourceType":"datasetVersion"}],"dockerImageVersionId":30635,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
