{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Artificial Neural Networks - 3rd Assignment - Armin Abbasi Najarzadeh"]},{"cell_type":"markdown","metadata":{},"source":["# Importing Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-06T12:29:26.655392Z","iopub.status.busy":"2024-02-06T12:29:26.655048Z","iopub.status.idle":"2024-02-06T12:29:33.922962Z","shell.execute_reply":"2024-02-06T12:29:33.922140Z","shell.execute_reply.started":"2024-02-06T12:29:26.655367Z"},"trusted":true},"outputs":[],"source":["import os\n","import spacy\n","import torch\n","import numpy as np\n","import pandas as pd\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import torchvision.transforms as T\n","\n","from PIL import Image\n","from torch import nn\n","from pathlib import Path\n","from matplotlib import pyplot as plt\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","from torch.nn.utils.rnn import pad_sequence\n","from torchvision.models import vgg16, VGG16_Weights"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-06T12:30:09.475605Z","iopub.status.busy":"2024-02-06T12:30:09.475032Z","iopub.status.idle":"2024-02-06T12:30:36.354743Z","shell.execute_reply":"2024-02-06T12:30:36.353589Z","shell.execute_reply.started":"2024-02-06T12:30:09.475572Z"},"trusted":true},"outputs":[],"source":["!pip install torchsummary\n","!pip install pycocoevalcap\n","\n","from torchsummary import summary\n","from pycocoevalcap.bleu.bleu import Bleu\n","from pycocoevalcap.meteor.meteor import Meteor\n","from pycocoevalcap.rouge.rouge import Rouge\n","from pycocoevalcap.cider.cider import Cider"]},{"cell_type":"markdown","metadata":{},"source":["### Setting Device"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-02-06T12:30:41.271596Z","iopub.status.busy":"2024-02-06T12:30:41.271230Z","iopub.status.idle":"2024-02-06T12:30:41.278013Z","shell.execute_reply":"2024-02-06T12:30:41.277119Z","shell.execute_reply.started":"2024-02-06T12:30:41.271564Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Device: cuda:0\n"]}],"source":["if torch.cuda.is_available():\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = True\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","print(\"Device:\", device)"]},{"cell_type":"markdown","metadata":{},"source":["### Loading Spacy English"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-02-06T12:30:43.571334Z","iopub.status.busy":"2024-02-06T12:30:43.570968Z","iopub.status.idle":"2024-02-06T12:30:44.737313Z","shell.execute_reply":"2024-02-06T12:30:44.736505Z","shell.execute_reply.started":"2024-02-06T12:30:43.571307Z"},"trusted":true},"outputs":[],"source":["spacy_eng = spacy.load(\"en_core_web_sm\")"]},{"cell_type":"markdown","metadata":{},"source":["# Preparing Dataset"]},{"cell_type":"markdown","metadata":{},"source":["## Custom Vocabulary"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-02-06T12:30:46.788258Z","iopub.status.busy":"2024-02-06T12:30:46.787880Z","iopub.status.idle":"2024-02-06T12:30:46.797852Z","shell.execute_reply":"2024-02-06T12:30:46.796882Z","shell.execute_reply.started":"2024-02-06T12:30:46.788228Z"},"trusted":true},"outputs":[],"source":["class Vocabulary:\n","    def __init__(self, freq_threshold):\n","        self.freq_threshold = freq_threshold\n","        self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n","        self.stoi = {v: k for k, v in self.itos.items()}\n","    \n","    def __len__(self):\n","        return len(self.itos)\n","  \n","    def build_vocab(self, sentence_list):\n","        freqs = {}\n","        idx = 4\n","\n","        for sentence in sentence_list:\n","            sentence = str(sentence)\n","\n","            for word in self.tokenize(sentence):\n","                freqs[word] = freqs.get(word, 0) + 1\n","\n","                if freqs[word] == self.freq_threshold:\n","                    self.itos[idx] = word\n","                    self.stoi[word] = idx\n","                    \n","                    idx += 1\n","\n","    def numericalize(self, sentence):\n","        tokens = self.tokenize(sentence)\n","        result = []\n","\n","        for token in tokens:\n","            result.append(self.stoi.get(token, self.stoi[\"<UNK>\"]))\n","\n","        return result\n","    \n","    @staticmethod\n","    def tokenize(sentence):\n","        return [token.text.lower() for token in spacy_eng.tokenizer(str(sentence))]"]},{"cell_type":"markdown","metadata":{},"source":["## Custom Dataset"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-02-06T12:30:47.051656Z","iopub.status.busy":"2024-02-06T12:30:47.050905Z","iopub.status.idle":"2024-02-06T12:30:47.061593Z","shell.execute_reply":"2024-02-06T12:30:47.060521Z","shell.execute_reply.started":"2024-02-06T12:30:47.051624Z"},"trusted":true},"outputs":[],"source":["class Flickr(Dataset):\n","    def __init__(self, root_dir, caption_path, transform, freq_threshold=5):\n","        self.freq_threshold = freq_threshold\n","        self.transform = transform\n","        self.root_dir = root_dir\n","    \n","        self.df = pd.read_csv(caption_path, delimiter='|')\n","        \n","        self.images = self.df[\"image_name\"]\n","        self.captions = self.df[\" comment\"]\n","        \n","        self.vocab = Vocabulary(freq_threshold)\n","        \n","        self.vocab.build_vocab(self.captions.tolist())\n","        \n","    def __len__(self):\n","        return len(self.df)\n","    \n","    def __getitem__(self, index):\n","        image, caption = self.images[index], self.captions[index]\n","        \n","        image = Image.open(os.path.join(self.root_dir, image)).convert(\"RGB\")\n","        \n","        image = self.transform(image)\n","        \n","        numericalized_caption = [self.vocab.stoi[\"<SOS>\"]]\n","        \n","        numericalized_caption += self.vocab.numericalize(caption)\n","        \n","        numericalized_caption.append(self.vocab.stoi[\"<EOS>\"])\n","        \n","        return image, torch.tensor(numericalized_caption)\n","    \n","    def get_label(self, index):\n","        _, caption = self[index]\n","    \n","        label = [self.vocab.itos[token] for token in caption.tolist()]\n","\n","        return label\n","        "]},{"cell_type":"markdown","metadata":{},"source":["## Custom Caption Collat"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-02-06T12:30:47.294944Z","iopub.status.busy":"2024-02-06T12:30:47.294337Z","iopub.status.idle":"2024-02-06T12:30:47.300619Z","shell.execute_reply":"2024-02-06T12:30:47.299675Z","shell.execute_reply.started":"2024-02-06T12:30:47.294906Z"},"trusted":true},"outputs":[],"source":["class CapCollat:\n","    def __init__(self, pad_idx):\n","        self.pad_idx = pad_idx\n","\n","    def __call__(self, batch):\n","        imgs = [item[0].unsqueeze(0) for item in batch]\n","        imgs = torch.cat(imgs, dim=0)\n","        targets = [item[1] for item in batch]\n","        targets = pad_sequence(targets, batch_first=False, padding_value=self.pad_idx)\n","\n","        return imgs, targets"]},{"cell_type":"markdown","metadata":{},"source":["## Custom Scorer "]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-02-06T12:30:47.771139Z","iopub.status.busy":"2024-02-06T12:30:47.770723Z","iopub.status.idle":"2024-02-06T12:30:47.779047Z","shell.execute_reply":"2024-02-06T12:30:47.778144Z","shell.execute_reply.started":"2024-02-06T12:30:47.771079Z"},"trusted":true},"outputs":[],"source":["class Scorer():\n","    def __init__(self, references, candidates):\n","        self.references = references\n","        self.candidates = candidates\n","\n","        self.word_based_scorers = [\n","            (Bleu(4), [\"BLEU 1\", \"BLEU 2\", \"BLEU 3\", \"BLEU 4\"]),\n","            (Meteor(),\"METEOR\"),\n","            (Rouge(), \"ROUGE_L\"),\n","            (Cider(), \"CIDEr\"),\n","            ]\n","\n","    def compute_scores(self):\n","        total_scores = {}\n","\n","        for scorer, method in self.word_based_scorers:\n","            score, _ = scorer.compute_score(self.references, self.candidates)\n","    \n","            if type(method) is list:\n","                total_scores[\"BLEU 1\"] = score[0]\n","                total_scores[\"BLEU 2\"] = score[1]\n","                total_scores[\"BLEU 3\"] = score[2]\n","                total_scores[\"BLEU 4\"] = score[3]\n","\n","            else:\n","                total_scores[method] = score\n","\n","        return total_scores"]},{"cell_type":"markdown","metadata":{},"source":["## Loading and Testing the Dataset "]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-02-06T12:30:50.231220Z","iopub.status.busy":"2024-02-06T12:30:50.230842Z","iopub.status.idle":"2024-02-06T12:31:34.167786Z","shell.execute_reply":"2024-02-06T12:31:34.166929Z","shell.execute_reply.started":"2024-02-06T12:30:50.231189Z"},"trusted":true},"outputs":[],"source":["root_folder = \"/kaggle/input/flickr-image-dataset/flickr30k_images/flickr30k_images/\"\n","csv_file = \"/kaggle/input/flickr-image-dataset/flickr30k_images/results.csv\"\n","\n","transform = T.Compose(\n","        [\n","            T.Resize((356, 356)),\n","            T.RandomCrop((299, 299)),\n","            T.ToTensor(),\n","            T.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n","        ]\n","    )\n","\n","batch_size = 32\n","num_workers = 2\n","freq_threshold = 5\n","batch_first = True\n","pin_memory = True\n","\n","dataset = Flickr(root_folder, csv_file, transform, freq_threshold)\n","pad_idx = dataset.vocab.stoi[\"<PAD>\"]\n","\n","data_size = len(dataset)\n","train_size = int(0.9 * data_size)\n","val_size = data_size - train_size\n","\n","val_set_start = train_size\n","val_set_end = train_size + val_size - 1\n","\n","train_set, val_set = torch.utils.data.Subset(dataset, range(0, train_size)), torch.utils.data.Subset(dataset, range(train_size, data_size))\n","\n","train_loader = DataLoader(train_set,\n","                            batch_size=batch_size,\n","                            pin_memory=pin_memory,\n","                            num_workers=num_workers,\n","                            shuffle=True,\n","                            collate_fn=CapCollat(pad_idx))\n","\n","val_loader = DataLoader(val_set,\n","                            batch_size=batch_size,\n","                            pin_memory=pin_memory,\n","                            num_workers=num_workers,\n","                            shuffle=False,\n","                            collate_fn=CapCollat(pad_idx))"]},{"cell_type":"markdown","metadata":{},"source":["### Setting Vocabulary"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-06T12:31:38.091528Z","iopub.status.busy":"2024-02-06T12:31:38.091157Z","iopub.status.idle":"2024-02-06T12:31:38.096733Z","shell.execute_reply":"2024-02-06T12:31:38.095838Z","shell.execute_reply.started":"2024-02-06T12:31:38.091498Z"},"trusted":true},"outputs":[],"source":["vocab = dataset.vocab\n","vocab_size = len(vocab)\n","\n","print(f\"vocabulary size: {vocab_size}\")"]},{"cell_type":"markdown","metadata":{},"source":["### Testing Scores"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-06T12:31:41.818128Z","iopub.status.busy":"2024-02-06T12:31:41.817749Z","iopub.status.idle":"2024-02-06T12:31:53.210755Z","shell.execute_reply":"2024-02-06T12:31:53.209697Z","shell.execute_reply.started":"2024-02-06T12:31:41.818077Z"},"trusted":true},"outputs":[],"source":["dummy_references = {}\n","dummy_candidates = {}\n","\n","for idx in range(100):\n","    label = dataset.get_label(np.random.randint(0, data_size - 1))\n","    \n","    cut = np.random.randint(1, len(label) - 1)\n","    \n","    dummy_references[idx] = [\" \".join(label)]\n","    dummy_candidates[idx] = [\" \".join(label[:cut])]\n","    \n","test_metrics = Scorer(dummy_references, dummy_candidates).compute_scores()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-06T12:31:55.795236Z","iopub.status.busy":"2024-02-06T12:31:55.794854Z","iopub.status.idle":"2024-02-06T12:31:55.801776Z","shell.execute_reply":"2024-02-06T12:31:55.800815Z","shell.execute_reply.started":"2024-02-06T12:31:55.795206Z"},"trusted":true},"outputs":[],"source":["cumulative_bleu_score = np.exp((np.log(test_metrics[\"BLEU 1\"]) \n","                                + np.log(test_metrics[\"BLEU 2\"]) \n","                                + np.log(test_metrics[\"BLEU 3\"]) \n","                                + np.log(test_metrics[\"BLEU 4\"])) / 4)\n","\n","print(f\"Cumulative BLEU:\\t{cumulative_bleu_score:.6f}\")\n","\n","print(f\"CIDEr:\\t\\t\\t{test_metrics['CIDEr']:.6f}\")\n","\n","print(f\"ROUGE_L:\\t\\t{test_metrics['ROUGE_L']:.6f}\")\n","\n","print(f\"METEOR:\\t\\t\\t{test_metrics['METEOR']:.6f}\")"]},{"cell_type":"markdown","metadata":{},"source":["### Testing Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-06T12:31:59.283148Z","iopub.status.busy":"2024-02-06T12:31:59.282439Z","iopub.status.idle":"2024-02-06T12:32:02.791453Z","shell.execute_reply":"2024-02-06T12:32:02.790563Z","shell.execute_reply.started":"2024-02-06T12:31:59.283110Z"},"trusted":true},"outputs":[],"source":["for _ in range(10):\n","    idx = np.random.randint(0, data_size - 1)\n","\n","    image, _ = dataset[idx]\n","    \n","    label = \" \".join(dataset.get_label(idx)[1:-1])\n","\n","    image = image.permute(1, 2, 0)\n","    \n","    plt.title(label)\n","    plt.imshow(image)\n","\n","    plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["# Custom Models"]},{"cell_type":"markdown","metadata":{},"source":["## Encoder"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-02-06T12:33:12.477515Z","iopub.status.busy":"2024-02-06T12:33:12.477111Z","iopub.status.idle":"2024-02-06T12:33:12.485421Z","shell.execute_reply":"2024-02-06T12:33:12.484349Z","shell.execute_reply.started":"2024-02-06T12:33:12.477484Z"},"trusted":true},"outputs":[],"source":["class Encoder(nn.Module):\n","        def __init__(self, model, embed_size: int):\n","            super(Encoder, self).__init__() \n","\n","            self.model = model\n","            \n","            for param in self.model.parameters():\n","                param.requires_grad = False\n","            \n","            embedding_layer = nn.Linear(self.model.classifier[-1].out_features, embed_size) \n","            \n","            self.encoder = nn.Sequential(self.model,\n","                                         embedding_layer,\n","                                         nn.Dropout(p=0.5),\n","                                         nn.ReLU(inplace=True),)\n","        \n","        def forward(self, image):\n","            encoded_image = self.encoder(image)\n","    \n","            return encoded_image"]},{"cell_type":"markdown","metadata":{},"source":["## Decoder"]},{"cell_type":"code","execution_count":43,"metadata":{"execution":{"iopub.execute_input":"2024-02-06T16:37:49.733499Z","iopub.status.busy":"2024-02-06T16:37:49.733069Z","iopub.status.idle":"2024-02-06T16:37:49.750288Z","shell.execute_reply":"2024-02-06T16:37:49.749276Z","shell.execute_reply.started":"2024-02-06T16:37:49.733468Z"},"trusted":true},"outputs":[],"source":["class Decoder(nn.Module):\n","    class Attention(nn.Module):\n","        def __init__(self, hidden_size: int):\n","            super(Decoder.Attention, self).__init__()\n","            self.hidden_size = hidden_size\n","\n","            self.fc1 = nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n","            self.fc2 = nn.Linear(self.hidden_size * 2, self.hidden_size, bias=False)\n","\n","            self.softmax = nn.Softmax(dim=1)\n","\n","        def forward(self, hidden_states):\n","            score_first_part = self.fc1(hidden_states)\n","\n","            h_t = hidden_states[:,-1,:]\n","\n","            score = torch.bmm(score_first_part, h_t.unsqueeze(2)).squeeze(2)\n","\n","            attention_weights = self.softmax(score)\n","\n","            context_vector = torch.bmm(hidden_states.permute(0,2,1), attention_weights.unsqueeze(2)).squeeze(2)\n","\n","            pre_activation = torch.cat((context_vector, h_t), dim=1)\n","\n","            attention_vector = self.fc2(pre_activation)\n","            attention_vector = torch.tanh(attention_vector)\n","\n","            return attention_vector, attention_weights\n","        \n","    def __init__(self, model, embed_size: int, vocab_size: int, hidden_size: int, num_layers: int, attention: bool = False):\n","        super(Decoder, self).__init__()\n","\n","        self.attention = attention\n","        \n","        self.embedding = nn.Embedding(vocab_size, embed_size)\n","\n","        self.model = model(embed_size, hidden_size, num_layers)\n","            \n","        self.linear = nn.Linear(hidden_size, vocab_size)\n","        \n","        if self.attention:\n","            self.attention = self.Attention(hidden_size)\n","            self.linear = nn.Linear(embed_size + hidden_size, vocab_size)\n","\n","    def forward(self, features, captions):\n","        captions_embed = self.dropout(self.embedding(captions))\n","        \n","        captions_embed = torch.cat((features.unsqueeze(0), captions_embed), dim=0)\n","\n","        output, hidden_states = self.model(captions_embed)\n","\n","        if self.attention:\n","            output = self._pay_attention(output, hidden_states)\n","\n","        output = self.linear(output)\n","\n","        return output\n","    \n","    def _pay_attention(self, output, hidden_states):\n","        attention_weighted_encoding, _ = self.attention(hidden_states[0])\n","        attention_weighted_encoding = attention_weighted_encoding.unsqueeze(0).repeat(output.size(0), 1, 1)\n","        attention_weighted_encoding = attention_weighted_encoding.repeat(1, output.size(1), 1)\n","        \n","        return torch.cat([output, attention_weighted_encoding], dim=2)"]},{"cell_type":"markdown","metadata":{},"source":["## Encoder-Decoder"]},{"cell_type":"code","execution_count":50,"metadata":{"execution":{"iopub.execute_input":"2024-02-06T16:52:50.726517Z","iopub.status.busy":"2024-02-06T16:52:50.726047Z","iopub.status.idle":"2024-02-06T16:52:50.738064Z","shell.execute_reply":"2024-02-06T16:52:50.737178Z","shell.execute_reply.started":"2024-02-06T16:52:50.726483Z"},"trusted":true},"outputs":[],"source":["class EncoderDecoder(nn.Module):\n","    def __init__(self, encoder: Encoder, decoder: Decoder):\n","        super(EncoderDecoder, self).__init__()\n","    \n","        self.encoder = encoder\n","        self.decoder = decoder\n","    \n","    def forward(self, images, captions):\n","        features = self.encoder(images)\n","        output = self.decoder(features, captions)\n","        \n","        return output\n","    \n","    def caption(self, image, vocabulary, max_length=50):\n","        result_caption = []\n","        \n","        with torch.no_grad():\n","            features = self.encoder(image).unsqueeze(0)\n","            hidden_states = None\n","\n","            for _ in range(max_length):\n","                output, hidden_states = self.decoder.model(features, hidden_states)\n","                \n","                if self.decoder.attention:\n","                    output = self.decoder._pay_attention(output, hidden_states)\n","                \n","                output = self.decoder.linear(output.squeeze(0))\n","                \n","                pred = output.argmax(1)\n","                \n","                result_caption.append(pred.item())\n","                \n","                features = self.decoder.embedding(pred).unsqueeze(0)\n","\n","                if vocabulary.itos[pred.item()] == \"<EOS>\":\n","                    break\n","\n","        return [vocabulary.itos[idx] for idx in result_caption]\n"]},{"cell_type":"markdown","metadata":{},"source":["# Custom Trainer"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-02-06T12:33:19.328374Z","iopub.status.busy":"2024-02-06T12:33:19.327736Z","iopub.status.idle":"2024-02-06T12:33:19.357034Z","shell.execute_reply":"2024-02-06T12:33:19.356078Z","shell.execute_reply.started":"2024-02-06T12:33:19.328343Z"},"trusted":true},"outputs":[],"source":["class Trainer:\n","    def __init__(self, model, criterion, optimizer, train_loader, val_loader, num_epochs, save_path = None):\n","        self.model = model\n","        self.criterion = criterion\n","        self.optimizer = optimizer\n","\n","        self.train_loader = train_loader\n","        self.val_loader = val_loader\n","        \n","        self.num_epochs = num_epochs\n","\n","        self.save_path = save_path\n","        \n","        if self.save_path:\n","            self.save_path = Path(save_path)\n","\n","        self.train_losses = []\n","        self.val_losses = []\n","\n","        self.bleu1_scores = []\n","        self.bleu2_scores = []\n","        self.bleu3_scores = []\n","        self.bleu4_scores = []\n","\n","        self.cumulative_bleu_scores = []\n","\n","        self.cider_scores = []\n","\n","        self.meteor_scores = []\n","\n","        self.rougel_scores = []\n","\n","    def train_model(self):\n","        for epoch in range(self.num_epochs):\n","            self.model.train()\n","            \n","            running_loss = 0.0\n","\n","            for images, captions in train_loader:\n","                images, captions = images.to(device), captions.to(device)\n","                \n","                train_score = self.model(images, captions[:-1])\n","                \n","                loss = self.criterion(train_score.reshape(-1, train_score.shape[2]), captions.reshape(-1))\n","                running_loss += loss.item()\n","                \n","                self.optimizer.zero_grad()\n","                \n","                loss.backward(loss)\n","                \n","                self.optimizer.step()\n","                \n","            train_loss = running_loss / len(train_loader)\n","            self.train_losses.append(train_loss)\n","            \n","            with torch.no_grad():\n","                self.model.eval()\n","\n","                idx = val_set_start\n","                \n","                references = {}\n","                candidates = {}\n","                \n","                val_iter = iter(val_loader)\n","                \n","                running_loss = 0.0\n","                \n","                while idx < val_set_end:\n","                    batch = next(val_iter)\n","\n","                    images, captions = batch\n","\n","                    images, captions = images.to(device), captions.to(device)\n","\n","                    val_score = self.model(images, captions[:-1])\n","\n","                    loss = self.criterion(val_score.reshape(-1, val_score.shape[2]), captions.reshape(-1))\n","                    running_loss += loss.item()\n","                    \n","                    for image in images:\n","                        val_pred = self.model.caption(image.unsqueeze(0), vocab)\n","                        \n","                        candidate = \" \".join(val_pred)\n","\n","                        candidates[idx] = [candidate]\n","                        \n","                        label = \" \".join(dataset.get_label(idx))\n","\n","                        references[idx] = [label]\n","                        \n","                        idx += 1\n","                        \n","                val_loss = running_loss / len(val_loader)\n","                self.val_losses.append(val_loss)\n","            \n","            self._account_scores(references, candidates)\n","\n","            self._log(epoch)\n","\n","        if self.save_path:\n","            self.save_weights(self.save_path / Path(f\"model_{id(self.model)}.pth\"))\n","\n","    def get_train_losses(self):\n","        return self.train_losses\n","    \n","    def get_val_losses(self):\n","        return self.val_losses\n","    \n","    def get_scores(self):\n","        scores = {\n","                \"BLEU 1\": self.bleu1_scores,\n","                \"BLEU 2\": self.bleu2_scores,\n","                \"BLEU 3\": self.bleu3_scores,\n","                \"BLEU 4\": self.bleu4_scores,\n","                \"Cumulative BLEU\": self.cumulative_bleu_scores,\n","                \"METEOR\": self.meteor_scores,\n","                \"ROUGE_L\": self.rougel_scores,\n","                \"CIDEr\": self.cider_scores,\n","                }\n","        \n","        return scores\n","    \n","    def save_weights(self, save_path):\n","        torch.save(self.model.state_dict(), save_path)\n","\n","    def plot(self):\n","        plt.figure(0)\n","        plt.plot(self.train_losses, label = \"Training loss\")\n","        plt.plot(self.val_losses, label = \"Validation loss\")\n","        plt.ylabel(\"Cross Entropy Loss\")\n","        plt.legend()\n","\n","        if self.save_path:\n","            plt.savefig(self.save_path / Path(\"losses.png\"))\n","\n","        plt.figure(1)\n","        plt.plot(self.bleu1_scores, label = \"BLEU 1\")\n","        plt.plot(self.bleu2_scores, label = \"BLEU 2\")\n","        plt.plot(self.bleu3_scores, label = \"BLEU 3\")\n","        plt.plot(self.bleu4_scores, label = \"BLEU 4\")\n","        plt.ylabel(\"BLEU Scores\")\n","        plt.legend()\n","\n","        if self.save_path:\n","            plt.savefig(self.save_path / Path(\"bleu_scores.png\"))\n","                \n","        plt.figure(2)\n","        plt.plot(self.cumulative_bleu_scores, label = \"Cumulative BLEU\")\n","        plt.plot(self.cider_scores, label = \"CIDEr\")\n","        plt.plot(self.meteor_scores, label = \"METEOR\")\n","        plt.plot(self.rougel_scores, label = \"ROUGE_L\")\n","        plt.ylabel(\"Scores\")\n","        plt.legend()\n","\n","        if self.save_path:\n","            plt.savefig(self.save_path / Path(\"scores.png\"))\n","\n","        plt.show()\n","\n","    def _account_scores(self, references, candidates):\n","        metrics = Scorer(references, candidates).compute_scores()\n","\n","        self.bleu1_scores.append(metrics[\"BLEU 1\"])\n","        self.bleu2_scores.append(metrics[\"BLEU 2\"])\n","        self.bleu3_scores.append(metrics[\"BLEU 3\"])\n","        self.bleu4_scores.append(metrics[\"BLEU 4\"])\n","\n","        cumulative_bleu_score = np.exp((np.log(metrics[\"BLEU 1\"]) \n","                                        + np.log(metrics[\"BLEU 2\"]) \n","                                        + np.log(metrics[\"BLEU 3\"]) \n","                                        + np.log(metrics[\"BLEU 4\"])) / 4)\n","        \n","        self.cumulative_bleu_scores.append(cumulative_bleu_score)\n","\n","        self.cider_scores.append(metrics[\"CIDEr\"])\n","\n","        self.rougel_scores.append(metrics[\"ROUGE_L\"])\n","\n","        self.meteor_scores.append(metrics[\"METEOR\"])\n","\n","    def _log(self, epoch):\n","            log = \"\"\n","            log += f\" ---------------------------------------------------------------------------------------------------\\n\"\n","            log += f\"|     Epoch [{epoch + 1}/{self.num_epochs}]     |         Training Loss: {self.train_losses[-1]: .6f}         |      Validation Loss: {self.val_losses[-1]: .6f}  |\\n\"\n","            log += f\" ---------------------------------------------------------------------------------------------------\\n\"\n","            log += f\"|        Cumulative BLEU Score: {self.cumulative_bleu_scores[-1]: .6f}       |               CIDEr Score: {self.cider_scores[-1]: .6f}              |\\n\"\n","            log += f\" ---------------------------------------------------------------------------------------------------\\n\"\n","            log += f\"|        METEOR Score: {self.meteor_scores[-1]: .6f}                |               ROUGE_L Score: {self.rougel_scores[-1]: .6f}            |\\n\"\n","            log += f\" ---------------------------------------------------------------------------------------------------\\n\"\n","\n","            print(log)\n","        "]},{"cell_type":"markdown","metadata":{},"source":["# Training and Evaluating Models"]},{"cell_type":"markdown","metadata":{},"source":["## EncoderDecoder Model = VGG16 + RNN"]},{"cell_type":"markdown","metadata":{},"source":["### Setting Hyperparameters"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-02-06T12:33:20.631154Z","iopub.status.busy":"2024-02-06T12:33:20.630461Z","iopub.status.idle":"2024-02-06T12:33:20.635302Z","shell.execute_reply":"2024-02-06T12:33:20.634290Z","shell.execute_reply.started":"2024-02-06T12:33:20.631123Z"},"trusted":true},"outputs":[],"source":["num_epochs = 5\n","embed_size = 256\n","hidden_size = 256\n","num_layers = 1\n","learning_rate = 3e-4"]},{"cell_type":"markdown","metadata":{},"source":["### Configuring The Model"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-02-06T12:48:34.784265Z","iopub.status.busy":"2024-02-06T12:48:34.783498Z","iopub.status.idle":"2024-02-06T12:48:36.528800Z","shell.execute_reply":"2024-02-06T12:48:36.527763Z","shell.execute_reply.started":"2024-02-06T12:48:34.784232Z"},"trusted":true},"outputs":[],"source":["encoder = Encoder(vgg16(weights=VGG16_Weights.DEFAULT), embed_size).to(device)\n","decoder = Decoder(nn.RNN, embed_size, vocab_size, hidden_size, num_layers, attention=False).to(device)\n","\n","model = EncoderDecoder(encoder, decoder).to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-06T12:48:36.530872Z","iopub.status.busy":"2024-02-06T12:48:36.530517Z","iopub.status.idle":"2024-02-06T12:48:36.549950Z","shell.execute_reply":"2024-02-06T12:48:36.549092Z","shell.execute_reply.started":"2024-02-06T12:48:36.530840Z"},"trusted":true},"outputs":[],"source":["print(\"Encoder Model Summary:\")\n","summary(model.encoder, (3, 299, 299))"]},{"cell_type":"markdown","metadata":{},"source":["### Training The Model"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2024-02-06T12:48:54.477332Z","iopub.status.busy":"2024-02-06T12:48:54.476876Z","iopub.status.idle":"2024-02-06T12:48:54.483243Z","shell.execute_reply":"2024-02-06T12:48:54.482245Z","shell.execute_reply.started":"2024-02-06T12:48:54.477300Z"},"trusted":true},"outputs":[],"source":["criterion = nn.CrossEntropyLoss(ignore_index = vocab.stoi[\"<PAD>\"]).to(device)\n","\n","optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n","\n","save_path = \"/kaggle/working\"\n","\n","trainer = Trainer(model, criterion, optimizer, train_loader, val_loader, num_epochs, save_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-06T12:48:57.385879Z","iopub.status.busy":"2024-02-06T12:48:57.385057Z","iopub.status.idle":"2024-02-06T14:33:20.532175Z","shell.execute_reply":"2024-02-06T14:33:20.531113Z","shell.execute_reply.started":"2024-02-06T12:48:57.385847Z"},"trusted":true},"outputs":[],"source":["trainer.train_model()"]},{"cell_type":"markdown","metadata":{},"source":["### Plotting Cross-Validation and Scores"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-06T14:42:03.951211Z","iopub.status.busy":"2024-02-06T14:42:03.950248Z","iopub.status.idle":"2024-02-06T14:42:05.088536Z","shell.execute_reply":"2024-02-06T14:42:05.087629Z","shell.execute_reply.started":"2024-02-06T14:42:03.951173Z"},"trusted":true},"outputs":[],"source":["trainer.plot()"]},{"cell_type":"markdown","metadata":{},"source":["### *Loading Pre-Trained Parameters"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["load_path = \"/kaggle/working/model_132110981422032.pth\"\n","\n","model.load_state_dict(torch.load(load_path))\n","\n","model.eval()"]},{"cell_type":"markdown","metadata":{},"source":["### Inference"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-06T14:42:51.011899Z","iopub.status.busy":"2024-02-06T14:42:51.011523Z","iopub.status.idle":"2024-02-06T14:42:56.901899Z","shell.execute_reply":"2024-02-06T14:42:56.900911Z","shell.execute_reply.started":"2024-02-06T14:42:51.011871Z"},"trusted":true},"outputs":[],"source":["for _ in range(20):\n","    idx = np.random.randint(val_set_start, val_set_end - 1)\n","\n","    image, caption = dataset[idx]\n","    \n","    image = image.to(device)\n","\n","    prediction = ' '.join(model.caption(image.unsqueeze(0), vocab)[1:-1])\n","\n","    image = image.permute(1,2,0)\n","    \n","    plt.title(prediction)\n","    plt.imshow(image.cpu())\n","\n","    plt.show()    "]},{"cell_type":"markdown","metadata":{},"source":["## EncoderDecoder Model = VGG16 + LSTM"]},{"cell_type":"markdown","metadata":{},"source":["### Setting Hyperparameters"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2024-02-06T14:50:22.524492Z","iopub.status.busy":"2024-02-06T14:50:22.523437Z","iopub.status.idle":"2024-02-06T14:50:22.529062Z","shell.execute_reply":"2024-02-06T14:50:22.528120Z","shell.execute_reply.started":"2024-02-06T14:50:22.524457Z"},"trusted":true},"outputs":[],"source":["num_epochs = 5\n","embed_size = 256\n","hidden_size = 256\n","num_layers = 1\n","learning_rate = 3e-4"]},{"cell_type":"markdown","metadata":{},"source":["### Configuring The Model"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2024-02-06T14:50:27.396582Z","iopub.status.busy":"2024-02-06T14:50:27.395893Z","iopub.status.idle":"2024-02-06T14:50:29.032846Z","shell.execute_reply":"2024-02-06T14:50:29.032054Z","shell.execute_reply.started":"2024-02-06T14:50:27.396549Z"},"trusted":true},"outputs":[],"source":["encoder = Encoder(vgg16(weights=VGG16_Weights.DEFAULT), embed_size).to(device)\n","decoder = Decoder(nn.LSTM, embed_size, vocab_size, hidden_size, num_layers, attention=False).to(device)\n","\n","model = EncoderDecoder(encoder, decoder).to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-06T14:50:31.401584Z","iopub.status.busy":"2024-02-06T14:50:31.400719Z","iopub.status.idle":"2024-02-06T14:50:31.791711Z","shell.execute_reply":"2024-02-06T14:50:31.790762Z","shell.execute_reply.started":"2024-02-06T14:50:31.401550Z"},"trusted":true},"outputs":[],"source":["print(\"Encoder Model Summary:\")\n","summary(model.encoder, (3, 224, 224))"]},{"cell_type":"markdown","metadata":{},"source":["### Training The Model"]},{"cell_type":"code","execution_count":33,"metadata":{"execution":{"iopub.execute_input":"2024-02-06T14:50:36.946939Z","iopub.status.busy":"2024-02-06T14:50:36.946009Z","iopub.status.idle":"2024-02-06T14:50:36.953723Z","shell.execute_reply":"2024-02-06T14:50:36.952787Z","shell.execute_reply.started":"2024-02-06T14:50:36.946907Z"},"trusted":true},"outputs":[],"source":["criterion = nn.CrossEntropyLoss(ignore_index = vocab.stoi[\"<PAD>\"]).to(device)\n","\n","optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n","\n","save_path = \"/kaggle/working\"\n","\n","trainer = Trainer(model, criterion, optimizer, train_loader, val_loader, num_epochs, save_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-06T14:50:38.519066Z","iopub.status.busy":"2024-02-06T14:50:38.518684Z","iopub.status.idle":"2024-02-06T16:32:36.675406Z","shell.execute_reply":"2024-02-06T16:32:36.674306Z","shell.execute_reply.started":"2024-02-06T14:50:38.519038Z"},"trusted":true},"outputs":[],"source":["trainer.train_model()"]},{"cell_type":"markdown","metadata":{},"source":["### Plotting Cross-Validation and Scores"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-06T16:33:46.381952Z","iopub.status.busy":"2024-02-06T16:33:46.380978Z","iopub.status.idle":"2024-02-06T16:33:47.410036Z","shell.execute_reply":"2024-02-06T16:33:47.409168Z","shell.execute_reply.started":"2024-02-06T16:33:46.381906Z"},"trusted":true},"outputs":[],"source":["trainer.plot()"]},{"cell_type":"markdown","metadata":{},"source":["### *Loading Pre-Trained Parameters"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["load_path = \"/kaggle/working/model_132110195458160.pth\"\n","\n","model.load_state_dict(torch.load(load_path))\n","\n","model.eval()"]},{"cell_type":"markdown","metadata":{},"source":["### Inference"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-06T16:34:38.022173Z","iopub.status.busy":"2024-02-06T16:34:38.021450Z","iopub.status.idle":"2024-02-06T16:34:43.671064Z","shell.execute_reply":"2024-02-06T16:34:43.670141Z","shell.execute_reply.started":"2024-02-06T16:34:38.022136Z"},"trusted":true},"outputs":[],"source":["for _ in range(20):\n","    idx = np.random.randint(val_set_start, val_set_end - 1)\n","\n","    image, caption = dataset[idx]\n","    \n","    image = image.to(device)\n","\n","    prediction = ' '.join(model.caption(image.unsqueeze(0), vocab)[1:-1])\n","\n","    image = image.permute(1,2,0)\n","    \n","    plt.title(prediction)\n","    plt.imshow(image.cpu())\n","\n","    plt.show()   "]},{"cell_type":"markdown","metadata":{},"source":["## EncoderDecoder Model = VGG16 + LSTM with Attention"]},{"cell_type":"markdown","metadata":{},"source":["### Setting Hyperparameters"]},{"cell_type":"code","execution_count":51,"metadata":{"execution":{"iopub.execute_input":"2024-02-06T16:52:58.054076Z","iopub.status.busy":"2024-02-06T16:52:58.053700Z","iopub.status.idle":"2024-02-06T16:52:58.058866Z","shell.execute_reply":"2024-02-06T16:52:58.057752Z","shell.execute_reply.started":"2024-02-06T16:52:58.054048Z"},"trusted":true},"outputs":[],"source":["num_epochs = 5\n","embed_size = 256\n","hidden_size = 256\n","num_layers = 1\n","learning_rate = 3e-4"]},{"cell_type":"markdown","metadata":{},"source":["### Configuring The Model"]},{"cell_type":"code","execution_count":52,"metadata":{"execution":{"iopub.execute_input":"2024-02-06T16:52:59.343926Z","iopub.status.busy":"2024-02-06T16:52:59.343576Z","iopub.status.idle":"2024-02-06T16:53:01.042468Z","shell.execute_reply":"2024-02-06T16:53:01.041471Z","shell.execute_reply.started":"2024-02-06T16:52:59.343900Z"},"trusted":true},"outputs":[],"source":["encoder = Encoder(vgg16(weights=VGG16_Weights.DEFAULT), embed_size).to(device)\n","decoder = Decoder(nn.LSTM, embed_size, vocab_size, hidden_size, num_layers, attention=True).to(device)\n","\n","model = EncoderDecoder(encoder, decoder).to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-06T16:53:02.896516Z","iopub.status.busy":"2024-02-06T16:53:02.895668Z","iopub.status.idle":"2024-02-06T16:53:02.914709Z","shell.execute_reply":"2024-02-06T16:53:02.913768Z","shell.execute_reply.started":"2024-02-06T16:53:02.896480Z"},"trusted":true},"outputs":[],"source":["print(\"Encoder Model Summary:\")\n","summary(model.encoder, (3, 224, 224))"]},{"cell_type":"markdown","metadata":{},"source":["### Training The Model"]},{"cell_type":"code","execution_count":54,"metadata":{"execution":{"iopub.execute_input":"2024-02-06T16:53:05.784746Z","iopub.status.busy":"2024-02-06T16:53:05.783800Z","iopub.status.idle":"2024-02-06T16:53:05.790499Z","shell.execute_reply":"2024-02-06T16:53:05.789654Z","shell.execute_reply.started":"2024-02-06T16:53:05.784714Z"},"trusted":true},"outputs":[],"source":["criterion = nn.CrossEntropyLoss(ignore_index = vocab.stoi[\"<PAD>\"]).to(device)\n","\n","optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n","\n","save_path = \"/kaggle/working\"\n","\n","trainer = Trainer(model, criterion, optimizer, train_loader, val_loader, num_epochs, save_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-06T16:53:07.228357Z","iopub.status.busy":"2024-02-06T16:53:07.227534Z","iopub.status.idle":"2024-02-06T18:35:17.768312Z","shell.execute_reply":"2024-02-06T18:35:17.767155Z","shell.execute_reply.started":"2024-02-06T16:53:07.228327Z"},"trusted":true},"outputs":[],"source":["trainer.train_model()"]},{"cell_type":"markdown","metadata":{},"source":["### Plotting Cross-Validation and Scores"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-06T18:35:43.632754Z","iopub.status.busy":"2024-02-06T18:35:43.632370Z","iopub.status.idle":"2024-02-06T18:35:44.814361Z","shell.execute_reply":"2024-02-06T18:35:44.813428Z","shell.execute_reply.started":"2024-02-06T18:35:43.632724Z"},"trusted":true},"outputs":[],"source":["trainer.plot()"]},{"cell_type":"markdown","metadata":{},"source":["### *Loading Pre-Trained Parameters"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["load_path = \"/kaggle/working/model_132110822869936.pth\"\n","\n","model.load_state_dict(torch.load(load_path))\n","\n","model.eval()"]},{"cell_type":"markdown","metadata":{},"source":["### Inference"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-06T18:36:43.223430Z","iopub.status.busy":"2024-02-06T18:36:43.223030Z","iopub.status.idle":"2024-02-06T18:36:49.052256Z","shell.execute_reply":"2024-02-06T18:36:49.051314Z","shell.execute_reply.started":"2024-02-06T18:36:43.223401Z"},"trusted":true},"outputs":[],"source":["for _ in range(20):\n","    idx = np.random.randint(val_set_start, val_set_end - 1)\n","\n","    image, caption = dataset[idx]\n","    \n","    image = image.to(device)\n","\n","    prediction = ' '.join(model.caption(image.unsqueeze(0), vocab)[1:-1])\n","\n","    image = image.permute(1,2,0)\n","    \n","    plt.title(prediction)\n","    plt.imshow(image.cpu())\n","\n","    plt.show()   "]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":31296,"sourceId":39911,"sourceType":"datasetVersion"}],"dockerImageVersionId":30635,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
