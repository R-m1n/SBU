{"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"sourceId":39911,"sourceType":"datasetVersion","datasetId":31296}],"dockerImageVersionId":30635,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Importing Libraries","metadata":{}},{"cell_type":"code","source":"!pip install torchsummary","metadata":{"execution":{"iopub.status.busy":"2024-01-28T18:50:04.089853Z","iopub.execute_input":"2024-01-28T18:50:04.090326Z","iopub.status.idle":"2024-01-28T18:50:19.746395Z","shell.execute_reply.started":"2024-01-28T18:50:04.090291Z","shell.execute_reply":"2024-01-28T18:50:19.745124Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting torchsummary\n  Downloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\nInstalling collected packages: torchsummary\nSuccessfully installed torchsummary-1.5.1\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport math\nimport spacy\nimport torch\nimport numpy as np\nimport pandas as pd\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport torchvision.transforms as T\n\nfrom PIL import Image\nfrom torch import nn\nfrom torchsummary import summary\nfrom matplotlib import pyplot as plt\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torchvision.models import vgg16, VGG16_Weights\nfrom nltk.translate.bleu_score import sentence_bleu\n\nspacy_eng = spacy.load(\"en_core_web_sm\")","metadata":{"execution":{"iopub.status.busy":"2024-01-28T18:50:19.749075Z","iopub.execute_input":"2024-01-28T18:50:19.749567Z","iopub.status.idle":"2024-01-28T18:50:29.092017Z","shell.execute_reply.started":"2024-01-28T18:50:19.749504Z","shell.execute_reply":"2024-01-28T18:50:29.091151Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"### Setting the Device","metadata":{}},{"cell_type":"code","source":"if torch.cuda.is_available():\n    torch.backends.cudnn.deterministic = True\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\nprint('Device:', device)","metadata":{"execution":{"iopub.status.busy":"2024-01-28T18:50:37.526911Z","iopub.execute_input":"2024-01-28T18:50:37.527485Z","iopub.status.idle":"2024-01-28T18:50:37.533938Z","shell.execute_reply.started":"2024-01-28T18:50:37.527453Z","shell.execute_reply":"2024-01-28T18:50:37.532825Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Device: cpu\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Preparing the Flikr30k Dataset\n### Preparing the Vocabulary","metadata":{}},{"cell_type":"code","source":"class Vocabulary:\n    def __init__(self, freq_threshold):\n        self.freq_threshold = freq_threshold\n        self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n        self.stoi = {v: k for k,v in self.itos.items()}\n    \n    def __len__(self):\n        return len(self.itos)\n  \n    def build_vocab(self, sentence_list):\n        freqs = {}\n        idx = 4\n\n        for sentence in sentence_list:\n            sentence = str(sentence)\n\n            for word in self.tokenize(sentence):\n                if word not in freqs:\n                    freqs[word] = 1\n                    \n                else:\n                    freqs[word] += 1\n\n                if freqs[word] == self.freq_threshold:\n                    self.itos[idx] = word\n                    self.stoi[word] = idx\n                    \n                    idx += 1\n\n    def numericalize(self, sentence):\n        tokens = self.tokenize(sentence)\n        result = []\n\n        for token in tokens:\n            if token in self.stoi:\n                result.append(self.stoi[token])\n            else:\n                result.append(self.stoi[\"<UNK>\"])\n\n        return result\n    \n    @staticmethod\n    def tokenize(sentence):\n        return [token.text.lower() for token in spacy_eng.tokenizer(str(sentence))]","metadata":{"execution":{"iopub.status.busy":"2024-01-28T17:49:30.528596Z","iopub.execute_input":"2024-01-28T17:49:30.529054Z","iopub.status.idle":"2024-01-28T17:49:30.541671Z","shell.execute_reply.started":"2024-01-28T17:49:30.529021Z","shell.execute_reply":"2024-01-28T17:49:30.540697Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"### Defining a Custom Dataset","metadata":{}},{"cell_type":"code","source":"class Flickr(Dataset):\n    def __init__(self, root_dir, caption_path, transform, freq_threshold=5):\n        self.freq_threshold = freq_threshold\n        self.transform = transform\n        self.root_dir = root_dir\n    \n        self.df = pd.read_csv(caption_path, delimiter='|')\n        \n        self.images = self.df['image_name']\n        self.captions = self.df[' comment']\n        \n        self.vocab = Vocabulary(freq_threshold)\n        \n        self.vocab.build_vocab(self.captions.tolist())\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):\n        image = self.images[index]\n        caption = self.captions[index]\n        \n        image = Image.open(os.path.join(self.root_dir, image)).convert(\"RGB\")\n        \n        image = self.transform(image)\n        \n        numericalized_caption = [self.vocab.stoi[\"<SOS>\"]]\n        \n        numericalized_caption += self.vocab.numericalize(caption)\n        \n        numericalized_caption.append(self.vocab.stoi[\"<EOS>\"])\n        \n        return image, torch.tensor(numericalized_caption)\n    \n    def get_label(self, index):\n        image, caption = self[index]\n    \n        label = [self.vocab.itos[token] for token in caption.tolist()]\n\n        eos_index = label.index('<EOS>')\n\n        label = label[1: eos_index]\n\n        return ' '.join(label)\n    \n    def to_list(self):\n        return [self.get_label(idx) for idx in range(len(self))]\n        ","metadata":{"execution":{"iopub.status.busy":"2024-01-28T17:49:32.209998Z","iopub.execute_input":"2024-01-28T17:49:32.210430Z","iopub.status.idle":"2024-01-28T17:49:32.226343Z","shell.execute_reply.started":"2024-01-28T17:49:32.210395Z","shell.execute_reply":"2024-01-28T17:49:32.225043Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"### Defining a Custom Caption Collat for Padding","metadata":{}},{"cell_type":"code","source":"class CapCollat:\n    def __init__(self, pad_seq, batch_first=False):\n        self.pad_seq = pad_seq\n        self.batch_first = batch_first\n  \n    def __call__(self, batch):\n        imgs = [itm[0].unsqueeze(0) for itm in batch]\n        imgs = torch.cat(imgs, dim=0)\n\n        target_caps = [itm[1] for itm in batch]\n        target_caps = pad_sequence(target_caps, batch_first=self.batch_first,\n                                   padding_value=self.pad_seq)\n        \n        return imgs, target_caps","metadata":{"execution":{"iopub.status.busy":"2024-01-28T17:49:33.119340Z","iopub.execute_input":"2024-01-28T17:49:33.119774Z","iopub.status.idle":"2024-01-28T17:49:33.126963Z","shell.execute_reply.started":"2024-01-28T17:49:33.119736Z","shell.execute_reply":"2024-01-28T17:49:33.125548Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"### Loading and Testing the Dataset ","metadata":{}},{"cell_type":"code","source":"root_folder = \"/kaggle/input/flickr-image-dataset/flickr30k_images/flickr30k_images/\"\ncsv_file = \"/kaggle/input/flickr-image-dataset/flickr30k_images/results.csv\"\n\ntransform = T.Compose([\n        T.Resize((256, 256), interpolation=T.InterpolationMode.BILINEAR),\n        T.CenterCrop((224, 224)),\n        T.ToTensor(),\n        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\nbatch_size = 32\nnum_workers = 2\nbatch_first = True\npin_memory = True\ndataset = Flickr(root_folder, csv_file, transform)\npad_idx = dataset.vocab.stoi[\"<PAD>\"]\n\ndata_size = len(dataset)\ntrain_size = int(0.9 * data_size)\nval_size = data_size - train_size\n\ntrain_set, val_set = torch.utils.data.Subset(dataset, range(0, train_size)), torch.utils.data.Subset(dataset, range(train_size, data_size))\n\ntrain_loader = DataLoader(train_set,\n                            batch_size=batch_size,\n                            pin_memory=pin_memory,\n                            num_workers=num_workers,\n                            shuffle=True,\n                            collate_fn=CapCollat(pad_seq=pad_idx, batch_first=batch_first))\n\nval_loader = DataLoader(val_set,\n                            batch_size=batch_size,\n                            pin_memory=pin_memory,\n                            num_workers=num_workers,\n                            shuffle=False,\n                            collate_fn=CapCollat(pad_seq=pad_idx, batch_first=batch_first))","metadata":{"execution":{"iopub.status.busy":"2024-01-28T18:33:01.995834Z","iopub.execute_input":"2024-01-28T18:33:01.996293Z","iopub.status.idle":"2024-01-28T18:33:54.212922Z","shell.execute_reply.started":"2024-01-28T18:33:01.996256Z","shell.execute_reply":"2024-01-28T18:33:54.211607Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"val_set_start = data_size - val_size - 1\nval_set_end = data_size - 1\n\nall_labels = dataset.to_list()","metadata":{"execution":{"iopub.status.busy":"2024-01-28T17:50:31.903562Z","iopub.execute_input":"2024-01-28T17:50:31.904030Z","iopub.status.idle":"2024-01-28T18:17:23.745576Z","shell.execute_reply.started":"2024-01-28T17:50:31.903997Z","shell.execute_reply":"2024-01-28T18:17:23.744137Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"for idx in range(10):\n    image, _ = dataset[idx + val_size]\n    \n    label = all_labels[idx + val_size]\n\n    image = image.permute(1,2,0)\n    \n    plt.imshow(image)\n    plt.title(label)\n\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Pre-Trained CNN Encoder: VGG16 ","metadata":{}},{"cell_type":"code","source":"class EncoderCNN(nn.Module):\n    def __init__(self):\n        super(EncoderCNN, self).__init__()\n\n        vgg = vgg16(weights=VGG16_Weights.DEFAULT)\n        all_modules = list(vgg.children())\n        feature_extractor = all_modules[:-1]\n        classifier = all_modules[-1]\n\n        self.vgg = nn.Sequential(*feature_extractor, nn.Flatten(), classifier)\n\n        self.disable_learning()\n       \n    def forward(self, image):\n        encoded_image = self.vgg(image)\n  \n        return encoded_image\n\n    def disable_learning(self):\n        for param in self.parameters():\n            param.requires_grad = False","metadata":{"execution":{"iopub.status.busy":"2024-01-28T19:00:39.921002Z","iopub.execute_input":"2024-01-28T19:00:39.921475Z","iopub.status.idle":"2024-01-28T19:00:39.931155Z","shell.execute_reply.started":"2024-01-28T19:00:39.921441Z","shell.execute_reply":"2024-01-28T19:00:39.929165Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"summary(EncoderCNN(), (3, 224, 224))","metadata":{"execution":{"iopub.status.busy":"2024-01-28T19:00:41.158847Z","iopub.execute_input":"2024-01-28T19:00:41.159243Z","iopub.status.idle":"2024-01-28T19:00:43.780424Z","shell.execute_reply.started":"2024-01-28T19:00:41.159212Z","shell.execute_reply":"2024-01-28T19:00:43.779401Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1         [-1, 64, 224, 224]           1,792\n              ReLU-2         [-1, 64, 224, 224]               0\n            Conv2d-3         [-1, 64, 224, 224]          36,928\n              ReLU-4         [-1, 64, 224, 224]               0\n         MaxPool2d-5         [-1, 64, 112, 112]               0\n            Conv2d-6        [-1, 128, 112, 112]          73,856\n              ReLU-7        [-1, 128, 112, 112]               0\n            Conv2d-8        [-1, 128, 112, 112]         147,584\n              ReLU-9        [-1, 128, 112, 112]               0\n        MaxPool2d-10          [-1, 128, 56, 56]               0\n           Conv2d-11          [-1, 256, 56, 56]         295,168\n             ReLU-12          [-1, 256, 56, 56]               0\n           Conv2d-13          [-1, 256, 56, 56]         590,080\n             ReLU-14          [-1, 256, 56, 56]               0\n           Conv2d-15          [-1, 256, 56, 56]         590,080\n             ReLU-16          [-1, 256, 56, 56]               0\n        MaxPool2d-17          [-1, 256, 28, 28]               0\n           Conv2d-18          [-1, 512, 28, 28]       1,180,160\n             ReLU-19          [-1, 512, 28, 28]               0\n           Conv2d-20          [-1, 512, 28, 28]       2,359,808\n             ReLU-21          [-1, 512, 28, 28]               0\n           Conv2d-22          [-1, 512, 28, 28]       2,359,808\n             ReLU-23          [-1, 512, 28, 28]               0\n        MaxPool2d-24          [-1, 512, 14, 14]               0\n           Conv2d-25          [-1, 512, 14, 14]       2,359,808\n             ReLU-26          [-1, 512, 14, 14]               0\n           Conv2d-27          [-1, 512, 14, 14]       2,359,808\n             ReLU-28          [-1, 512, 14, 14]               0\n           Conv2d-29          [-1, 512, 14, 14]       2,359,808\n             ReLU-30          [-1, 512, 14, 14]               0\n        MaxPool2d-31            [-1, 512, 7, 7]               0\nAdaptiveAvgPool2d-32            [-1, 512, 7, 7]               0\n          Flatten-33                [-1, 25088]               0\n           Linear-34                 [-1, 4096]     102,764,544\n             ReLU-35                 [-1, 4096]               0\n          Dropout-36                 [-1, 4096]               0\n           Linear-37                 [-1, 4096]      16,781,312\n             ReLU-38                 [-1, 4096]               0\n          Dropout-39                 [-1, 4096]               0\n           Linear-40                 [-1, 1000]       4,097,000\n================================================================\nTotal params: 138,357,544\nTrainable params: 0\nNon-trainable params: 138,357,544\n----------------------------------------------------------------\nInput size (MB): 0.57\nForward/backward pass size (MB): 218.97\nParams size (MB): 527.79\nEstimated Total Size (MB): 747.34\n----------------------------------------------------------------\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# RNN Decoder: Vanilla RNN Module","metadata":{}},{"cell_type":"code","source":"class DecoderRNN(nn.Module):\n    def __init__(self, embed_size,vocab_size, hidden_size, num_layers):\n        super(DecoderRNN, self).__init__()\n\n        self.embedding = nn.Embedding(vocab_size, embed_size)\n        self.rnn = nn.RNN(embed_size, hidden_size, num_layers)\n        self.linear = nn.Linear(hidden_size, vocab_size)\n        self.dropout = nn.Dropout(0.5)\n    \n    def forward(self, features, caption):\n        embeddings = self.dropout(self.embedding(caption))\n        embeddings = torch.cat((features.unsqueeze(0),embeddings), dim=0)\n        hiddens, _ = self.rnn(embeddings)\n        outputs = self.linear(hiddens)\n\n        return outputs","metadata":{"execution":{"iopub.status.busy":"2024-01-28T18:36:08.615088Z","iopub.execute_input":"2024-01-28T18:36:08.615534Z","iopub.status.idle":"2024-01-28T18:36:08.624821Z","shell.execute_reply.started":"2024-01-28T18:36:08.615500Z","shell.execute_reply":"2024-01-28T18:36:08.623840Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"class Hybrid(nn.Module):\n    def __init__(self, embed_size, vocab_size, hidden_size, num_layers):\n        super(Hybrid, self).__init__()\n        \n        self.encoderCNN = EncoderCNN(embed_size, hidden_size)\n        self.decoderRNN = DecoderRNN(embed_size, vocab_size, hidden_size, num_layers)\n    \n    def forward(self, images, caption):\n        x = self.encoderCNN(images)\n        x = self.decoderRNN(x, caption)\n        \n        return x\n    \n    def captionImage(self, image, vocabulary, maxlength=50):\n        result_caption = []\n        \n        with torch.no_grad():\n            x = self.encoderCNN(image).unsqueeze(0)\n            states = None\n            \n            for _ in range(maxlength):\n                hiddens, states = self.decoderRNN.lstm(x, states)\n                output = self.decoderRNN.linear(hiddens.squeeze(0))\n                predicted = output.argmax(1)\n                print(predicted.shape)\n                result_caption.append(predicted.item())\n                x = self.decoderRNN.embedding(output).unsqueeze(0)\n                \n                if vocabulary.itos[predicted.item()] == \"<EOS>\":\n                    break\n                \n        return [vocabulary.itos[i] for i in result_caption]","metadata":{"execution":{"iopub.status.busy":"2024-01-28T18:40:12.804703Z","iopub.execute_input":"2024-01-28T18:40:12.805840Z","iopub.status.idle":"2024-01-28T18:40:12.819178Z","shell.execute_reply.started":"2024-01-28T18:40:12.805793Z","shell.execute_reply":"2024-01-28T18:40:12.818057Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"markdown","source":"# Training and Evaluating the Image Captioning Model: VGG16 + Multi-layer Vanilla RNN","metadata":{}},{"cell_type":"markdown","source":"### Setting Hyperparameters","metadata":{}},{"cell_type":"code","source":"num_epochs = 10\nfreq_threshold = 5\nenc_dim = 2048\nembed_size = 300\nhidden_size = 1000\nnum_layers = 2\nlearning_rate = 3e-4","metadata":{"execution":{"iopub.status.busy":"2024-01-28T18:36:11.796857Z","iopub.execute_input":"2024-01-28T18:36:11.797242Z","iopub.status.idle":"2024-01-28T18:36:11.803286Z","shell.execute_reply.started":"2024-01-28T18:36:11.797212Z","shell.execute_reply":"2024-01-28T18:36:11.802189Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"### Setting the Vocabulary","metadata":{}},{"cell_type":"code","source":"vocab = dataset.vocab\nvocab_size = len(vocab)","metadata":{"execution":{"iopub.status.busy":"2024-01-28T18:36:14.142666Z","iopub.execute_input":"2024-01-28T18:36:14.143123Z","iopub.status.idle":"2024-01-28T18:36:14.148139Z","shell.execute_reply.started":"2024-01-28T18:36:14.143088Z","shell.execute_reply":"2024-01-28T18:36:14.147036Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":"### Configuring Models","metadata":{}},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss(ignore_index = vocab.stoi[\"<PAD>\"])\n\nmodel = Hybrid(embed_size, hidden_size, vocab_size, num_layers).to(device=device)\n\noptimizer = optim.Adam(model.parameters(), lr = learning_rate)","metadata":{"execution":{"iopub.status.busy":"2024-01-28T18:41:10.975702Z","iopub.execute_input":"2024-01-28T18:41:10.976747Z","iopub.status.idle":"2024-01-28T18:41:16.237324Z","shell.execute_reply.started":"2024-01-28T18:41:10.976697Z","shell.execute_reply":"2024-01-28T18:41:16.235927Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"markdown","source":"### Training Models","metadata":{}},{"cell_type":"code","source":"!pip install pycocoevalcap","metadata":{"execution":{"iopub.status.busy":"2024-01-28T18:37:13.201949Z","iopub.execute_input":"2024-01-28T18:37:13.202509Z","iopub.status.idle":"2024-01-28T18:37:30.774962Z","shell.execute_reply.started":"2024-01-28T18:37:13.202449Z","shell.execute_reply":"2024-01-28T18:37:30.773664Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"Collecting pycocoevalcap\n  Downloading pycocoevalcap-1.2-py3-none-any.whl (104.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.3/104.3 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting pycocotools>=2.0.2 (from pycocoevalcap)\n  Obtaining dependency information for pycocotools>=2.0.2 from https://files.pythonhosted.org/packages/ba/64/0451cf41a00fd5ac4501de4ea0e395b7d909e09d665e56890b5d3809ae26/pycocotools-2.0.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n  Downloading pycocotools-2.0.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.1 kB)\nRequirement already satisfied: matplotlib>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from pycocotools>=2.0.2->pycocoevalcap) (3.7.4)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from pycocotools>=2.0.2->pycocoevalcap) (1.24.3)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (1.1.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (0.11.0)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (4.42.1)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (1.4.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (21.3)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (9.5.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (2.8.2)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (1.16.0)\nDownloading pycocotools-2.0.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (426 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m426.2/426.2 kB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: pycocotools, pycocoevalcap\nSuccessfully installed pycocoevalcap-1.2 pycocotools-2.0.7\n","output_type":"stream"}]},{"cell_type":"code","source":"from pycocoevalcap.bleu.bleu import Bleu\nfrom pycocoevalcap.meteor.meteor import Meteor\nfrom pycocoevalcap.rouge.rouge import Rouge\nfrom pycocoevalcap.cider.cider import Cider\n\nclass Scorer():\n    def __init__(self,ref,gt):\n        self.ref = ref\n        self.gt = gt\n\n        self.word_based_scorers = [\n            (Bleu(4), [\"Bleu_1\", \"Bleu_2\", \"Bleu_3\", \"Bleu_4\"]),\n            (Meteor(),\"METEOR\"),\n            (Rouge(), \"ROUGE_L\"),\n            (Cider(), \"CIDEr\"),\n            ]\n\n    def compute_scores(self):\n        total_scores = {\n            \"Bleu1\":[],\n            \"Bleu2\":[],\n            \"Bleu3\":[],\n            \"Bleu4\":[],\n            \"METEOR\":[],\n            \"ROUGE_L\":[],\n            \"CIDEr\":[],\n        }\n\n        for scorer, method in self.word_based_scorers:\n            score, scores = scorer.compute_score(self.ref, self.gt)\n    \n            if type(method) == list:\n                total_scores[\"Bleu1\"].append(score[0])\n                total_scores[\"Bleu2\"].append(score[1])\n                total_scores[\"Bleu3\"].append(score[2])\n                total_scores[\"Bleu4\"].append(score[3])\n\n            else:\n                total_scores[method].append(score)\n\n        return total_scores\n    \n    def compute_scores_iterative(self):\n        total_scores = {\n            \"Bleu1\":[],\n            \"Bleu2\":[],\n            \"Bleu3\":[],\n            \"Bleu4\":[],\n            \"METEOR\":[],\n            \"ROUGE_L\":[],\n            \"CIDEr\":[],\n            \"SPICE\":[]\n        \n        }\n\n        for key in self.ref:\n            curr_ref = {key:self.ref[key]}\n            curr_gt = {key:self.gt[key]}\n\n            for scorer, method in self.word_based_scorers:\n                score, _ = scorer.compute_score(curr_ref, curr_gt)\n                if type(method) == list:\n                    total_scores[\"Bleu1\"].append(score[0])\n                    total_scores[\"Bleu2\"].append(score[1])\n                    total_scores[\"Bleu3\"].append(score[2])\n                    total_scores[\"Bleu4\"].append(score[3])\n\n                else:\n                    total_scores[method].append(score)\n\n        return total_scores","metadata":{"execution":{"iopub.status.busy":"2024-01-28T18:37:33.792535Z","iopub.execute_input":"2024-01-28T18:37:33.793054Z","iopub.status.idle":"2024-01-28T18:37:33.822095Z","shell.execute_reply.started":"2024-01-28T18:37:33.793013Z","shell.execute_reply":"2024-01-28T18:37:33.820912Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"train_losses = list()\nval_losses = list()\n\ncumulative_bleu_scores = list()\n\ncider_scores = list()\n\nmeteor_scores = list()\n\nrougel_scores = list()\n\nbleu1_scores = list()\nbleu2_scores = list()\nbleu3_scores = list()\nbleu4_scores = list()\n\nval_iter = iter(val_loader)\n\nfor epoch in range(num_epochs):\n    model.train()\n\n    for batch_idx, (images, captions) in enumerate(train_loader):\n        images = images.to(device)\n        captions = captions.to(device)\n        \n        train_score = model(images, captions[:-1])\n\n        optimizer.zero_grad()\n        train_loss = criterion(train_score.reshape(-1, train_score.shape[2]), captions.reshape(-1))\n        train_losses.append(train_loss.item())\n        \n        train_loss.backward()\n        optimizer.step()\n\n    with torch.no_grad():\n        model.eval()\n\n        idx = val_set_start\n\n        hyp = {}\n        ref = {}\n\n        while idx <= val_set_end:\n            if idx % batch_size == 0:\n                batch = next(val_iter)\n\n            val_images, val_captions = batch\n\n            image, caption = val_images[idx % batch_size], val_captions[idx % batch_size]\n\n            image = image.to_device()\n            caption = caption.to_device()\n\n            val_score = model(image, caption[: -1])\n\n            val_loss = criterion(val_score.reshape(-1, val_score.shape[2]), val_captions[1:].reshape(-1))\n\n            val_losses.append(val_loss.item())\n\n            val_pred = model.caption_image(image, vocab)\n\n            hyp[idx] = [' '.join(val_pred)]\n\n            label = all_labels[idx]\n\n            ref[idx] = label\n\n            cumulative_bleu_score = sentence_bleu(label, val_pred, weights=(0.25, 0.25, 0.25, 0.25))\n            cumulative_bleu_scores.append(cumulative_bleu_score)\n            \n            idx += 1\n            \n        metrics = Scorer(ref, hyp).compute_scores()\n    \n        bleu1_scores.append(metrics['Bleu_1'])\n        bleu2_scores.append(metrics['Bleu_2'])\n        bleu3_scores.append(metrics['Bleu_3'])\n        bleu4_scores.append(metrics['Bleu_4'])\n\n        cider_scores.append(metrics['CIDEr'])\n\n        rougel_scores.append(metrics['ROUGE_L'])\n\n        meteor_scores.append(metrics['METEOR'])\n        \n        print(f\"Epoch [{epoch+1}/{num_epochs}] Batch [{batch_idx+1}/{len(train_loader)}] | Training loss: {train_loss.item()} Validation loss: {val_loss.item()} | Cumulative BLEU Score: {cumulative_bleu_score} CIDEr Score: {metrics['CIDEr']} METEOR Score: {metrics['METEOR']} ROUGE_L: {metrics['ROUGE_L']} \\n\")\n\n# if batch_idx == (len(train_loader) - 1):\n#     torch.save(model.state_dict(), f'/kaggle/working/encoder_{freq_threshold}_{batch_size}_{hidden_size}_{epoch+1}.pth')\n","metadata":{"execution":{"iopub.status.busy":"2024-01-28T18:41:21.030505Z","iopub.execute_input":"2024-01-28T18:41:21.031524Z","iopub.status.idle":"2024-01-28T18:41:34.474777Z","shell.execute_reply.started":"2024-01-28T18:41:21.031455Z","shell.execute_reply":"2024-01-28T18:41:34.471582Z"},"trusted":true},"execution_count":48,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[48], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     24\u001b[0m captions \u001b[38;5;241m=\u001b[39m captions\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 26\u001b[0m train_score \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptions\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     29\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m criterion(train_score\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, train_score\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m]), captions\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","Cell \u001b[0;32mIn[43], line 9\u001b[0m, in \u001b[0;36mHybrid.forward\u001b[0;34m(self, images, caption)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, images, caption):\n\u001b[0;32m----> 9\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoderCNN\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoderRNN(x, caption)\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","Cell \u001b[0;32mIn[42], line 14\u001b[0m, in \u001b[0;36mEncoderCNN.forward\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, image):\n\u001b[0;32m---> 14\u001b[0m         encoded_image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvgg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m#         batch_size = encoded_image.shape[0]\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m#         features = encoded_image.shape[1]\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m#         num_pixels = encoded_image.shape[2] * encoded_image.shape[3]\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m#         enc_image = enc_image.view(batch_size, num_pixels, features)\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m#         enc_image = F.relu(enc_image)\u001b[39;00m\n\u001b[1;32m     24\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m encoded_image\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (114688x7 and 25088x4096)"],"ename":"RuntimeError","evalue":"mat1 and mat2 shapes cannot be multiplied (114688x7 and 25088x4096)","output_type":"error"}]},{"cell_type":"markdown","source":"### Plotting Losses and Scores","metadata":{}},{"cell_type":"code","source":"plt.figure(0)\nplt.plot(train_losses, label = 'Training loss')\nplt.plot(val_losses, label = 'Validation loss')\nplt.ylabel('Cross Entropy Loss')\nplt.legend()\nplt.savefig(f'/kaggle/working/{freq_threshold}_{batch_size}_{hidden_size}_{num_epochs}_losses.png')\n\nplt.figure(1)\nplt.plot(bleu1_scores, label = 'BLEU 1')\nplt.plot(bleu2_scores, label = 'BLEU 2')\nplt.plot(bleu3_scores, label = 'BLEU 3')\nplt.plot(bleu4_scores, label = 'BLEU 4')\nplt.ylabel('BLEU Scores')\nplt.legend()\nplt.savefig(f'/kaggle/working/{freq_threshold}_{batch_size}_{hidden_size}_{num_epochs}_bleu_scores.png')\n        \nplt.figure(2)\nplt.plot(cumulative_bleu_scores, label = 'Cumulative BLEU SCORE')\nplt.plot(cider_scores, label = 'CIDEr SCORE')\nplt.plot(meteor_scores, label = 'METEOR SCORE')\nplt.plot(rougel_scores, label = 'ROUGE_L SCORE')\nplt.ylabel('Scores')\nplt.legend()\nplt.savefig(f'/kaggle/working/{freq_threshold}_{batch_size}_{hidden_size}_{num_epochs}_scores.png')","metadata":{},"execution_count":null,"outputs":[]}]}