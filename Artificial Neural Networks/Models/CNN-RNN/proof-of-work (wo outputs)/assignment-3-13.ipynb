{"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":39911,"sourceType":"datasetVersion","datasetId":31296}],"dockerImageVersionId":30635,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Importing Libraries","metadata":{}},{"cell_type":"code","source":"import os\nimport math\nimport spacy\nimport torch\nimport numpy as np\nimport pandas as pd\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport torchvision.transforms as T\n\nfrom PIL import Image\nfrom torch import nn\nfrom matplotlib import pyplot as plt\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torchvision.models import vgg16, VGG16_Weights\nfrom nltk.translate.bleu_score import sentence_bleu\n\nspacy_eng = spacy.load(\"en_core_web_sm\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install torchsummary\n!pip install pycocoevalcap\n\nfrom torchsummary import summary\nfrom pycocoevalcap.bleu.bleu import Bleu\nfrom pycocoevalcap.meteor.meteor import Meteor\nfrom pycocoevalcap.rouge.rouge import Rouge\nfrom pycocoevalcap.cider.cider import Cider","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Setting the Device","metadata":{}},{"cell_type":"code","source":"if torch.cuda.is_available():\n    torch.backends.cudnn.deterministic = True\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\nprint('Device:', device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preparing the Flikr30k Dataset\n### Preparing the Vocabulary","metadata":{}},{"cell_type":"code","source":"class Vocabulary:\n    def __init__(self, freq_threshold):\n        self.freq_threshold = freq_threshold\n        self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n        self.stoi = {v: k for k,v in self.itos.items()}\n    \n    def __len__(self):\n        return len(self.itos)\n  \n    def build_vocab(self, sentence_list):\n        freqs = {}\n        idx = 4\n\n        for sentence in sentence_list:\n            sentence = str(sentence)\n\n            for word in self.tokenize(sentence):\n                if word not in freqs:\n                    freqs[word] = 1\n                    \n                else:\n                    freqs[word] += 1\n\n                if freqs[word] == self.freq_threshold:\n                    self.itos[idx] = word\n                    self.stoi[word] = idx\n                    \n                    idx += 1\n\n    def numericalize(self, sentence):\n        tokens = self.tokenize(sentence)\n        result = []\n\n        for token in tokens:\n            if token in self.stoi:\n                result.append(self.stoi[token])\n            else:\n                result.append(self.stoi[\"<UNK>\"])\n\n        return result\n    \n    @staticmethod\n    def tokenize(sentence):\n        return [token.text.lower() for token in spacy_eng.tokenizer(str(sentence))]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Defining a Custom Dataset","metadata":{}},{"cell_type":"code","source":"class Flickr(Dataset):\n    def __init__(self, root_dir, caption_path, transform, freq_threshold=5):\n        self.freq_threshold = freq_threshold\n        self.transform = transform\n        self.root_dir = root_dir\n    \n        self.df = pd.read_csv(caption_path, delimiter='|')\n        \n        self.images = self.df['image_name']\n        self.captions = self.df[' comment']\n        \n        self.vocab = Vocabulary(freq_threshold)\n        \n        self.vocab.build_vocab(self.captions.tolist())\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):\n        max_seq_length = 50\n        \n        image = self.images[index]\n        caption = self.captions[index]\n        \n        image = Image.open(os.path.join(self.root_dir, image)).convert(\"RGB\")\n        \n        image = self.transform(image)\n        \n        numericalized_caption = [self.vocab.stoi[\"<SOS>\"]]\n        \n        numericalized_caption += self.vocab.numericalize(caption)\n        \n        numericalized_caption.append(self.vocab.stoi[\"<EOS>\"])\n        \n        if len(numericalized_caption) > max_seq_length:\n            numericalized_caption = numericalized_caption[:max_seq_length]\n        else:\n            numericalized_caption += [self.vocab.stoi[\"<PAD>\"]] * (max_seq_length - len(numericalized_caption))\n        \n        return image, torch.tensor(numericalized_caption)\n    \n    def get_label(self, index):\n        image, caption = self[index]\n    \n        label = [self.vocab.itos[token] for token in caption.tolist()]\n\n        eos_index = label.index('<EOS>')\n\n        label = label[1: eos_index]\n\n        return ' '.join(label)\n    \n    def to_list(self):\n        return self.captions.tolist()\n        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Defining a Custom Caption Collat for Padding","metadata":{}},{"cell_type":"code","source":"class CapCollat:\n    def __init__(self, pad_seq, batch_first=False):\n        self.pad_seq = pad_seq\n        self.batch_first = batch_first\n  \n    def __call__(self, batch):\n        imgs = [itm[0].unsqueeze(0) for itm in batch]\n        imgs = torch.cat(imgs, dim=0)\n\n        target_caps = [itm[1] for itm in batch]\n        target_caps = pad_sequence(target_caps, batch_first=self.batch_first,\n                                   padding_value=self.pad_seq)\n        \n        return imgs, target_caps","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Loading and Testing the Dataset ","metadata":{}},{"cell_type":"code","source":"root_folder = \"/kaggle/input/flickr-image-dataset/flickr30k_images/flickr30k_images/\"\ncsv_file = \"/kaggle/input/flickr-image-dataset/flickr30k_images/results.csv\"\n\ntransform = T.Compose([\n        T.Resize((256, 256), interpolation=T.InterpolationMode.BILINEAR),\n        T.CenterCrop((224, 224)),\n        T.ToTensor(),\n        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\nbatch_size = 37\nnum_workers = 2\nfreq_threshold = 5\nbatch_first = True\npin_memory = True\ndataset = Flickr(root_folder, csv_file, transform, freq_threshold)\npad_idx = dataset.vocab.stoi[\"<PAD>\"]\n\ndata_size = len(dataset)\ntrain_size = int(0.9 * data_size)\nval_size = data_size - train_size\n\ntrain_set, val_set = torch.utils.data.Subset(dataset, range(0, train_size)), torch.utils.data.Subset(dataset, range(train_size, data_size))\n\ntrain_loader = DataLoader(train_set,\n                            batch_size=batch_size,\n                            pin_memory=pin_memory,\n                            num_workers=num_workers,\n                            shuffle=True,\n                            collate_fn=CapCollat(pad_seq=pad_idx, batch_first=batch_first))\n\nval_loader = DataLoader(val_set,\n                            batch_size=batch_size,\n                            pin_memory=pin_memory,\n                            num_workers=num_workers,\n                            shuffle=False,\n                            collate_fn=CapCollat(pad_seq=pad_idx, batch_first=batch_first))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_set_start = data_size - val_size - 1\nval_set_end = data_size - 1\n\nall_labels = dataset.to_list()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for idx in range(0, 100, 10):\n    image, _ = dataset[idx + val_size]\n    \n    label = all_labels[idx + val_size]\n\n    image = image.permute(1,2,0)\n    \n    plt.imshow(image)\n    plt.title(label)\n\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Pre-Trained CNN Encoder: VGG16 ","metadata":{}},{"cell_type":"code","source":"class EncoderCNN(nn.Module):\n    def __init__(self, embed_size=256):\n        super(EncoderCNN, self).__init__()\n\n        vgg = vgg16(weights=VGG16_Weights.IMAGENET1K_FEATURES)\n        \n        for param in vgg.parameters():\n            param.requires_grad = False\n            \n        feature_extractor = list(vgg.children())[:-1]\n            \n        embedding_layer = nn.Linear(512 * 7 * 7, embed_size)\n\n        self.encoder = nn.Sequential(*feature_extractor,\n                                     nn.Flatten(),\n                                     embedding_layer)\n       \n    def forward(self, image):\n        encoded_image = self.encoder(image)\n  \n        return encoded_image","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"summary(EncoderCNN(224).to(device), (3, 224, 224))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# RNN Decoder: Vanilla RNN Module","metadata":{}},{"cell_type":"code","source":"class DecoderRNN(nn.Module):\n    def __init__(self, embed_size, vocab_size, hidden_size, num_layers):\n        super(DecoderRNN, self).__init__()\n\n        self.embedding = nn.Embedding(vocab_size, embed_size)\n        self.rnn = nn.RNN(embed_size, hidden_size, num_layers, batch_first=True)\n        self.linear = nn.Linear(hidden_size, vocab_size)\n\n        self.features2hidden = nn.Linear(embed_size, hidden_size)\n\n    def forward(self, features, captions):\n        captions_embed = self.embedding(captions)\n\n        initial_hidden_state = self.features2hidden(features).unsqueeze(0).repeat(self.rnn.num_layers, 1, 1)\n\n        output, _ = self.rnn(captions_embed, initial_hidden_state)\n        output = self.linear(output)\n\n        return output\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ImageCap(nn.Module):\n    def __init__(self, embed_size, vocab_size, hidden_size, num_layers):\n        super(ImageCap, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        \n        self.encoderCNN = EncoderCNN(embed_size)\n        self.decoderRNN = DecoderRNN(embed_size, vocab_size, hidden_size, num_layers)\n    \n    def forward(self, images, captions):\n        x = self.encoderCNN(images)\n        x = self.decoderRNN(x, captions)\n        \n        return x\n    \n    def caption(self, image, vocabulary, maxlength=50):\n        result_caption = []\n\n        with torch.no_grad():\n            x = self.encoderCNN(image).unsqueeze(0)\n            states = None\n\n            for _ in range(maxlength):\n                hiddens, states = self.decoderRNN.rnn(x, states)\n                output = self.decoderRNN.linear(hiddens.squeeze(0))\n                predicted = output.argmax(1)\n                result_caption.append(predicted.item())\n                x = self.decoderRNN.embedding(predicted).unsqueeze(0)\n\n                if vocabulary.itos[predicted.item()] == \"<EOS>\":\n                    break\n\n        return [vocabulary.itos[i] for i in result_caption]\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training and Evaluating the Image Captioning Model: VGG16 + Multi-layer Vanilla RNN","metadata":{}},{"cell_type":"markdown","source":"### Setting Hyperparameters","metadata":{}},{"cell_type":"code","source":"num_epochs = 4\nenc_dim = 2048\nembed_size = 224\nhidden_size = 512\nnum_layers = 1\nlearning_rate = 3e-4","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Setting the Vocabulary","metadata":{}},{"cell_type":"code","source":"vocab = dataset.vocab\nvocab_size = len(vocab)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Configuring Models","metadata":{}},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss(ignore_index = vocab.stoi[\"<PAD>\"]).to(device)\n\nmodel = ImageCap(embed_size, vocab_size, hidden_size, num_layers).to(device)\n\noptimizer = optim.Adam(model.parameters(), lr = learning_rate)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training Models","metadata":{}},{"cell_type":"code","source":"class Scorer():\n    def __init__(self, gt, res):\n        self.gt = gt\n        self.res = res\n\n        self.word_based_scorers = [\n            (Bleu(4), [\"Bleu_1\", \"Bleu_2\", \"Bleu_3\", \"Bleu_4\"]),\n            (Meteor(),\"METEOR\"),\n            (Rouge(), \"ROUGE_L\"),\n            (Cider(), \"CIDEr\"),\n            ]\n\n    def compute_scores(self):\n        total_scores = {\n            \"Bleu1\":[],\n            \"Bleu2\":[],\n            \"Bleu3\":[],\n            \"Bleu4\":[],\n            \"METEOR\":[],\n            \"ROUGE_L\":[],\n            \"CIDEr\":[],\n        }\n\n        for scorer, method in self.word_based_scorers:\n            score, scores = scorer.compute_score(self.gt, self.res)\n    \n            if type(method) == list:\n                total_scores[\"Bleu1\"].append(score[0])\n                total_scores[\"Bleu2\"].append(score[1])\n                total_scores[\"Bleu3\"].append(score[2])\n                total_scores[\"Bleu4\"].append(score[3])\n\n            else:\n                total_scores[method].append(score)\n\n        return total_scores\n    \n    def compute_scores_iterative(self):\n        total_scores = {\n            \"Bleu1\":[],\n            \"Bleu2\":[],\n            \"Bleu3\":[],\n            \"Bleu4\":[],\n            \"METEOR\":[],\n            \"ROUGE_L\":[],\n            \"CIDEr\":[],\n            \"SPICE\":[]\n        \n        }\n\n        for key in self.ref:\n            curr_gt = {key:self.gt[key]}\n            curr_ref = {key:self.ref[key]}\n\n            for scorer, method in self.word_based_scorers:\n                score, _ = scorer.compute_score(curr_gt, curr_res)\n                if type(method) == list:\n                    total_scores[\"Bleu1\"].append(score[0])\n                    total_scores[\"Bleu2\"].append(score[1])\n                    total_scores[\"Bleu3\"].append(score[2])\n                    total_scores[\"Bleu4\"].append(score[3])\n\n                else:\n                    total_scores[method].append(score)\n\n        return total_scores","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_losses = list()\nval_losses = list()\n\ncumulative_bleu_scores = list()\n\ncider_scores = list()\n\nmeteor_scores = list()\n\nrougel_scores = list()\n\nbleu1_scores = list()\nbleu2_scores = list()\nbleu3_scores = list()\nbleu4_scores = list()\n\nval_iter = iter(val_loader)\n\nfor epoch in range(num_epochs):\n    model.train()\n\n    for batch_idx, (images, captions) in enumerate(train_loader):\n        images = images.to(device)\n        captions = captions.to(device)\n        \n        train_score = model(images, captions)\n\n        optimizer.zero_grad()\n        \n        train_loss = criterion(train_score.view(-1, vocab_size), captions.view(-1))\n        train_losses.append(train_loss.item())\n        \n        train_loss.backward()\n        \n        optimizer.step()\n\n    with torch.no_grad():\n        model.eval()\n\n        idx = val_set_start\n        \n        refs = {}\n        hyps = {}\n\n        while idx < val_set_end:\n            if (idx - val_set_start) % batch_size == 0:\n                batch = next(val_iter)\n\n            images, captions = batch\n\n            images = images.to(device)\n            captions = captions.to(device)\n\n            val_score = model(images, captions)\n\n            val_loss = criterion(val_score.view(-1, vocab_size), captions.view(-1))\n\n            val_losses.append(val_loss.item())\n            \n            for image in images:\n                val_pred = model.caption(image.unsqueeze(0), vocab)\n                \n                candidate = ' '.join(val_pred)\n\n                hyps[idx] = [candidate]\n                \n                label = all_labels[idx]\n\n                refs[idx] = [label]\n\n                cumulative_bleu_score = sentence_bleu(label, candidate, weights=(0.25, 0.25, 0.25, 0.25))\n                cumulative_bleu_scores.append(cumulative_bleu_score)\n            \n                idx += 1\n            \n        metrics = Scorer(refs, hyps).compute_scores()\n    \n        bleu1_scores.append(metrics['Bleu_1'])\n        bleu2_scores.append(metrics['Bleu_2'])\n        bleu3_scores.append(metrics['Bleu_3'])\n        bleu4_scores.append(metrics['Bleu_4'])\n\n        cider_scores.append(metrics['CIDEr'])\n\n        rougel_scores.append(metrics['ROUGE_L'])\n\n        meteor_scores.append(metrics['METEOR'])\n        \n        print(f\"Epoch [{epoch+1}/{num_epochs}] Batch [{batch_idx+1}/{len(train_loader)}] | Training loss: {train_loss.item()} Validation loss: {val_loss.item()} | Cumulative BLEU Score: {cumulative_bleu_score} CIDEr Score: {metrics['CIDEr']} METEOR Score: {metrics['METEOR']} ROUGE_L: {metrics['ROUGE_L']} \\n\")\n        \n        torch.save(model.state_dict(), f'/kaggle/working/ImageCap_{epoch+1}.pth')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Plotting Losses and Scores","metadata":{}},{"cell_type":"code","source":"plt.figure(0)\nplt.plot(train_losses, label = 'Training loss')\nplt.plot(val_losses, label = 'Validation loss')\nplt.ylabel('Cross Entropy Loss')\nplt.legend()\nplt.savefig(f'/kaggle/working/{freq_threshold}_{batch_size}_{hidden_size}_{num_epochs}_losses.png')\n\nplt.figure(1)\nplt.plot(bleu1_scores, label = 'BLEU 1')\nplt.plot(bleu2_scores, label = 'BLEU 2')\nplt.plot(bleu3_scores, label = 'BLEU 3')\nplt.plot(bleu4_scores, label = 'BLEU 4')\nplt.ylabel('BLEU Scores')\nplt.legend()\nplt.savefig(f'/kaggle/working/{freq_threshold}_{batch_size}_{hidden_size}_{num_epochs}_bleu_scores.png')\n        \nplt.figure(2)\nplt.plot(cumulative_bleu_scores, label = 'Cumulative BLEU SCORE')\nplt.plot(cider_scores, label = 'CIDEr SCORE')\nplt.plot(meteor_scores, label = 'METEOR SCORE')\nplt.plot(rougel_scores, label = 'ROUGE_L SCORE')\nplt.ylabel('Scores')\nplt.legend()\nplt.savefig(f'/kaggle/working/{freq_threshold}_{batch_size}_{hidden_size}_{num_epochs}_scores.png')","metadata":{},"execution_count":null,"outputs":[]}]}