{"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"sourceId":39911,"sourceType":"datasetVersion","datasetId":31296}],"dockerImageVersionId":30635,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Importing Libraries","metadata":{}},{"cell_type":"code","source":"!pip install torchsummary\n\nimport os\nimport math\nimport spacy\nimport torch\nimport numpy as np\nimport pandas as pd\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport torchvision.transforms as T\n\nfrom PIL import Image\nfrom torch import nn\nfrom torchsummary import summary\nfrom matplotlib import pyplot as plt\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torchvision.models import vgg16, VGG16_Weights\nfrom nltk.translate.bleu_score import sentence_bleu\n\nspacy_eng = spacy.load(\"en_core_web_sm\")","metadata":{"execution":{"iopub.status.busy":"2024-01-30T12:29:03.202854Z","iopub.execute_input":"2024-01-30T12:29:03.203442Z","iopub.status.idle":"2024-01-30T12:29:28.933854Z","shell.execute_reply.started":"2024-01-30T12:29:03.203398Z","shell.execute_reply":"2024-01-30T12:29:28.932875Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting torchsummary\n  Downloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\nInstalling collected packages: torchsummary\nSuccessfully installed torchsummary-1.5.1\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Setting the Device","metadata":{}},{"cell_type":"code","source":"if torch.cuda.is_available():\n    torch.backends.cudnn.deterministic = True\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\nprint('Device:', device)","metadata":{"execution":{"iopub.status.busy":"2024-01-30T12:29:31.824174Z","iopub.execute_input":"2024-01-30T12:29:31.825039Z","iopub.status.idle":"2024-01-30T12:29:31.833714Z","shell.execute_reply.started":"2024-01-30T12:29:31.824964Z","shell.execute_reply":"2024-01-30T12:29:31.832084Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Device: cpu\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Preparing the Flikr30k Dataset\n### Preparing the Vocabulary","metadata":{}},{"cell_type":"code","source":"class Vocabulary:\n    def __init__(self, freq_threshold):\n        self.freq_threshold = freq_threshold\n        self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n        self.stoi = {v: k for k,v in self.itos.items()}\n    \n    def __len__(self):\n        return len(self.itos)\n  \n    def build_vocab(self, sentence_list):\n        freqs = {}\n        idx = 4\n\n        for sentence in sentence_list:\n            sentence = str(sentence)\n\n            for word in self.tokenize(sentence):\n                if word not in freqs:\n                    freqs[word] = 1\n                    \n                else:\n                    freqs[word] += 1\n\n                if freqs[word] == self.freq_threshold:\n                    self.itos[idx] = word\n                    self.stoi[word] = idx\n                    \n                    idx += 1\n\n    def numericalize(self, sentence):\n        tokens = self.tokenize(sentence)\n        result = []\n\n        for token in tokens:\n            if token in self.stoi:\n                result.append(self.stoi[token])\n            else:\n                result.append(self.stoi[\"<UNK>\"])\n\n        return result\n    \n    @staticmethod\n    def tokenize(sentence):\n        return [token.text.lower() for token in spacy_eng.tokenizer(str(sentence))]","metadata":{"execution":{"iopub.status.busy":"2024-01-30T12:29:34.044768Z","iopub.execute_input":"2024-01-30T12:29:34.045330Z","iopub.status.idle":"2024-01-30T12:29:34.061532Z","shell.execute_reply.started":"2024-01-30T12:29:34.045280Z","shell.execute_reply":"2024-01-30T12:29:34.060456Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"### Defining a Custom Dataset","metadata":{}},{"cell_type":"code","source":"class Flickr(Dataset):\n    def __init__(self, root_dir, caption_path, transform, freq_threshold=5):\n        self.freq_threshold = freq_threshold\n        self.transform = transform\n        self.root_dir = root_dir\n    \n        self.df = pd.read_csv(caption_path, delimiter='|')\n        \n        self.images = self.df['image_name']\n        self.captions = self.df[' comment']\n        \n        self.vocab = Vocabulary(freq_threshold)\n        \n        self.vocab.build_vocab(self.captions.tolist())\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):\n        image = self.images[index]\n        caption = self.captions[index]\n        \n        image = Image.open(os.path.join(self.root_dir, image)).convert(\"RGB\")\n        \n        image = self.transform(image)\n        \n        numericalized_caption = [self.vocab.stoi[\"<SOS>\"]]\n        \n        numericalized_caption += self.vocab.numericalize(caption)\n        \n        numericalized_caption.append(self.vocab.stoi[\"<EOS>\"])\n        \n        return image, torch.tensor(numericalized_caption)\n    \n    def get_label(self, index):\n        image, caption = self[index]\n    \n        label = [self.vocab.itos[token] for token in caption.tolist()]\n\n        eos_index = label.index('<EOS>')\n\n        label = label[1: eos_index]\n\n        return ' '.join(label)\n    \n    def to_list(self):\n        return self.captions.tolist()\n        ","metadata":{"execution":{"iopub.status.busy":"2024-01-30T12:29:35.967657Z","iopub.execute_input":"2024-01-30T12:29:35.968087Z","iopub.status.idle":"2024-01-30T12:29:35.981191Z","shell.execute_reply.started":"2024-01-30T12:29:35.968054Z","shell.execute_reply":"2024-01-30T12:29:35.979727Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"### Defining a Custom Caption Collat for Padding","metadata":{}},{"cell_type":"code","source":"class CapCollat:\n    def __init__(self, pad_seq, batch_first=False):\n        self.pad_seq = pad_seq\n        self.batch_first = batch_first\n  \n    def __call__(self, batch):\n        imgs = [itm[0].unsqueeze(0) for itm in batch]\n        imgs = torch.cat(imgs, dim=0)\n\n        target_caps = [itm[1] for itm in batch]\n        target_caps = pad_sequence(target_caps, batch_first=self.batch_first,\n                                   padding_value=self.pad_seq)\n        \n        return imgs, target_caps","metadata":{"execution":{"iopub.status.busy":"2024-01-30T12:29:39.430776Z","iopub.execute_input":"2024-01-30T12:29:39.431181Z","iopub.status.idle":"2024-01-30T12:29:39.439071Z","shell.execute_reply.started":"2024-01-30T12:29:39.431132Z","shell.execute_reply":"2024-01-30T12:29:39.438004Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"### Loading and Testing the Dataset ","metadata":{}},{"cell_type":"code","source":"root_folder = \"/kaggle/input/flickr-image-dataset/flickr30k_images/flickr30k_images/\"\ncsv_file = \"/kaggle/input/flickr-image-dataset/flickr30k_images/results.csv\"\n\ntransform = T.Compose([\n        T.Resize((256, 256), interpolation=T.InterpolationMode.BILINEAR),\n        T.CenterCrop((224, 224)),\n        T.ToTensor(),\n        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\nbatch_size = 32\nnum_workers = 2\nfreq_threshold = 5\nbatch_first = True\npin_memory = True\ndataset = Flickr(root_folder, csv_file, transform, freq_threshold)\npad_idx = dataset.vocab.stoi[\"<PAD>\"]\n\ndata_size = len(dataset)\ntrain_size = int(0.9 * data_size)\nval_size = data_size - train_size\n\ntrain_set, val_set = torch.utils.data.Subset(dataset, range(0, train_size)), torch.utils.data.Subset(dataset, range(train_size, data_size))\n\ntrain_loader = DataLoader(train_set,\n                            batch_size=batch_size,\n                            pin_memory=pin_memory,\n                            num_workers=num_workers,\n                            shuffle=True,\n                            collate_fn=CapCollat(pad_seq=pad_idx, batch_first=batch_first))\n\nval_loader = DataLoader(val_set,\n                            batch_size=batch_size,\n                            pin_memory=pin_memory,\n                            num_workers=num_workers,\n                            shuffle=False,\n                            collate_fn=CapCollat(pad_seq=pad_idx, batch_first=batch_first))","metadata":{"execution":{"iopub.status.busy":"2024-01-30T12:34:39.802532Z","iopub.execute_input":"2024-01-30T12:34:39.802998Z","iopub.status.idle":"2024-01-30T12:35:32.606214Z","shell.execute_reply.started":"2024-01-30T12:34:39.802962Z","shell.execute_reply":"2024-01-30T12:35:32.604775Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"val_set_start = data_size - val_size - 1\nval_set_end = data_size - 1\n\nall_labels = dataset.to_list()","metadata":{"execution":{"iopub.status.busy":"2024-01-30T12:30:34.794981Z","iopub.status.idle":"2024-01-30T12:30:34.795445Z","shell.execute_reply.started":"2024-01-30T12:30:34.795234Z","shell.execute_reply":"2024-01-30T12:30:34.795255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for idx in range(0, 100, 10):\n    image, _ = dataset[idx + val_size]\n    \n    label = all_labels[idx + val_size]\n\n    image = image.permute(1,2,0)\n    \n    plt.imshow(image)\n    plt.title(label)\n\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Pre-Trained CNN Encoder: VGG16 ","metadata":{}},{"cell_type":"code","source":"class EncoderCNN(nn.Module):\n    def __init__(self, embed_size):\n        super(EncoderCNN, self).__init__()\n\n        vgg = vgg16(weights=VGG16_Weights.DEFAULT)\n        feature_extractor = list(vgg.children())[:-1]\n        \n        for param in vgg.parameters():\n            param.requires_grad = False\n            \n        embedding_layer = nn.Linear(512 * 7 * 7, embed_size)\n\n        self.vgg = nn.Sequential(*feature_extractor,\n                                 nn.Flatten(),\n                                 embedding_layer)\n       \n    def forward(self, image):\n        encoded_image = self.vgg(image)\n  \n        return encoded_image","metadata":{"execution":{"iopub.status.busy":"2024-01-30T12:36:32.284677Z","iopub.execute_input":"2024-01-30T12:36:32.285172Z","iopub.status.idle":"2024-01-30T12:36:32.296178Z","shell.execute_reply.started":"2024-01-30T12:36:32.285121Z","shell.execute_reply":"2024-01-30T12:36:32.294700Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"summary(EncoderCNN(256), (3, 224, 224))","metadata":{"execution":{"iopub.status.busy":"2024-01-30T12:36:33.660793Z","iopub.execute_input":"2024-01-30T12:36:33.661293Z","iopub.status.idle":"2024-01-30T12:36:36.641273Z","shell.execute_reply.started":"2024-01-30T12:36:33.661257Z","shell.execute_reply":"2024-01-30T12:36:36.640054Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1         [-1, 64, 224, 224]           1,792\n              ReLU-2         [-1, 64, 224, 224]               0\n            Conv2d-3         [-1, 64, 224, 224]          36,928\n              ReLU-4         [-1, 64, 224, 224]               0\n         MaxPool2d-5         [-1, 64, 112, 112]               0\n            Conv2d-6        [-1, 128, 112, 112]          73,856\n              ReLU-7        [-1, 128, 112, 112]               0\n            Conv2d-8        [-1, 128, 112, 112]         147,584\n              ReLU-9        [-1, 128, 112, 112]               0\n        MaxPool2d-10          [-1, 128, 56, 56]               0\n           Conv2d-11          [-1, 256, 56, 56]         295,168\n             ReLU-12          [-1, 256, 56, 56]               0\n           Conv2d-13          [-1, 256, 56, 56]         590,080\n             ReLU-14          [-1, 256, 56, 56]               0\n           Conv2d-15          [-1, 256, 56, 56]         590,080\n             ReLU-16          [-1, 256, 56, 56]               0\n        MaxPool2d-17          [-1, 256, 28, 28]               0\n           Conv2d-18          [-1, 512, 28, 28]       1,180,160\n             ReLU-19          [-1, 512, 28, 28]               0\n           Conv2d-20          [-1, 512, 28, 28]       2,359,808\n             ReLU-21          [-1, 512, 28, 28]               0\n           Conv2d-22          [-1, 512, 28, 28]       2,359,808\n             ReLU-23          [-1, 512, 28, 28]               0\n        MaxPool2d-24          [-1, 512, 14, 14]               0\n           Conv2d-25          [-1, 512, 14, 14]       2,359,808\n             ReLU-26          [-1, 512, 14, 14]               0\n           Conv2d-27          [-1, 512, 14, 14]       2,359,808\n             ReLU-28          [-1, 512, 14, 14]               0\n           Conv2d-29          [-1, 512, 14, 14]       2,359,808\n             ReLU-30          [-1, 512, 14, 14]               0\n        MaxPool2d-31            [-1, 512, 7, 7]               0\nAdaptiveAvgPool2d-32            [-1, 512, 7, 7]               0\n          Flatten-33                [-1, 25088]               0\n           Linear-34                  [-1, 256]       6,422,784\n================================================================\nTotal params: 21,137,472\nTrainable params: 6,422,784\nNon-trainable params: 14,714,688\n----------------------------------------------------------------\nInput size (MB): 0.57\nForward/backward pass size (MB): 218.78\nParams size (MB): 80.63\nEstimated Total Size (MB): 299.99\n----------------------------------------------------------------\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# RNN Decoder: Vanilla RNN Module","metadata":{}},{"cell_type":"code","source":"class DecoderRNN(nn.Module):\n    def __init__(self, embed_size, vocab_size, hidden_size, num_layers):\n        super(DecoderRNN, self).__init__()\n        \n        self.embedding = nn.Embedding(vocab_size, embed_size)\n        self.rnn = nn.RNN(embed_size, hidden_size, num_layers, batch_first=True)\n        self.linear = nn.Linear(hidden_size, vocab_size)\n    \n    def forward(self, features, caption):\n        caption_embed = self.embedding(caption)\n        caption_embed = torch.cat((features.unsqueeze(dim=1), caption_embed), 1)\n        output, hidden = self.rnn(caption_embed)\n        output = self.linear(output)\n\n        return output","metadata":{"execution":{"iopub.status.busy":"2024-01-30T12:37:40.190699Z","iopub.execute_input":"2024-01-30T12:37:40.191211Z","iopub.status.idle":"2024-01-30T12:37:40.201930Z","shell.execute_reply.started":"2024-01-30T12:37:40.191173Z","shell.execute_reply":"2024-01-30T12:37:40.200326Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"class ImageCap(nn.Module):\n    def __init__(self, embed_size, vocab_size, hidden_size, num_layers):\n        super(ImageCap, self).__init__()\n        \n        self.encoderCNN = EncoderCNN(embed_size)\n        self.decoderRNN = DecoderRNN(embed_size, vocab_size, hidden_size, num_layers)\n    \n    def forward(self, images, captions):\n        x = self.encoderCNN(images)\n        x = self.decoderRNN(x, captions)\n        \n        return x\n    \n    def caption(self, image, vocabulary, maxlength=50):\n        result_caption = []\n        \n        with torch.no_grad():\n            x = self.encoderCNN(image).unsqueeze(0)\n            states = None\n            \n            for _ in range(maxlength):\n                hiddens, states = self.decoderRNN.lstm(x, states)\n                output = self.decoderRNN.linear(hiddens.squeeze(0))\n                predicted = output.argmax(1)\n                print(predicted.shape)\n                result_caption.append(predicted.item())\n                x = self.decoderRNN.embedding(output).unsqueeze(0)\n                \n                if vocabulary.itos[predicted.item()] == \"<EOS>\":\n                    break\n                \n        return [vocabulary.itos[i] for i in result_caption]","metadata":{"execution":{"iopub.status.busy":"2024-01-30T12:42:42.439607Z","iopub.execute_input":"2024-01-30T12:42:42.440082Z","iopub.status.idle":"2024-01-30T12:42:42.452859Z","shell.execute_reply.started":"2024-01-30T12:42:42.440048Z","shell.execute_reply":"2024-01-30T12:42:42.451506Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"# Training and Evaluating the Image Captioning Model: VGG16 + Multi-layer Vanilla RNN","metadata":{}},{"cell_type":"markdown","source":"### Setting Hyperparameters","metadata":{}},{"cell_type":"code","source":"num_epochs = 10\nenc_dim = 2048\nembed_size = 256\nhidden_size = 512\nnum_layers = 1\nlearning_rate = 3e-4","metadata":{"execution":{"iopub.status.busy":"2024-01-30T12:41:59.016855Z","iopub.execute_input":"2024-01-30T12:41:59.017526Z","iopub.status.idle":"2024-01-30T12:41:59.023602Z","shell.execute_reply.started":"2024-01-30T12:41:59.017484Z","shell.execute_reply":"2024-01-30T12:41:59.022570Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"### Setting the Vocabulary","metadata":{}},{"cell_type":"code","source":"vocab = dataset.vocab\nvocab_size = len(vocab)","metadata":{"execution":{"iopub.status.busy":"2024-01-30T12:42:13.056105Z","iopub.execute_input":"2024-01-30T12:42:13.056588Z","iopub.status.idle":"2024-01-30T12:42:13.063573Z","shell.execute_reply.started":"2024-01-30T12:42:13.056554Z","shell.execute_reply":"2024-01-30T12:42:13.062003Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"### Configuring Models","metadata":{}},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss(ignore_index = vocab.stoi[\"<PAD>\"]).to(device)\n\nmodel = ImageCap(embed_size, vocab_size, hidden_size, num_layers).to(device)\n\noptimizer = optim.Adam(model.parameters(), lr = learning_rate)","metadata":{"execution":{"iopub.status.busy":"2024-01-30T12:42:45.931958Z","iopub.execute_input":"2024-01-30T12:42:45.933108Z","iopub.status.idle":"2024-01-30T12:42:48.565798Z","shell.execute_reply.started":"2024-01-30T12:42:45.933067Z","shell.execute_reply":"2024-01-30T12:42:48.564462Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"### Training Models","metadata":{}},{"cell_type":"code","source":"!pip install pycocoevalcap","metadata":{"execution":{"iopub.status.busy":"2024-01-30T12:43:17.536712Z","iopub.execute_input":"2024-01-30T12:43:17.537117Z","iopub.status.idle":"2024-01-30T12:43:35.594764Z","shell.execute_reply.started":"2024-01-30T12:43:17.537086Z","shell.execute_reply":"2024-01-30T12:43:35.593494Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"Collecting pycocoevalcap\n  Downloading pycocoevalcap-1.2-py3-none-any.whl (104.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.3/104.3 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting pycocotools>=2.0.2 (from pycocoevalcap)\n  Obtaining dependency information for pycocotools>=2.0.2 from https://files.pythonhosted.org/packages/ba/64/0451cf41a00fd5ac4501de4ea0e395b7d909e09d665e56890b5d3809ae26/pycocotools-2.0.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n  Downloading pycocotools-2.0.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.1 kB)\nRequirement already satisfied: matplotlib>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from pycocotools>=2.0.2->pycocoevalcap) (3.7.4)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from pycocotools>=2.0.2->pycocoevalcap) (1.24.3)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (1.1.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (0.11.0)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (4.42.1)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (1.4.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (21.3)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (9.5.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (2.8.2)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (1.16.0)\nDownloading pycocotools-2.0.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (426 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m426.2/426.2 kB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: pycocotools, pycocoevalcap\nSuccessfully installed pycocoevalcap-1.2 pycocotools-2.0.7\n","output_type":"stream"}]},{"cell_type":"code","source":"from pycocoevalcap.bleu.bleu import Bleu\nfrom pycocoevalcap.meteor.meteor import Meteor\nfrom pycocoevalcap.rouge.rouge import Rouge\nfrom pycocoevalcap.cider.cider import Cider\n\nclass Scorer():\n    def __init__(self,ref,gt):\n        self.ref = ref\n        self.gt = gt\n\n        self.word_based_scorers = [\n            (Bleu(4), [\"Bleu_1\", \"Bleu_2\", \"Bleu_3\", \"Bleu_4\"]),\n            (Meteor(),\"METEOR\"),\n            (Rouge(), \"ROUGE_L\"),\n            (Cider(), \"CIDEr\"),\n            ]\n\n    def compute_scores(self):\n        total_scores = {\n            \"Bleu1\":[],\n            \"Bleu2\":[],\n            \"Bleu3\":[],\n            \"Bleu4\":[],\n            \"METEOR\":[],\n            \"ROUGE_L\":[],\n            \"CIDEr\":[],\n        }\n\n        for scorer, method in self.word_based_scorers:\n            score, scores = scorer.compute_score(self.ref, self.gt)\n    \n            if type(method) == list:\n                total_scores[\"Bleu1\"].append(score[0])\n                total_scores[\"Bleu2\"].append(score[1])\n                total_scores[\"Bleu3\"].append(score[2])\n                total_scores[\"Bleu4\"].append(score[3])\n\n            else:\n                total_scores[method].append(score)\n\n        return total_scores\n    \n    def compute_scores_iterative(self):\n        total_scores = {\n            \"Bleu1\":[],\n            \"Bleu2\":[],\n            \"Bleu3\":[],\n            \"Bleu4\":[],\n            \"METEOR\":[],\n            \"ROUGE_L\":[],\n            \"CIDEr\":[],\n            \"SPICE\":[]\n        \n        }\n\n        for key in self.ref:\n            curr_ref = {key:self.ref[key]}\n            curr_gt = {key:self.gt[key]}\n\n            for scorer, method in self.word_based_scorers:\n                score, _ = scorer.compute_score(curr_ref, curr_gt)\n                if type(method) == list:\n                    total_scores[\"Bleu1\"].append(score[0])\n                    total_scores[\"Bleu2\"].append(score[1])\n                    total_scores[\"Bleu3\"].append(score[2])\n                    total_scores[\"Bleu4\"].append(score[3])\n\n                else:\n                    total_scores[method].append(score)\n\n        return total_scores","metadata":{"execution":{"iopub.status.busy":"2024-01-30T12:43:38.737709Z","iopub.execute_input":"2024-01-30T12:43:38.738266Z","iopub.status.idle":"2024-01-30T12:43:38.770088Z","shell.execute_reply.started":"2024-01-30T12:43:38.738224Z","shell.execute_reply":"2024-01-30T12:43:38.768426Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"train_losses = list()\nval_losses = list()\n\ncumulative_bleu_scores = list()\n\ncider_scores = list()\n\nmeteor_scores = list()\n\nrougel_scores = list()\n\nbleu1_scores = list()\nbleu2_scores = list()\nbleu3_scores = list()\nbleu4_scores = list()\n\nval_iter = iter(val_loader)\n\nfor epoch in range(num_epochs):\n    model.train()\n\n    for batch_idx, (images, captions) in enumerate(train_loader):\n        images = images.to(device)\n        captions = captions.to(device)\n        \n        train_score = model(images, captions)\n\n        optimizer.zero_grad()\n        \n        print((train_score.view(-1, vocab_size)).size())\n        print((captions.view(-1)).size())\n        \n        train_loss = criterion(train_score.view(-1, vocab_size), captions.view(-1))\n        train_losses.append(train_loss.item())\n        \n        train_loss.backward()\n        \n        optimizer.step()\n\n    with torch.no_grad():\n        model.eval()\n\n        idx = val_set_start\n\n        hyp = {}\n        ref = {}\n\n        while idx <= val_set_end:\n            if idx % batch_size == 0:\n                batch = next(val_iter)\n\n            val_images, val_captions = batch\n\n            image, caption = val_images[idx % batch_size], val_captions[idx % batch_size]\n\n            image = image.to_device()\n            caption = caption.to_device()\n\n            val_score = model(image, caption[: -1])\n\n            val_loss = criterion(val_score.reshape(-1, val_score.shape[2]), val_captions[1:].reshape(-1))\n\n            val_losses.append(val_loss.item())\n\n            val_pred = model.caption_image(image, vocab)\n\n            hyp[idx] = [' '.join(val_pred)]\n\n            label = all_labels[idx]\n\n            ref[idx] = label\n\n            cumulative_bleu_score = sentence_bleu(label, val_pred, weights=(0.25, 0.25, 0.25, 0.25))\n            cumulative_bleu_scores.append(cumulative_bleu_score)\n            \n            idx += 1\n            \n        metrics = Scorer(ref, hyp).compute_scores()\n    \n        bleu1_scores.append(metrics['Bleu_1'])\n        bleu2_scores.append(metrics['Bleu_2'])\n        bleu3_scores.append(metrics['Bleu_3'])\n        bleu4_scores.append(metrics['Bleu_4'])\n\n        cider_scores.append(metrics['CIDEr'])\n\n        rougel_scores.append(metrics['ROUGE_L'])\n\n        meteor_scores.append(metrics['METEOR'])\n        \n        print(f\"Epoch [{epoch+1}/{num_epochs}] Batch [{batch_idx+1}/{len(train_loader)}] | Training loss: {train_loss.item()} Validation loss: {val_loss.item()} | Cumulative BLEU Score: {cumulative_bleu_score} CIDEr Score: {metrics['CIDEr']} METEOR Score: {metrics['METEOR']} ROUGE_L: {metrics['ROUGE_L']} \\n\")\n\n# if batch_idx == (len(train_loader) - 1):\n#     torch.save(model.state_dict(), f'/kaggle/working/encoder_{freq_threshold}_{batch_size}_{hidden_size}_{epoch+1}.pth')\n","metadata":{"execution":{"iopub.status.busy":"2024-01-30T12:44:44.504282Z","iopub.execute_input":"2024-01-30T12:44:44.504750Z","iopub.status.idle":"2024-01-30T12:44:58.537248Z","shell.execute_reply.started":"2024-01-30T12:44:44.504717Z","shell.execute_reply":"2024-01-30T12:44:58.535357Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"torch.Size([928, 7666])\ntorch.Size([896])\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[28], line 33\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m((train_score\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, vocab_size))\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m((captions\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m---> 33\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_score\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m train_losses\u001b[38;5;241m.\u001b[39mappend(train_loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     36\u001b[0m train_loss\u001b[38;5;241m.\u001b[39mbackward()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/loss.py:1174\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1173\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1175\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1176\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:3029\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3027\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3028\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3029\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mValueError\u001b[0m: Expected input batch_size (928) to match target batch_size (896)."],"ename":"ValueError","evalue":"Expected input batch_size (928) to match target batch_size (896).","output_type":"error"}]},{"cell_type":"markdown","source":"### Plotting Losses and Scores","metadata":{}},{"cell_type":"code","source":"plt.figure(0)\nplt.plot(train_losses, label = 'Training loss')\nplt.plot(val_losses, label = 'Validation loss')\nplt.ylabel('Cross Entropy Loss')\nplt.legend()\nplt.savefig(f'/kaggle/working/{freq_threshold}_{batch_size}_{hidden_size}_{num_epochs}_losses.png')\n\nplt.figure(1)\nplt.plot(bleu1_scores, label = 'BLEU 1')\nplt.plot(bleu2_scores, label = 'BLEU 2')\nplt.plot(bleu3_scores, label = 'BLEU 3')\nplt.plot(bleu4_scores, label = 'BLEU 4')\nplt.ylabel('BLEU Scores')\nplt.legend()\nplt.savefig(f'/kaggle/working/{freq_threshold}_{batch_size}_{hidden_size}_{num_epochs}_bleu_scores.png')\n        \nplt.figure(2)\nplt.plot(cumulative_bleu_scores, label = 'Cumulative BLEU SCORE')\nplt.plot(cider_scores, label = 'CIDEr SCORE')\nplt.plot(meteor_scores, label = 'METEOR SCORE')\nplt.plot(rougel_scores, label = 'ROUGE_L SCORE')\nplt.ylabel('Scores')\nplt.legend()\nplt.savefig(f'/kaggle/working/{freq_threshold}_{batch_size}_{hidden_size}_{num_epochs}_scores.png')","metadata":{},"execution_count":null,"outputs":[]}]}