{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Importing Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import os\n","import math\n","import spacy\n","import torch\n","import numpy as np\n","import pandas as pd\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import torchvision.transforms as T\n","\n","from PIL import Image\n","from torch import nn\n","from matplotlib import pyplot as plt\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","from torch.nn.utils.rnn import pad_sequence\n","from torchvision.models import vgg16, VGG16_Weights\n","from nltk.translate.bleu_score import sentence_bleu\n","\n","spacy_eng = spacy.load(\"en_core_web_sm\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!pip install torchsummary\n","!pip install pycocoevalcap\n","\n","from torchsummary import summary\n","from pycocoevalcap.bleu.bleu import Bleu\n","from pycocoevalcap.meteor.meteor import Meteor\n","from pycocoevalcap.rouge.rouge import Rouge\n","from pycocoevalcap.cider.cider import Cider"]},{"cell_type":"markdown","metadata":{},"source":["### Setting the Device"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["if torch.cuda.is_available():\n","    torch.backends.cudnn.deterministic = True\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","print('Device:', device)"]},{"cell_type":"markdown","metadata":{},"source":["# Preparing the Flikr30k Dataset\n","### Preparing the Vocabulary"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class Vocabulary:\n","    def __init__(self, freq_threshold):\n","        self.freq_threshold = freq_threshold\n","        self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n","        self.stoi = {v: k for k, v in self.itos.items()}\n","    \n","    def __len__(self):\n","        return len(self.itos)\n","  \n","    def build_vocab(self, sentence_list):\n","        freqs = {}\n","        idx = 4\n","\n","        for sentence in sentence_list:\n","            sentence = str(sentence)\n","\n","            for word in self.tokenize(sentence):\n","                freqs[word] = freqs.get(word, 0) + 1\n","\n","                if freqs[word] == self.freq_threshold:\n","                    self.itos[idx] = word\n","                    self.stoi[word] = idx\n","                    \n","                    idx += 1\n","\n","    def numericalize(self, sentence):\n","        tokens = self.tokenize(sentence)\n","        result = []\n","\n","        for token in tokens:\n","            result.append(self.stoi.get(token, self.stoi[\"<UNK>\"]))\n","\n","        return result\n","    \n","    @staticmethod\n","    def tokenize(sentence):\n","        return [token.text.lower() for token in spacy_eng.tokenizer(str(sentence))]"]},{"cell_type":"markdown","metadata":{},"source":["### Defining a Custom Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class Flickr(Dataset):\n","    def __init__(self, root_dir, caption_path, transform, freq_threshold=5):\n","        self.freq_threshold = freq_threshold\n","        self.transform = transform\n","        self.root_dir = root_dir\n","    \n","        self.df = pd.read_csv(caption_path, delimiter='|')\n","        \n","        self.images = self.df['image_name']\n","        self.captions = self.df[' comment']\n","        \n","        self.vocab = Vocabulary(freq_threshold)\n","        \n","        self.vocab.build_vocab(self.captions.tolist())\n","        \n","    def __len__(self):\n","        return len(self.df)\n","    \n","    def __getitem__(self, index):\n","        max_seq_length = 50\n","        \n","        image, caption = self.images[index], self.captions[index]\n","        \n","        image = Image.open(os.path.join(self.root_dir, image)).convert(\"RGB\")\n","        \n","        image = self.transform(image)\n","        \n","        numericalized_caption = [self.vocab.stoi[\"<SOS>\"]]\n","        \n","        numericalized_caption += self.vocab.numericalize(caption)\n","        \n","        numericalized_caption.append(self.vocab.stoi[\"<EOS>\"])\n","        \n","        if len(numericalized_caption) > max_seq_length:\n","            numericalized_caption = numericalized_caption[: max_seq_length]\n","        else:\n","            numericalized_caption += [self.vocab.stoi[\"<PAD>\"]] * (max_seq_length - len(numericalized_caption))\n","        \n","        return image, torch.tensor(numericalized_caption)\n","    \n","    def get_label(self, index):\n","        _, caption = self[index]\n","    \n","        label = [self.vocab.itos[token] for token in caption.tolist()][1: -1]\n","\n","        return ' '.join(label)\n","    \n","    def captions_list(self):\n","        return self.captions.tolist()\n","        "]},{"cell_type":"markdown","metadata":{},"source":["### Defining a Custom Caption Collat for Padding"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class CapCollat:\n","    def __init__(self, pad_seq, batch_first=False):\n","        self.pad_seq = pad_seq\n","        self.batch_first = batch_first\n","  \n","    def __call__(self, batch):\n","        images = [item[0].unsqueeze(0) for item in batch]\n","        images = torch.cat(images, dim=0)\n","\n","        target_caps = [item[1] for item in batch]\n","        target_caps = pad_sequence(target_caps,\n","                                   batch_first=self.batch_first,\n","                                   padding_value=self.pad_seq)\n","        \n","        return images, target_caps"]},{"cell_type":"markdown","metadata":{},"source":["### Defining Custom Scorer "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class Scorer():\n","    def __init__(self, references, candidates):\n","        self.references = references\n","        self.candidates = candidates\n","\n","        self.word_based_scorers = [\n","            (Bleu(4), [\"Bleu_1\", \"Bleu_2\", \"Bleu_3\", \"Bleu_4\"]),\n","            (Meteor(),\"METEOR\"),\n","            (Rouge(), \"ROUGE_L\"),\n","            (Cider(), \"CIDEr\"),\n","            ]\n","\n","    def compute_scores(self):\n","        total_scores = {\n","            \"Bleu1\":[],\n","            \"Bleu2\":[],\n","            \"Bleu3\":[],\n","            \"Bleu4\":[],\n","            \"METEOR\":[],\n","            \"ROUGE_L\":[],\n","            \"CIDEr\":[],\n","        }\n","\n","        for scorer, method in self.word_based_scorers:\n","            score, _ = scorer.compute_score(self.references, self.candidates)\n","    \n","            if type(method) is list:\n","                total_scores[\"Bleu1\"].append(score[0])\n","                total_scores[\"Bleu2\"].append(score[1])\n","                total_scores[\"Bleu3\"].append(score[2])\n","                total_scores[\"Bleu4\"].append(score[3])\n","\n","            else:\n","                total_scores[method].append(score)\n","\n","        return total_scores\n","    \n","    def compute_scores_iterative(self):\n","        total_scores = {\n","            \"Bleu1\":[],\n","            \"Bleu2\":[],\n","            \"Bleu3\":[],\n","            \"Bleu4\":[],\n","            \"METEOR\":[],\n","            \"ROUGE_L\":[],\n","            \"CIDEr\":[],\n","            \"SPICE\":[]\n","        \n","        }\n","\n","        for key in self.candidates:\n","            curr_reference = {key:self.references[key]}\n","            curr_candidate = {key:self.candidates[key]}\n","\n","            for scorer, method in self.word_based_scorers:\n","                score, _ = scorer.compute_score(curr_reference, curr_candidate)\n","                if type(method) == list:\n","                    total_scores[\"Bleu1\"].append(score[0])\n","                    total_scores[\"Bleu2\"].append(score[1])\n","                    total_scores[\"Bleu3\"].append(score[2])\n","                    total_scores[\"Bleu4\"].append(score[3])\n","\n","                else:\n","                    total_scores[method].append(score)\n","\n","        return total_scores"]},{"cell_type":"markdown","metadata":{},"source":["### Loading and Testing the Dataset "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["root_folder = \"/kaggle/input/flickr-image-dataset/flickr30k_images/flickr30k_images/\"\n","csv_file = \"/kaggle/input/flickr-image-dataset/flickr30k_images/results.csv\"\n","\n","transform = T.Compose([\n","                T.Resize((256, 256), interpolation=T.InterpolationMode.BILINEAR),\n","                T.CenterCrop((224, 224)),\n","                T.ToTensor(),\n","                T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","                ])\n","\n","batch_size = 37\n","num_workers = 2\n","freq_threshold = 5\n","batch_first = True\n","pin_memory = True\n","\n","dataset = Flickr(root_folder, csv_file, transform, freq_threshold)\n","all_labels = dataset.captions_list()\n","pad_idx = dataset.vocab.stoi[\"<PAD>\"]\n","\n","data_size = len(dataset)\n","train_size = int(0.9 * data_size)\n","val_size = data_size - train_size\n","\n","val_set_start = data_size - val_size - 1\n","val_set_end = data_size - 1\n","\n","train_set, val_set = torch.utils.data.Subset(dataset, range(0, train_size)), torch.utils.data.Subset(dataset, range(train_size, data_size))\n","\n","train_loader = DataLoader(train_set,\n","                            batch_size=batch_size,\n","                            pin_memory=pin_memory,\n","                            num_workers=num_workers,\n","                            shuffle=True,\n","                            collate_fn=CapCollat(pad_seq=pad_idx, batch_first=batch_first))\n","\n","val_loader = DataLoader(val_set,\n","                            batch_size=batch_size,\n","                            pin_memory=pin_memory,\n","                            num_workers=num_workers,\n","                            shuffle=False,\n","                            collate_fn=CapCollat(pad_seq=pad_idx, batch_first=batch_first))"]},{"cell_type":"markdown","metadata":{},"source":["### Setting Vocabulary"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["vocab = dataset.vocab\n","vocab_size = len(vocab)"]},{"cell_type":"markdown","metadata":{},"source":["### Testing Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["for idx in range(0, 100, 10):\n","    image, _ = dataset[idx + val_size]\n","    \n","    label = all_labels[idx + val_size]\n","\n","    image = image.permute(1,2,0)\n","    \n","    plt.imshow(image)\n","    plt.title(label)\n","\n","    plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["# Encoder: Pre-Trained VGG16 "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class Encoder(nn.Module):\n","    def __init__(self, embed_size=224):\n","        super(Encoder, self).__init__()\n","\n","        vgg = vgg16(weights=VGG16_Weights.IMAGENET1K_FEATURES)\n","        \n","        for param in vgg.parameters():\n","            param.requires_grad = False\n","            \n","        feature_extractor = list(vgg.children())[:-1]\n","            \n","        embedding_layer = nn.Linear(512 * 7 * 7, embed_size)\n","\n","        self.encoder = nn.Sequential(*feature_extractor,\n","                                     nn.Flatten(),\n","                                     embedding_layer)\n","       \n","    def forward(self, image):\n","        encoded_image = self.encoder(image)\n","  \n","        return encoded_image"]},{"cell_type":"markdown","metadata":{},"source":["# Decoder: Vanilla RNN"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class Decoder(nn.Module):\n","    def __init__(self, embed_size, vocab_size, hidden_size, num_layers):\n","        super(Decoder, self).__init__()\n","\n","        self.embedding = nn.Embedding(vocab_size, embed_size)\n","        self.rnn = nn.RNN(embed_size, hidden_size, num_layers, batch_first=True)\n","        self.linear = nn.Linear(hidden_size, vocab_size)\n","\n","        self.features2hidden = nn.Linear(embed_size, hidden_size)\n","\n","    def forward(self, features, captions):\n","        captions_embed = self.embedding(captions)\n","\n","        initial_hidden_state = self.features2hidden(features).unsqueeze(0).repeat(self.rnn.num_layers, 1, 1)\n","\n","        output, _ = self.rnn(captions_embed, initial_hidden_state)\n","        output = self.linear(output)\n","\n","        return output\n"]},{"cell_type":"markdown","metadata":{},"source":["# Encoder-Decoder: VGG16 + Vanilla RNN"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class EncoderDecoder(nn.Module):\n","    def __init__(self, embed_size, vocab_size, hidden_size, num_layers):\n","        super(EncoderDecoder, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","        \n","        self.encoder = Encoder(embed_size)\n","        self.decoder = Decoder(embed_size, vocab_size, hidden_size, num_layers)\n","    \n","    def forward(self, images, captions):\n","        x = self.encoder(images)\n","        x = self.decoder(x, captions)\n","        \n","        return x\n","    \n","    def caption(self, image, vocabulary, maxlength=50):\n","        result_caption = []\n","\n","        with torch.no_grad():\n","            x = self.encoder(image).unsqueeze(0)\n","            states = None\n","\n","            for _ in range(maxlength):\n","                hiddens, states = self.decoder.rnn(x, states)\n","                output = self.decoder.linear(hiddens.squeeze(0))\n","                predicted = output.argmax(1)\n","                result_caption.append(predicted.item())\n","                x = self.decoder.embedding(predicted).unsqueeze(0)\n","\n","                if vocabulary.itos[predicted.item()] == \"<EOS>\":\n","                    break\n","\n","        return [vocabulary.itos[i] for i in result_caption]\n"]},{"cell_type":"markdown","metadata":{},"source":["# Training and Evaluating"]},{"cell_type":"markdown","metadata":{},"source":["### Setting Hyperparameters"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["num_epochs = 4\n","enc_dim = 2048\n","embed_size = 224\n","hidden_size = 512\n","num_layers = 1\n","learning_rate = 3e-4"]},{"cell_type":"markdown","metadata":{},"source":["### Configuring Model"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["criterion = nn.CrossEntropyLoss(ignore_index = vocab.stoi[\"<PAD>\"]).to(device)\n","\n","model = EncoderDecoder(embed_size, vocab_size, hidden_size, num_layers).to(device)\n","\n","optimizer = optim.Adam(model.parameters(), lr = learning_rate)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(\"ImageCap Model Summary: VGG16 + Vanilla RNN\")\n","summary(model, (3, 224, 224))\n","print(\"\\n--------------------------------------------------\\n\")\n","print(\"Encoder Model Summary: VGG16\")\n","summary(model.encoder, (3, 224, 224))\n","print(\"\\n--------------------------------------------------\\n\")\n","print(\"Decoder Model Summary: Vanilla RNN\")\n","summary(model.decoder, (1, 512))"]},{"cell_type":"markdown","metadata":{},"source":["### Training The Model"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_losses = []\n","val_losses = []\n","\n","bleu1_scores = []\n","bleu2_scores = []\n","bleu3_scores = []\n","bleu4_scores = []\n","\n","cider_scores = []\n","\n","meteor_scores = []\n","\n","rougel_scores = []\n","\n","val_iter = iter(val_loader)\n","\n","for epoch in range(num_epochs):\n","    model.train()\n","\n","    for batch_idx, (images, captions) in enumerate(train_loader):\n","        images, captions = images.to(device), captions.to(device)\n","        \n","        train_score = model(images, captions)\n","\n","        optimizer.zero_grad()\n","        \n","        train_loss = criterion(train_score.view(-1, vocab_size), captions.view(-1))\n","        train_losses.append(train_loss.item())\n","        \n","        train_loss.backward()\n","        \n","        optimizer.step()\n","\n","    with torch.no_grad():\n","        model.eval()\n","\n","        idx = val_set_start\n","        \n","        references = {}\n","        candidates = {}\n","\n","        while idx < val_set_end:\n","            if (idx - val_set_start) % batch_size == 0:\n","                batch = next(val_iter)\n","\n","            images, captions = batch\n","\n","            images, captions = images.to(device), captions.to(device)\n","\n","            val_score = model(images, captions)\n","\n","            val_loss = criterion(val_score.view(-1, vocab_size), captions.view(-1))\n","\n","            val_losses.append(val_loss.item())\n","            \n","            for image in images:\n","                val_pred = model.caption(image.unsqueeze(0), vocab)\n","                \n","                candidate = ' '.join(val_pred)\n","\n","                candidates[idx] = [candidate]\n","                \n","                label = all_labels[idx]\n","\n","                references[idx] = [label]\n","            \n","                idx += 1\n","            \n","    metrics = Scorer(references, candidates).compute_scores()\n","\n","    bleu1_scores.append(metrics['Bleu_1'])\n","    bleu2_scores.append(metrics['Bleu_2'])\n","    bleu3_scores.append(metrics['Bleu_3'])\n","    bleu4_scores.append(metrics['Bleu_4'])\n","\n","    cumulative_bleu_score = (metrics['Bleu_1'] / 4) + (metrics['Bleu_2'] / 4) + (metrics['Bleu_3'] / 4) + (metrics['Bleu_4'] / 4)\n","\n","    cider_scores.append(metrics['CIDEr'])\n","\n","    rougel_scores.append(metrics['ROUGE_L'])\n","\n","    meteor_scores.append(metrics['METEOR'])\n","    \n","    print(f\"Epoch [{epoch + 1}/{num_epochs}] | Training loss: {train_loss.item()} Validation loss: {val_loss.item()} \\nCumulative BLEU Score: {cumulative_bleu_score}\\tCIDEr Score: {metrics['CIDEr']}\\tMETEOR Score: {metrics['METEOR']}\\tROUGE_L: {metrics['ROUGE_L']}\\n\")\n","    \n","    torch.save(model.state_dict(), f'/kaggle/working/ImageCap_{epoch + 1}.pth')\n"]},{"cell_type":"markdown","metadata":{},"source":["### Plotting Cross-Validation and Scores"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.figure(0)\n","plt.plot(train_losses, label = 'Training loss')\n","plt.plot(val_losses, label = 'Validation loss')\n","plt.ylabel('Cross Entropy Loss')\n","plt.legend()\n","plt.savefig(f'/kaggle/working/{freq_threshold}_{batch_size}_{hidden_size}_{num_epochs}_losses.png')\n","\n","plt.figure(1)\n","plt.plot(bleu1_scores, label = 'BLEU 1')\n","plt.plot(bleu2_scores, label = 'BLEU 2')\n","plt.plot(bleu3_scores, label = 'BLEU 3')\n","plt.plot(bleu4_scores, label = 'BLEU 4')\n","plt.ylabel('BLEU Scores')\n","plt.legend()\n","plt.savefig(f'/kaggle/working/{freq_threshold}_{batch_size}_{hidden_size}_{num_epochs}_bleu_scores.png')\n","        \n","plt.figure(2)\n","plt.plot(cider_scores, label = 'CIDEr')\n","plt.plot(meteor_scores, label = 'METEOR')\n","plt.plot(rougel_scores, label = 'ROUGE_L')\n","plt.ylabel('Scores')\n","plt.legend()\n","plt.savefig(f'/kaggle/working/{freq_threshold}_{batch_size}_{hidden_size}_{num_epochs}_scores.png')\n","\n","plt.show()"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":31296,"sourceId":39911,"sourceType":"datasetVersion"}],"dockerImageVersionId":30635,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
