{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Importing Libraries"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-01-26T04:57:46.575742Z","iopub.status.busy":"2024-01-26T04:57:46.571466Z","iopub.status.idle":"2024-01-26T04:57:55.840874Z","shell.execute_reply":"2024-01-26T04:57:55.839853Z","shell.execute_reply.started":"2024-01-26T04:57:46.575681Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n","  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"]}],"source":["import os\n","import math\n","import spacy\n","import torch\n","import numpy as np\n","import pandas as pd\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import torchvision.transforms as T\n","\n","from PIL import Image\n","from torch import nn\n","from matplotlib import pyplot as plt\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","from torch.nn.utils.rnn import pad_sequence\n","from torchvision.models import vgg16, VGG16_Weights\n","from nltk.translate.bleu_score import sentence_bleu\n","\n","spacy_eng = spacy.load(\"en_core_web_sm\")"]},{"cell_type":"markdown","metadata":{},"source":["# Preparing the Flikr30k Dataset\n","### Preparing the Vocabulary"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-01-25T17:33:17.650369Z","iopub.status.busy":"2024-01-25T17:33:17.649874Z","iopub.status.idle":"2024-01-25T17:33:17.664683Z","shell.execute_reply":"2024-01-25T17:33:17.662854Z","shell.execute_reply.started":"2024-01-25T17:33:17.650333Z"},"trusted":true},"outputs":[],"source":["class Vocabulary:\n","    def __init__(self, freq_threshold):\n","        self.freq_threshold = freq_threshold\n","        self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n","        self.stoi = {v: k for k,v in self.itos.items()}\n","    \n","    def __len__(self):\n","        return len(self.itos)\n","  \n","    def build_vocab(self, sentence_list):\n","        freqs = {}\n","        idx = 4\n","\n","        for sentence in sentence_list:\n","            sentence = str(sentence)\n","\n","            for word in self.tokenize(sentence):\n","                if word not in freqs:\n","                    freqs[word] = 1\n","                    \n","                else:\n","                    freqs[word] += 1\n","\n","                if freqs[word] == self.freq_threshold:\n","                    self.itos[idx] = word\n","                    self.stoi[word] = idx\n","                    idx += 1\n","\n","    def numericalize(self, sentence):\n","        tokens = self.tokenize(sentence)\n","        result = []\n","\n","        for token in tokens:\n","            if token in self.stoi:\n","                result.append(self.stoi[token])\n","            else:\n","                result.append(self.stoi[\"<UNK>\"])\n","\n","        return result\n","    \n","    @staticmethod\n","    def tokenize(sentence):\n","        return [token.text.lower() for token in spacy_eng.tokenizer(sentence)]"]},{"cell_type":"markdown","metadata":{},"source":["### Defining a Custom Dataset"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-01-25T17:33:19.752517Z","iopub.status.busy":"2024-01-25T17:33:19.752114Z","iopub.status.idle":"2024-01-25T17:33:19.765188Z","shell.execute_reply":"2024-01-25T17:33:19.763640Z","shell.execute_reply.started":"2024-01-25T17:33:19.752486Z"},"trusted":true},"outputs":[],"source":["class Flickr(Dataset):\n","    def __init__(self, root_dir, caps, transforms=None, freq_threshold=5):\n","        self.root_dir = root_dir\n","        self.df = pd.read_csv(caps, delimiter='|')\n","        self.transforms = transforms\n","\n","        self.img_pts = self.df['image_name']\n","        self.caps = self.df[' comment']\n","        self.vocab = Vocabulary(freq_threshold)\n","        self.vocab.build_vocab(self.caps.tolist())\n","\n","    def __len__(self):\n","        return len(self.df)\n","  \n","    def __getitem__(self, idx):\n","        captions = self.caps[idx]\n","        img_pt = self.img_pts[idx]\n","        img = Image.open(os.path.join(self.root_dir, img_pt)).convert('RGB')\n","\n","        if self.transforms is not None:\n","            img = self.transforms(img)\n","\n","        numberized_caps = []\n","        numberized_caps += [self.vocab.stoi[\"<SOS>\"]]\n","        numberized_caps += self.vocab.numericalize(captions)\n","        numberized_caps += [self.vocab.stoi[\"<EOS>\"]]\n","        \n","        return img, torch.tensor(numberized_caps)"]},{"cell_type":"markdown","metadata":{},"source":["### Defining a Custom Caption Collat for Padding"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-01-25T17:33:22.708928Z","iopub.status.busy":"2024-01-25T17:33:22.708460Z","iopub.status.idle":"2024-01-25T17:33:22.718258Z","shell.execute_reply":"2024-01-25T17:33:22.716600Z","shell.execute_reply.started":"2024-01-25T17:33:22.708894Z"},"trusted":true},"outputs":[],"source":["class CapCollat:\n","    def __init__(self, pad_seq, batch_first=False):\n","        self.pad_seq = pad_seq\n","        self.batch_first = batch_first\n","  \n","    def __call__(self, batch):\n","        imgs = [itm[0].unsqueeze(0) for itm in batch]\n","        imgs = torch.cat(imgs, dim=0)\n","\n","        target_caps = [itm[1] for itm in batch]\n","        target_caps = pad_sequence(target_caps, batch_first=self.batch_first,\n","                                   padding_value=self.pad_seq)\n","        return imgs, target_caps"]},{"cell_type":"markdown","metadata":{},"source":["### Loading and Testing the Dataset "]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-01-25T17:41:04.841334Z","iopub.status.busy":"2024-01-25T17:41:04.839931Z","iopub.status.idle":"2024-01-25T17:42:00.107076Z","shell.execute_reply":"2024-01-25T17:42:00.105547Z","shell.execute_reply.started":"2024-01-25T17:41:04.841277Z"},"trusted":true},"outputs":[],"source":["batch_size = 32\n","root_folder = \"/kaggle/input/flickr-image-dataset/flickr30k_images/flickr30k_images/\"\n","csv_file = \"/kaggle/input/flickr-image-dataset/flickr30k_images/results.csv\"\n","\n","transforms = T.Compose([\n","                        T.Resize((224,224)),\n","                        T.ToTensor(),\n","                        T.Normalize(mean=[0.485, 0.456, 0.406],\n","                                    std=[0.229, 0.224, 0.225])\n","                       ])\n","\n","num_workers = 2\n","batch_first = True\n","pin_memory = True\n","dataset = Flickr(root_folder, csv_file, transforms)\n","pad_idx = dataset.vocab.stoi[\"<PAD>\"]\n","\n","train_size = int(0.9 * len(dataset))\n","val_size = int(0.05 * len(dataset))\n","test_size = len(dataset) - train_size - val_size\n","\n","train_set, val_set, test_set = torch.utils.data.random_split(dataset, [train_size, val_size, test_size])\n","\n","train_loader = DataLoader(train_set,\n","                            batch_size=batch_size,\n","                            pin_memory=pin_memory,\n","                            num_workers=num_workers,\n","                            shuffle=True,\n","                            collate_fn=CapCollat(pad_seq=pad_idx, batch_first=batch_first))\n","\n","val_loader = DataLoader(val_set,\n","                            batch_size=batch_size,\n","                            pin_memory=pin_memory,\n","                            num_workers=num_workers,\n","                            shuffle=False,\n","                            collate_fn=CapCollat(pad_seq=pad_idx, batch_first=batch_first))\n","\n","test_loader = DataLoader(test_set,\n","                            batch_size=batch_size,\n","                            pin_memory=pin_memory,\n","                            num_workers=num_workers,\n","                            shuffle=False,\n","                            collate_fn=CapCollat(pad_seq=pad_idx, batch_first=batch_first))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["all_labels = list()\n","\n","for img in dataset['images']:   \n","    for sentence in img['sentences']:\n","        all_labels.append(sentence['tokens'])\n","\n","dict_tokens = dict()\n","imgs_idx = dict()\n","idx_imgs = dict()\n","\n","for idx, img in enumerate(dataset['images']):\n","    dict_tokens[idx] = [sentence['tokens'] for sentence in img['sentences']]   \n","    imgs_idx[img['filename']] = idx\n","    idx_imgs[idx] = img['filename']"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-25T17:42:08.885666Z","iopub.status.busy":"2024-01-25T17:42:08.885273Z","iopub.status.idle":"2024-01-25T17:42:21.909079Z","shell.execute_reply":"2024-01-25T17:42:21.907503Z","shell.execute_reply.started":"2024-01-25T17:42:08.885635Z"},"trusted":true},"outputs":[],"source":["dataitr = iter(test_loader)\n","batch = next(dataitr)\n","\n","images, captions = batch\n","\n","for i in range(batch_size):\n","    img, cap = images[i], captions[i]\n","    \n","    caption_label = [dataset.vocab.itos[token] for token in cap.tolist()]\n","    eos_index = caption_label.index('<EOS>')\n","\n","    caption_label = caption_label[1:eos_index]\n","    caption_label = ' '.join(caption_label)\n","\n","    img = img.permute(1,2,0)\n","    plt.imshow(img)\n","    plt.title(caption_label)\n","\n","    plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["# Pre-Trained CNN Encoder: VGG16 "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class EncoderCNN(nn.Module):\n","    def __init__(self, enc_dim, embed_size, hidden_size):\n","        super(EncoderCNN, self).__init__()\n","\n","        vgg = vgg16(weights=VGG16_Weights.IMAGENET1K_FEATURES)\n","        all_modules = list(vgg.children())\n","        modules = all_modules[:-2]\n","\n","        self.vgg = nn.Sequential(*modules) \n","        self.avgpool = nn.AvgPool2d(7)\n","        self.V_affine = nn.Linear(enc_dim, hidden_size)\n","        self.vg_affine = nn.Linear(enc_dim, embed_size)\n","\n","        self.disable_learning()\n","       \n","    def forward(self, images):\n","        encoded_image = self.vgg(images)\n","\n","        batch_size = encoded_image.shape[0]\n","        features = encoded_image.shape[1]\n","        num_pixels = encoded_image.shape[2] * encoded_image.shape[3]\n","\n","        global_features = self.avgpool(encoded_image).view(batch_size, -1)\n","        global_features = F.relu(self.vg_affine(global_features))\n","\n","        enc_image = encoded_image.permute(0, 2, 3, 1)  \n","        enc_image = enc_image.view(batch_size,num_pixels,features)\n","        enc_image = F.relu(self.V_affine(enc_image))\n","  \n","        return enc_image, global_features\n","\n","    def disable_learning(self):\n","        for param in self.vgg.parameters():\n","            param.requires_grad = False"]},{"cell_type":"markdown","metadata":{},"source":["# RNN Decoder: Vanilla RNN Module"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class DecoderRNN(nn.Module):\n","    def __init__(self, embed_size,vocab_size, hidden_size, num_layers):\n","        super(DecoderRNN, self).__init__()\n","\n","        self.embedding = nn.Embedding(vocab_size, embed_size)\n","        self.rnn = nn.RNN(embed_size, hidden_size, num_layers)\n","        self.linear = nn.Linear(hidden_size, vocab_size)\n","        self.dropout = nn.Dropout(0.5)\n","    \n","    def forward(self, features, caption):\n","        embeddings = self.dropout(self.embedding(caption))\n","        embeddings = torch.cat((features.unsqueeze(0),embeddings), dim=0)\n","        hiddens, _ = self.rnn(embeddings)\n","        outputs = self.linear(hiddens)\n","\n","        return outputs"]},{"cell_type":"markdown","metadata":{},"source":["# Training and Evaluating the Image Captioning Model: VGG16 + Multi-layer Vanilla RNN"]},{"cell_type":"markdown","metadata":{},"source":["### Setting the Device to GPU"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["if torch.cuda.is_available():\n","    torch.backends.cudnn.deterministic = True\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","print('Device:', device)"]},{"cell_type":"markdown","metadata":{},"source":["### Setting Hyperparameters"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["num_epochs = 10\n","freq_threshold = 5\n","enc_dim = 2048\n","embed_size = 300\n","hidden_size = 512\n","att_dim = 49\n","train_embed = False\n","lr_enc = 1e-5\n","lr_dec = 5e-4"]},{"cell_type":"markdown","metadata":{},"source":["### Building the Vocabulary"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["vocab = Vocabulary(freq_threshold)\n","vocab.build_vocabulary(all_labels)\n","vocab_size = len(vocab)"]},{"cell_type":"markdown","metadata":{},"source":["### Configuring Models"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","criterion = nn.CrossEntropyLoss(ignore_index = vocab.stoi[\"<PAD>\"])\n","\n","enc = EncoderCNN(enc_dim, embed_size, hidden_size).to(device)\n","dec = DecoderRNN(embed_size, hidden_size, att_dim, vocab_size, train_embed).to(device)\n","\n","optim_dec = optim.Adam(dec.parameters(), lr = lr_dec, betas = (0.8, 0.999))"]},{"cell_type":"markdown","metadata":{},"source":["### Defining Metrics"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import copy\n","from collections import defaultdict\n","import pdb\n","\n","def precook(s, n=4, out=False):\n","    words = s.split()\n","    counts = defaultdict(int)\n","\n","    for k in range(1,n+1):\n","        for i in range(len(words)-k+1):\n","            ngram = tuple(words[i:i+k])\n","            counts[ngram] += 1\n","\n","    return counts\n","\n","def cook_refs(refs, n=4):\n","    return [precook(ref, n) for ref in refs]\n","\n","def cook_test(test, n=4):\n","    return precook(test, n, True)\n","\n","class CiderScorer(object):\n","    def copy(self):\n","        new = CiderScorer(n=self.n)\n","        new.ctest = copy.copy(self.ctest)\n","        new.crefs = copy.copy(self.crefs)\n","\n","        return new\n","\n","    def __init__(self, test=None, refs=None, n=4, sigma=6.0):\n","        self.n = n\n","        self.sigma = sigma\n","        self.crefs = []\n","        self.ctest = []\n","        self.document_frequency = defaultdict(float)\n","        self.cook_append(test, refs)\n","        self.ref_len = None\n","\n","    def cook_append(self, test, refs):\n","        if refs is not None:\n","            self.crefs.append(cook_refs(refs))\n","            if test is not None:\n","                self.ctest.append(cook_test(test))\n","            else:\n","                self.ctest.append(None)\n","\n","    def size(self):\n","        assert len(self.crefs) == len(self.ctest), \"refs/test mismatch! %d<>%d\" % (len(self.crefs), len(self.ctest))\n","        return len(self.crefs)\n","\n","    def __iadd__(self, other):\n","        if type(other) is tuple:\n","            self.cook_append(other[0], other[1])\n","\n","        else:\n","            self.ctest.extend(other.ctest)\n","            self.crefs.extend(other.crefs)\n","\n","        return self\n","    \n","    def compute_doc_freq(self):\n","        for refs in self.crefs:\n","            for ngram in set([ngram for ref in refs for (ngram,count) in ref.iteritems()]):\n","                self.document_frequency[ngram] += 1\n","\n","    def compute_cider(self):\n","        def counts2vec(cnts):\n","            vec = [defaultdict(float) for _ in range(self.n)]\n","            length = 0\n","            norm = [0.0 for _ in range(self.n)]\n","            for (ngram,term_freq) in cnts.iteritems():\n","                df = np.log(max(1.0, self.document_frequency[ngram]))\n","                n = len(ngram)-1\n","                vec[n][ngram] = float(term_freq)*(self.ref_len - df)\n","                norm[n] += pow(vec[n][ngram], 2)\n","\n","                if n == 1:\n","                    length += term_freq\n","\n","            norm = [np.sqrt(n) for n in norm]\n","\n","            return vec, norm, length\n","\n","        def sim(vec_hyp, vec_ref, norm_hyp, norm_ref, length_hyp, length_ref):\n","            delta = float(length_hyp - length_ref)\n","            val = np.array([0.0 for _ in range(self.n)])\n","\n","            for n in range(self.n):\n","                for (ngram,count) in vec_hyp[n].iteritems():\n","                    val[n] += min(vec_hyp[n][ngram], vec_ref[n][ngram]) * vec_ref[n][ngram]\n","\n","                if (norm_hyp[n] != 0) and (norm_ref[n] != 0):\n","                    val[n] /= (norm_hyp[n]*norm_ref[n])\n","\n","                assert(not math.isnan(val[n]))\n","                val[n] *= np.e**(-(delta**2)/(2*self.sigma**2))\n","\n","            return val\n","\n","        self.ref_len = np.log(float(len(self.crefs)))\n","\n","        scores = []\n","        for test, refs in zip(self.ctest, self.crefs):\n","            vec, norm, length = counts2vec(test)\n","            score = np.array([0.0 for _ in range(self.n)])\n","\n","            for ref in refs:\n","                vec_ref, norm_ref, length_ref = counts2vec(ref)\n","                score += sim(vec, vec_ref, norm, norm_ref, length, length_ref)\n","\n","            score_avg = np.mean(score)\n","            score_avg /= len(refs)\n","            score_avg *= 10.0\n","            scores.append(score_avg)\n","\n","        return scores\n","\n","    def compute_score(self, option=None, verbose=0):\n","        self.compute_doc_freq()\n","        assert(len(self.ctest) >= max(self.document_frequency.values()))\n","        score = self.compute_cider()\n","\n","        return np.mean(np.array(score)), np.array(score)\n","    \n","    class Cider:\n","        def __init__(self, test=None, refs=None, n=4, sigma=6.0):\n","            self._n = n\n","            self._sigma = sigma\n","\n","        def compute_score(self, gts, res):\n","            assert(gts.keys() == res.keys())\n","            imgIds = gts.keys()\n","\n","            cider_scorer = CiderScorer(n=self._n, sigma=self._sigma)\n","\n","            for id in imgIds:\n","                hypo = res[id]\n","                ref = gts[id]\n","\n","                assert(type(hypo) is list)\n","                assert(len(hypo) == 1)\n","                assert(type(ref) is list)\n","                assert(len(ref) > 0)\n","\n","                cider_scorer += (hypo[0], ref)\n","\n","            (score, scores) = cider_scorer.compute_score()\n","\n","            return score, scores\n","\n","        def method(self):\n","            return \"CIDEr\""]},{"cell_type":"markdown","metadata":{},"source":["### Training Models"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!pip install pycocoevalcap\n","\n","from pycocoevalcap import Scorer"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def pred(inp, V, vg, states, decoder):\n","    inp_c = torch.cat((vg, inp), dim = 1)\n","    h_t, m_t = states\n","    s_t, h_t, m_t = decoder.adaptive_lstm(inp_c, states)\n","            \n","    states = (h_t, m_t)\n","            \n","    out_l = decoder.adaptive_att(V, h_t, s_t)\n","    output = decoder.p_affine(decoder.dropout(out_l))\n","    output = F.softmax(output, dim = 1)\n","    \n","    return output.view(output.size(1)).detach().cpu().numpy(), states\n","\n","def caption_image_beam(image, vocabulary, encoder, decoder, device = 'cpu', k = 10, max_length=50):\n","    result_caption = []\n","\n","    with torch.no_grad():\n","        V, vg = encoder(image)\n","        states = (torch.zeros((1, V.size(2))).to(device), torch.zeros((1, V.size(2))).to(device))\n","        sequences = [[list(), 0.0, states]]\n","        inp = vocabulary.stoi['<SOS>']\n","\n","        for _ in range(max_length):\n","            \n","            all_candidates = list()\n","            \n","            for i in range(len(sequences)):\n","                seq, score, states = sequences[i]\n","                \n","                if len(seq) != 0:\n","                    inp = seq[-1]\n","                    \n","                    if vocabulary.itos[inp] == \"<EOS>\":\n","                        all_candidates.append(sequences[i])\n","                        continue\n","                        \n","                inp = decoder.embed(torch.tensor([inp]).to(device))\n","                    \n","                predictions, states = pred(inp, V, vg, states, decoder)\n","                \n","                word_preds = np.argsort(predictions)[-k:]\n","                \n","                for j in word_preds:\n","                    candidate = (seq + [j], score - math.log(predictions[j]), states)\n","                    all_candidates.append(candidate)\n","                    \n","            ordered = sorted(all_candidates, key=lambda tup:tup[1]/(len(tup[0])))\n","            sequences = ordered[:k]     \n","            \n","    output_arr = sequences[0][0]\n","            \n","    if vocabulary.itos[sequences[0][0][-1]] == '<EOS>':\n","        output_arr = sequences[0][0][:-1]\n","        \n","    if vocabulary.itos[sequences[0][0][0]] == '<SOS>':\n","        output_arr = output_arr[1:]    \n","        \n","    return [vocabulary.itos[idx] for idx in output_arr]\n","\n","def compute_individual_metrics(reference, candidate):\n","    metrics = dict()\n","\n","    metrics['Bleu_1'] = sentence_bleu(reference, candidate, weights=(1, 0, 0, 0))\n","    metrics['Bleu_2'] = sentence_bleu(reference, candidate, weights=(0, 1, 0, 0))\n","    metrics['Bleu_3'] = sentence_bleu(reference, candidate, weights=(0, 0, 1, 0))\n","    metrics['Bleu_4'] = sentence_bleu(reference, candidate, weights=(0, 0, 0, 1))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["val_iter = iter(val_loader)\n","\n","train_losses = list()\n","val_losses = list()\n","\n","cumulative_bleu_scores = list()\n","\n","cider_scores = list()\n","\n","meteor_scores = list()\n","\n","rougel_scores = list()\n","\n","bleu1_scores = list()\n","bleu2_scores = list()\n","bleu3_scores = list()\n","bleu4_scores = list()\n","\n","for epoch in range(num_epochs):\n","    for batch_idx, (imgs, captions, img_ids) in enumerate(train_loader):\n","\n","        dec.train()\n","                \n","        imgs = imgs.to(device)\n","        captions = captions.to(device)\n","\n","        enc_imgs, global_features = enc(imgs)\n","        outputs = dec(enc_imgs, global_features, captions[:-1], captions[:-1].size(0), device)\n","\n","        loss = criterion(outputs.reshape(-1, outputs.shape[2]), captions[1:].reshape(-1))\n","                \n","        optim_dec.zero_grad()\n","\n","        loss.backward()\n","                \n","        optim_dec.step()\n","\n","        if batch_idx % 600 == 0:\n","            \n","            with torch.no_grad():\n","                \n","                enc.eval()\n","                dec.eval()\n","                \n","                try:\n","                    val_imgs, val_captions, val_img_ids = next(val_iter)\n","                \n","                except StopIteration:\n","                    val_iter = iter(val_loader)\n","                    val_imgs, val_captions, val_img_ids = next(val_iter)\n","                \n","                val_imgs = val_imgs.to(device)\n","                val_captions = val_captions.to(device)\n","\n","                enc_val_imgs, global_val_features = enc(val_imgs)\n","                val_outputs = dec(enc_val_imgs, global_val_features, val_captions[:-1], val_captions[:-1].size(0), device)\n","\n","                val_loss = criterion(val_outputs.reshape(-1, val_outputs.shape[2]), val_captions[1:].reshape(-1))\n","                \n","                val_losses.append(val_loss.item())\n","                train_losses.append(loss.item())\n","                \n","                val_img_ids = val_img_ids.squeeze(1).numpy()\n","                \n","                r_set = np.arange(batch_size)\n","                \n","                np.random.shuffle(r_set)\n","                \n","                index = r_set[0]\n","                \n","                val_img = val_imgs[index]\n","                candidate = caption_image_beam(val_img.unsqueeze(0), vocab, enc, dec, device)\n","                ref_tokens = dict_tokens[val_img_ids[index]]\n","                \n","                cumulative_bleu_score = sentence_bleu(ref_tokens, candidate, weights=(0.25, 0.25, 0.25, 0.25))\n","                    \n","                hyp = ' '.join(candidate)\n","                refs = list()\n","                    \n","                for sentence in ref_tokens:\n","                    ref = ' '.join(sentence)\n","                    refs.append(ref)\n","                    \n","                metrics = compute_individual_metrics(refs, hyp)\n","                \n","                np.random.shuffle(refs)\n","                \n","                bleu1_scores.append(metrics['Bleu_1'])\n","                bleu2_scores.append(metrics['Bleu_2'])\n","                bleu3_scores.append(metrics['Bleu_3'])\n","                bleu4_scores.append(metrics['Bleu_4'])\n","\n","                cider_scores.append(metrics['CIDEr'])\n","\n","                cumulative_bleu_scores.append(cumulative_bleu_score)\n","\n","                rougel_scores.append(metrics['ROUGE_L'])\n","\n","                meteor_scores.append(metrics['METEOR'])\n","                \n","                pil_img = Image.open(os.path.join('flickr30k_images', idx_imgs[val_img_ids[index]])).convert(\"RGB\")\n","                plt.imshow(np.asarray(pil_img))\n","                \n","                print(f\"Epoch [{epoch+1}/{num_epochs}] Batch [{batch_idx+1}/{len(train_loader)}] \n","                      Training loss: {loss.item()} \n","                      Validation loss: {val_loss.item()} \n","                      Cumulative BLEU Score: {cumulative_bleu_score} \n","                      CIDEr Score: {metrics['CIDEr']} \n","                      METEOR Score: {metrics['METEOR']} \n","                      ROUGE_L: {metrics['ROUGE_L']} \\n\")\n","                \n","                print(f\"Ground Truth: {refs[0]}\\n\")\n","                \n","                print(f\"Predicted Caption: {hyp}\\n\\n\")\n","    \n","        if batch_idx == (len(train_loader) - 1):\n","            torch.save(enc.state_dict(), f'/kaggle/working/encoder_{freq_threshold}_{batch_size}_{hidden_size}_{epoch+1}.pth')\n","            torch.save(dec.state_dict(), f'/kaggle/working/decoder_{freq_threshold}_{batch_size}_{hidden_size}_{epoch+1}.pth')\n"]},{"cell_type":"markdown","metadata":{},"source":["### Plotting Losses and Scores"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.figure(0)\n","plt.plot(train_losses, label = 'Training loss')\n","plt.plot(val_losses, label = 'Validation loss')\n","plt.ylabel('Cross Entropy Loss')\n","plt.legend()\n","plt.savefig(f'/kaggle/working/{freq_threshold}_{batch_size}_{hidden_size}_{num_epochs}_losses.png')\n","\n","plt.figure(1)\n","plt.plot(bleu1_scores, label = 'BLEU 1')\n","plt.plot(bleu2_scores, label = 'BLEU 2')\n","plt.plot(bleu3_scores, label = 'BLEU 3')\n","plt.plot(bleu4_scores, label = 'BLEU 4')\n","plt.ylabel('BLEU Scores')\n","plt.legend()\n","plt.savefig(f'/kaggle/working/{freq_threshold}_{batch_size}_{hidden_size}_{num_epochs}_bleu_scores.png')\n","        \n","plt.figure(2)\n","plt.plot(cumulative_bleu_scores, label = 'Cumulative BLEU SCORE')\n","plt.plot(cider_scores, label = 'CIDEr SCORE')\n","plt.plot(meteor_scores, label = 'METEOR SCORE')\n","plt.plot(rougel_scores, label = 'ROUGE_L SCORE')\n","plt.ylabel('Scores')\n","plt.legend()\n","plt.savefig(f'/kaggle/working/{freq_threshold}_{batch_size}_{hidden_size}_{num_epochs}_scores.png')"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":31296,"sourceId":39911,"sourceType":"datasetVersion"}],"dockerImageVersionId":30635,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
